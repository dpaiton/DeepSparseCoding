{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook # required for training plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os                                                                       \n",
    "import numpy as np                                                              \n",
    "import tensorflow as tf                                                         \n",
    "import data.data_picker as dp                                                   \n",
    "import utils.plot_functions as pf                                               \n",
    "import utils.image_processing as ip                                             \n",
    "import utils.notebook as nb\n",
    "\n",
    "import matplotlib.gridspec as gsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "  ## Model params\n",
    "  \"out_dir\": os.path.expanduser(\"~\")+\"/Work/Projects/ICAstrongPCA/outputs/\",\n",
    "  \"chk_dir\": os.path.expanduser(\"~\")+\"/Work/Projects/ICAstrongPCA/checkpoints/\",\n",
    "  \"data_dir\": os.path.expanduser(\"~\")+\"/Work/Datasets/\",\n",
    "  \"load_chk\": True,\n",
    "  \"load_cov\": False,\n",
    "  \"update_interval\": 500,\n",
    "  \"device\": \"/gpu:0\",\n",
    "  \"prior\": \"laplacian\",\n",
    "  \"learning_rate\": 0.01,\n",
    "  \"num_neurons\": int(16**2),\n",
    "  \"eta\": 0.001/0.03, #dt/tau\n",
    "  \"eps\": 1e-12,\n",
    "  \"max_cp_to_keep\": 2,\n",
    "  ## Data params\n",
    "  \"data_type\": \"vanhateren\",\n",
    "  \"rand_state\": np.random.RandomState(12345),\n",
    "  \"num_images\": 100,\n",
    "  \"num_batches\": int(1e5), #Total dataset size is num_batches*batch_size\n",
    "  \"batch_size\": 100,\n",
    "  \"patch_edge_size\": 16,\n",
    "  \"overlapping_patches\": True,\n",
    "  \"patch_variance_threshold\": 1e-6,\n",
    "  \"conv\": False,\n",
    "  \"whiten_images\": False,\n",
    "  \"contrast_normalize\": False,\n",
    "  ## Fourier analysis params\n",
    "  \"ft_padding\": 20,\n",
    "  ## Pooling params\n",
    "  \"cov_num_images\":int(1e7),\n",
    "  \"num_pooling_dims\": 24, #K\n",
    "  ## Visualization params\n",
    "  \"num_pooling_filters\":50,\n",
    "  \"num_connected_weights\":256}\n",
    "\n",
    "## Calculated params\n",
    "params[\"epoch_size\"] = params[\"batch_size\"] * params[\"num_batches\"]\n",
    "params[\"num_training_steps\"] = 20000 # params[\"num_batches\"] # Could be set to less, model trains in 20k\n",
    "params[\"num_pixels\"] = int(params[\"patch_edge_size\"]**2)\n",
    "params[\"dataset_shape\"] = [int(val)\n",
    "    for val in [params[\"epoch_size\"], params[\"num_pixels\"]]],\n",
    "params[\"phi_shape\"] = [params[\"num_neurons\"], params[\"num_pixels\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling filters function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_pooling_filters(activity_covariance, num_pooling_dims):\n",
    "  evals, evecs = np.linalg.eig(activity_covariance)\n",
    "  sort_indices = np.argsort(evals)[::-1]\n",
    "  top_vecs = evecs[:, sort_indices[:num_pooling_dims]]\n",
    "  pooling_filters = np.dot(top_vecs, top_vecs.T)\n",
    "  return pooling_filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not params[\"load_cov\"] or not params[\"load_chk\"]: \n",
    "  data = dp.get_data(params[\"data_type\"], params)                                 \n",
    "  params[\"input_shape\"] = [                                                       \n",
    "      data[\"train\"].num_rows*data[\"train\"].num_cols*data[\"train\"].num_channels]     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with tf.device(params[\"device\"]):\n",
    "  with graph.as_default():\n",
    "    with tf.name_scope(\"placeholders\") as scope:\n",
    "      x = tf.placeholder(tf.float32, shape=[None, params[\"num_pixels\"]],\n",
    "        name=\"input_data\")\n",
    "\n",
    "    with tf.name_scope(\"step_counter\") as scope:\n",
    "      global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "\n",
    "    with tf.variable_scope(\"weights\") as scope:\n",
    "      ## Q matrix from QR decomp is guaranteed to be orthonormal and\n",
    "      ## non-singular, which prevents a gradient explosion from inverting\n",
    "      ## the weight matrix.\n",
    "      Q, R = np.linalg.qr(np.random.standard_normal(params[\"phi_shape\"]))\n",
    "      phi = tf.get_variable(name=\"phi\", dtype=tf.float32,\n",
    "        initializer=Q.astype(np.float32), trainable=True)\n",
    "      phi_inverse = tf.matrix_inverse(phi, name=\"phi_inverse\")\n",
    "\n",
    "    with tf.name_scope(\"inference\") as scope:\n",
    "      u = tf.matmul(x, phi_inverse, name=\"coefficients\")\n",
    "      if params[\"prior\"] == \"laplacian\":\n",
    "        a = tf.sign(u)\n",
    "      else: #It must be laplacian or cauchy\n",
    "        a = (2*u) / (1 + tf.pow(u, 2.0))\n",
    "      u_covariance = tf.divide(tf.matmul(tf.transpose(tf.nn.relu(u)), tf.nn.relu(u)),\n",
    "        tf.to_float(tf.shape(x)[0]), name=\"u_cov_matrix\")\n",
    "\n",
    "    with tf.name_scope(\"optimizers\") as scope:\n",
    "      learning_rates = tf.train.exponential_decay(\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        global_step=global_step,\n",
    "        decay_steps=int(np.floor(params[\"num_batches\"]*0.8)),\n",
    "        decay_rate=0.7,\n",
    "        staircase=True,\n",
    "        name=\"phi_annealing_schedule\")\n",
    "      optimizer = tf.train.GradientDescentOptimizer(learning_rates, name=\"phi_optimizer\")\n",
    "      a_u_avg = tf.divide(tf.matmul(tf.transpose(u), a),                   \n",
    "        tf.to_float(tf.shape(x)[0]), name=\"avg_samples\")                        \n",
    "      gradient = -tf.subtract(tf.matmul(a_u_avg, phi), phi, name=\"phi_gradient\")                                                \n",
    "      update_weights = optimizer.apply_gradients([(gradient, phi)], global_step=global_step)\n",
    "\n",
    "    full_saver = tf.train.Saver(var_list=[phi], max_to_keep=2)\n",
    "\n",
    "    with tf.name_scope(\"summaries\") as scope:\n",
    "      #tf.summary.image(\"input\", tf.reshape(x, [params[\"batch_size\"],\n",
    "      #  params[\"patch_edge_size\"], params[\"patch_edge_size\"], 1]))\n",
    "      tf.summary.histogram(\"u\", u)\n",
    "      tf.summary.histogram(\"a\", a)\n",
    "      tf.summary.histogram(\"phi\", phi)\n",
    "\n",
    "    merged_summaries = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(params[\"out_dir\"], graph)\n",
    "\n",
    "    with tf.name_scope(\"initialization\") as scope:\n",
    "      init_op = tf.group(tf.global_variables_initializer(),\n",
    "        tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ICA model (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not params[\"load_chk\"]:\n",
    "  if not os.path.exists(params[\"out_dir\"]):\n",
    "    os.makedirs(params[\"out_dir\"])\n",
    "  if not os.path.exists(params[\"chk_dir\"]):\n",
    "    os.makedirs(params[\"chk_dir\"])\n",
    "\n",
    "  with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init_op,\n",
    "    feed_dict={x:np.zeros([params[\"batch_size\"]]+params[\"input_shape\"],\n",
    "      dtype=np.float32)})\n",
    "    sess.graph.finalize() # Graph is read-only after this statement\n",
    "    \n",
    "    #fig, sub_ax = plt.subplots(1, 1, figsize=(10, 10))  \n",
    "    #init_data = sess.run(phi)\n",
    "    #init_data = ip.normalize_data_with_max(init_data)\n",
    "    #plot_data = pf.pad_data(init_data.reshape((params[\"num_neurons\"],\n",
    "    #  params[\"patch_edge_size\"], params[\"patch_edge_size\"])))\n",
    "    #im_ax = sub_ax.imshow(plot_data, cmap=\"Greys_r\", interpolation=\"nearest\")\n",
    "    #sub_ax.tick_params(axis=\"both\", bottom=\"off\", top=\"off\", left=\"off\", right=\"off\")\n",
    "    #sub_ax.get_xaxis().set_visible(False)\n",
    "    #sub_ax.get_yaxis().set_visible(False)\n",
    "    #plt.show()\n",
    "    for b_step in range(params[\"num_training_steps\"]):\n",
    "      input_data = data[\"train\"].next_batch(params[\"batch_size\"])[0]\n",
    "      feed_dict = {x:input_data}\n",
    "      sess.run(update_weights, feed_dict)\n",
    "\n",
    "      current_step = sess.run(global_step)\n",
    "      if (current_step % params[\"update_interval\"] == 0):\n",
    "        summary = sess.run(merged_summaries, feed_dict)\n",
    "        train_writer.add_summary(summary, current_step)\n",
    "        full_saver.save(sess, save_path=params[\"chk_dir\"]+\"ica_chk\", global_step=global_step)\n",
    "        #weights = sess.run(phi, feed_dict)\n",
    "        #weights = ip.normalize_data_with_max(weights)\n",
    "        #plot_data = pf.pad_data(weights.reshape((params[\"num_neurons\"],\n",
    "        #  params[\"patch_edge_size\"], params[\"patch_edge_size\"])))\n",
    "        #im_ax.set_data(plot_data)\n",
    "        #sub_ax.set_title(\"Basis Functions at Time Step \"+str(current_step), fontsize=24)  \n",
    "        #fig.canvas.draw()\n",
    "    full_saver.save(sess, save_path=params[\"chk_dir\"]+\"ica_chk\",\n",
    "      global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute activity covariance & pooling filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute activity covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"load_cov\"]:\n",
    "  u_cov = np.load(params[\"out_dir\"]+\"u_cov.npz\")[\"data\"]\n",
    "  weights = np.load(params[\"out_dir\"]+\"weights.npz\")[\"data\"]\n",
    "else:\n",
    "  with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init_op,\n",
    "      feed_dict={x:np.zeros([params[\"batch_size\"]]+params[\"input_shape\"],\n",
    "      dtype=np.float32)})\n",
    "    full_saver.restore(sess, tf.train.latest_checkpoint(params[\"chk_dir\"]))\n",
    "    weights = sess.run(phi).T\n",
    "    u_cov = None\n",
    "    num_cov_in_avg = 0\n",
    "    tot_images = 0\n",
    "    for tot_images in nb.log_progress(range(0, params[\"cov_num_images\"], params[\"batch_size\"]), every=1):\n",
    "      input_data = data[\"train\"].next_batch(params[\"batch_size\"])[0]\n",
    "      feed_dict = {x:input_data}\n",
    "      if u_cov is None:\n",
    "        u_cov = sess.run(u_covariance, feed_dict)\n",
    "      else:\n",
    "        u_cov += sess.run(u_covariance, feed_dict)\n",
    "      num_cov_in_avg += 1\n",
    "    u_cov /= num_cov_in_avg\n",
    "  np.savez(params[\"out_dir\"]+\"u_cov.npz\", data=u_cov)\n",
    "  np.savez(params[\"out_dir\"]+\"weights.npz\", data=weights)\n",
    "#import scipy.io as sio\n",
    "#sio.savemat(params[\"out_dir\"]+\"u_cov.mat\", {\"u_cov\":u_cov})\n",
    "#sio.savemat(params[\"out_dir\"]+\"phi_weights.mat\", {\"weights\":weights})\n",
    "u_evals, u_evecs = np.linalg.eig(u_cov)\n",
    "u_sort_indices = np.argsort(u_evals)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute pooling filters and basis function summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pooling_filters = compute_pooling_filters(u_cov, params[\"num_pooling_dims\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bf_stats = ip.get_dictionary_stats(weights, padding=20, num_gauss_fits=20, gauss_thresh=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct analysis plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = pf.plot_cov_matrix(u_cov, str(params[\"cov_num_images\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = pf.plot_eigenvalues(u_evals[u_sort_indices], ylim=[0, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_random_bases = 8\n",
    "num_top_cov_bases = 10\n",
    "bf_indices = np.random.choice(np.arange(u_cov.shape[0]), num_random_bases)\n",
    "fig = pf.plot_top_bases(u_cov, weights, bf_indices, num_top_cov_bases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis function analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = pf.plot_hilbert_analysis(weights, params[\"ft_padding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pf.plot_loc_freq_summary(bf_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = pf.plot_bf_stats(bf_stats, num_bf=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pf.plot_gaussian_contours(bf_stats, num_plots=49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling and eigen summary plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = pf.plot_ellipse_summaries(bf_stats, num_bf=params[\"num_neurons\"], lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"num_pooling_filters\"] = 64\n",
    "params[\"num_connected_weights\"] = params[\"num_neurons\"]#250#75\n",
    "fig = pf.plot_pooling_summaries(bf_stats, u_evecs[:, u_sort_indices[:params[\"num_pooling_filters\"]]],\n",
    "  params[\"num_pooling_filters\"], params[\"num_connected_weights\"], lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params[\"num_pooling_filters\"] = 36\n",
    "params[\"num_connected_weights\"] = params[\"num_neurons\"]#250#75\n",
    "fig = pf.plot_pooling_summaries(bf_stats, pooling_filters, params[\"num_pooling_filters\"],\n",
    "  params[\"num_connected_weights\"], lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_pooling_filters = 16\n",
    "fig_size = (10, 10)\n",
    "spot_size = 100\n",
    "fig = pf.plot_pooling_centers(bf_stats, pooling_filters, num_pooling_filters, fig_size, spot_size)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
