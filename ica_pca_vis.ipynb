{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* rename ICA variables to match LCA variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os                                                                       \n",
    "import scipy.optimize as opt\n",
    "import numpy as np                                                              \n",
    "import tensorflow as tf                                                         \n",
    "import data.data_picker as dp                                                   \n",
    "import utils.plot_functions as pf                                               \n",
    "import utils.image_processing as ip                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {                                                                      \n",
    "  ## Model params                                                               \n",
    "  \"out_dir\": os.path.expanduser(\"~\")+\"/Work/Projects/ICAstrongPCA/outputs/\",       \n",
    "  \"chk_dir\": os.path.expanduser(\"~\")+\"/Work/Projects/ICAstrongPCA/checkpoints/\",\n",
    "  \"data_dir\": os.path.expanduser(\"~\")+\"/Work/Datasets/\",\n",
    "  \"load_chk\": False,                                                             \n",
    "  \"load_cov\": False,\n",
    "  \"update_interval\": 1,\n",
    "  \"device\": \"/gpu:0\",                                                           \n",
    "  \"prior\": \"laplacian\",\n",
    "  \"learning_rate\": 0.01,                                                        \n",
    "  \"num_neurons\": int(16**2),                                                           \n",
    "  \"num_inference_steps\": 20,                                                    \n",
    "  \"eta\": 0.001/0.03, #dt/tau                                                    \n",
    "  \"eps\": 1e-12,                                                                 \n",
    "  ## Data params                                                                \n",
    "  \"data_type\": \"vanhateren\",                                                    \n",
    "  \"rand_state\": np.random.RandomState(12345),                                   \n",
    "  \"num_images\": 100,                                                          \n",
    "  \"num_batches\": int(1e5), #Total dataset size is num_batches*batch_size\n",
    "  \"batch_size\": 100,                                                            \n",
    "  \"patch_edge_size\": 16,                                                        \n",
    "  \"overlapping_patches\": True,                                                  \n",
    "  \"patch_variance_threshold\": 1e-6,                                             \n",
    "  \"conv\": False,                                                                \n",
    "  \"whiten_images\": False,\n",
    "  ## Pooling params\n",
    "  \"cov_num_images\":int(1e7),\n",
    "  \"num_pooling_dims\": 50, #K\n",
    "  ## Visualization params\n",
    "  \"num_pooling_filters\":50,\n",
    "  \"num_connected_weights\":250}\n",
    "\n",
    "## Calculated params                                                            \n",
    "params[\"epoch_size\"] = params[\"batch_size\"] * params[\"num_batches\"]             \n",
    "params[\"num_pixels\"] = int(params[\"patch_edge_size\"]**2)                        \n",
    "params[\"dataset_shape\"] = [int(val)                                             \n",
    "    for val in [params[\"epoch_size\"], params[\"num_pixels\"]]],                   \n",
    "params[\"a_shape\"] = [params[\"num_pixels\"], params[\"num_neurons\"]]             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling filters function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_pooling_filters(a_cov, num_pooling_dims):\n",
    "  evals, evecs = np.linalg.eig(a_cov)\n",
    "  sort_indices = np.argsort(evals)[::-1]\n",
    "  top_vecs = evecs[sort_indices[:num_pooling_dims]]\n",
    "  pooling_filters = np.dot(top_vecs.T, top_vecs)\n",
    "  return pooling_filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not params[\"load_cov\"] and not params[\"load_chk\"]: \n",
    "  data = dp.get_data(params[\"data_type\"], params)                                 \n",
    "  params[\"input_shape\"] = [                                                       \n",
    "    data[\"train\"].num_rows*data[\"train\"].num_cols*data[\"train\"].num_channels]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params[\"input_shape\"] = [                                                       \n",
    "  data[\"train\"].num_rows*data[\"train\"].num_cols*data[\"train\"].num_channels]     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with tf.device(params[\"device\"]):\n",
    "  with graph.as_default():\n",
    "    with tf.name_scope(\"placeholders\") as scope:\n",
    "      x = tf.placeholder(tf.float32, shape=[None, params[\"num_pixels\"]],\n",
    "        name=\"input_data\")\n",
    "\n",
    "    with tf.name_scope(\"step_counter\") as scope:\n",
    "      global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "\n",
    "    with tf.variable_scope(\"weights\") as scope:\n",
    "      ## Q matrix from QR decomp is guaranteed to be orthonormal and\n",
    "      ## non-singular, which prevents a gradient explosion from inverting\n",
    "      ## the weight matrix.\n",
    "      Q, R = np.linalg.qr(np.random.standard_normal(params[\"a_shape\"]))\n",
    "      a = tf.get_variable(name=\"a\", dtype=tf.float32,\n",
    "        initializer=Q.astype(np.float32), trainable=True)\n",
    "\n",
    "    with tf.name_scope(\"inference\") as scope:\n",
    "      u = tf.matmul(x, tf.matrix_inverse(a, name=\"a_inverse\"),\n",
    "        name=\"coefficients\")\n",
    "      if params[\"prior\"] == \"laplacian\":\n",
    "        z = tf.sign(u)\n",
    "      else: #It must be laplacian or cauchy\n",
    "        z = (2*u) / (1 + tf.pow(u, 2.0))\n",
    "\n",
    "    with tf.name_scope(\"optimizers\") as scope:\n",
    "      learning_rates = tf.train.exponential_decay(\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        global_step=global_step,\n",
    "        decay_steps=int(np.floor(params[\"num_batches\"]*0.8)),\n",
    "        decay_rate=0.5,\n",
    "        staircase=True,\n",
    "        name=\"a_annealing_schedule\")\n",
    "      optimizer = tf.train.GradientDescentOptimizer(learning_rates,\n",
    "        name=\"a_optimizer\")\n",
    "      z_u_avg = tf.divide(tf.matmul(tf.transpose(u), z),                   \n",
    "        tf.to_float(tf.shape(x)[0]), name=\"avg_samples\")                        \n",
    "      gradient = -tf.subtract(tf.matmul(z_u_avg, a), a, name=\"a_gradient\")                                                \n",
    "      update_weights = optimizer.apply_gradients([(gradient, a)],\n",
    "        global_step=global_step)\n",
    "\n",
    "    full_saver = tf.train.Saver(var_list=[a], max_to_keep=2)\n",
    "\n",
    "    with tf.name_scope(\"summaries\") as scope:\n",
    "      #tf.summary.image(\"input\", tf.reshape(x, [params[\"batch_size\"],\n",
    "      #  params[\"patch_edge_size\"], params[\"patch_edge_size\"], 1]))\n",
    "      tf.summary.histogram(\"u\", u)\n",
    "      tf.summary.histogram(\"z\", z)\n",
    "      tf.summary.histogram(\"a\", a)\n",
    "\n",
    "    merged_summaries = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(params[\"out_dir\"], graph)\n",
    "\n",
    "    with tf.name_scope(\"initialization\") as scope:\n",
    "      init_op = tf.group(tf.global_variables_initializer(),\n",
    "        tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LCA model (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not params[\"load_chk\"]:\n",
    "  if not os.path.exists(params[\"out_dir\"]):\n",
    "    os.makedirs(params[\"out_dir\"])\n",
    "  if not os.path.exists(params[\"chk_dir\"]):\n",
    "    os.makedirs(params[\"chk_dir\"])\n",
    "\n",
    "  fig, sub_axes = plt.subplots(1, 1, figsize=(10,10))  \n",
    "  with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init_op,\n",
    "    feed_dict={x:np.zeros([params[\"batch_size\"]]+params[\"input_shape\"],\n",
    "      dtype=np.float32)})\n",
    "    \n",
    "    sess.graph.finalize() # Graph is read-only after this statement.\n",
    "\n",
    "    num_updates = np.int32(np.floor(params[\"num_batches\"]/params[\"update_interval\"]))\n",
    "    batch_steps = [None]*num_updates\n",
    "    sparsities = [None]*num_updates\n",
    "    #update_idx = 0\n",
    "    for b_step in range(params[\"num_batches\"]):\n",
    "      input_data = data[\"train\"].next_batch(params[\"batch_size\"])[0]\n",
    "\n",
    "      feed_dict = {x:input_data}\n",
    "\n",
    "      sess.run(update_weights, feed_dict)\n",
    "\n",
    "      current_step = sess.run(global_step)\n",
    "      if (current_step % params[\"update_interval\"] == 0):\n",
    "        summary = sess.run(merged_summaries, feed_dict)\n",
    "        train_writer.add_summary(summary, current_step)\n",
    "        full_saver.save(sess, save_path=params[\"chk_dir\"]+\"ica_chk\",\n",
    "          global_step=global_step)\n",
    "        [u_vals, z_vals, weights] = sess.run(\n",
    "          [u, z, a], feed_dict)\n",
    "\n",
    "        fig.clf()\n",
    "        plot_data = pf.pad_data(weights.T.reshape((params[\"num_neurons\"],\n",
    "          params[\"patch_edge_size\"], params[\"patch_edge_size\"])))\n",
    "        sub_axes.imshow(plot_data, cmap=\"Greys_r\", interpolation=\"nearest\")\n",
    "        sub_axes.tick_params(axis=\"both\", bottom=\"off\", top=\"off\", left=\"off\", right=\"off\")\n",
    "        sub_axes.get_xaxis().set_visible(False)\n",
    "        sub_axes.get_yaxis().set_visible(False)\n",
    "        sub_axes.set_title(\"Basis Functions at time step \"+str(current_step),\n",
    "          fontsize=24)  \n",
    "        fig.canvas.draw()\n",
    "        plt.show()\n",
    "        \n",
    "        #u_vals_max = np.array(u_vals.max()).tolist()\n",
    "        #z_frac_act = np.array(np.count_nonzero(z_vals)\n",
    "        #  / float(params[\"batch_size\"]*params[\"num_neurons\"])).tolist()\n",
    "        #batch_steps[update_idx] = current_step\n",
    "        #sparsities[update_idx] = z_frac_act\n",
    "\n",
    "        #print_dict = {\"current_step\":str(current_step).zfill(5),\n",
    "        #  \"u_max\":str(u_vals_max),\n",
    "        #  \"z_frac_act\":str(z_frac_act)}\n",
    "        #print(print_dict)\n",
    "        #pf.save_data_tiled(weights.T.reshape((params[\"num_neurons\"],\n",
    "        #  params[\"patch_edge_size\"], params[\"patch_edge_size\"])),\n",
    "        #  normalize=False, title=\"Dictionary at step \"+str(current_step),\n",
    "        #  save_filename=(params[\"out_dir\"]+\"phi_\"+str(current_step).zfill(5)\n",
    "        #  +\".png\"))\n",
    "        #update_idx += 1\n",
    "    # TODO: Broke\n",
    "    #output_data = {\"batch_step\":batch_steps, \"frac_active\":sparsities}\n",
    "    #pf.save_stats(output_data,\n",
    "    #  save_filename=(params[\"out_dir\"]+\"train_stats.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute activity covariance & pooling filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute activity covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if params[\"load_cov\"]:\n",
    "  u_cov = np.load(params[\"out_dir\"]+\"u_cov.npz\")[\"data\"]\n",
    "  weights = np.load(params[\"out_dir\"]+\"weights.npz\")[\"data\"]\n",
    "else:\n",
    "  with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init_op,\n",
    "      feed_dict={x:np.zeros([params[\"batch_size\"]]+params[\"input_shape\"],\n",
    "      dtype=np.float32)})\n",
    "    full_saver.restore(sess, tf.train.latest_checkpoint(params[\"chk_dir\"]))\n",
    "    u_cov = None\n",
    "    num_cov_in_avg = 0\n",
    "    tot_images = 0\n",
    "    while tot_images < params[\"cov_num_images\"]: \n",
    "      input_data = data[\"train\"].next_batch(params[\"batch_size\"])[0]\n",
    "      feed_dict = {x:input_data}\n",
    "      [u_vals, weights] = sess.run([u, a], feed_dict)\n",
    "      if u_cov is None:\n",
    "        u_cov = np.cov(u_vals.T)\n",
    "      else:\n",
    "        u_cov += np.cov(u_vals.T)\n",
    "      num_cov_in_avg += 1\n",
    "      tot_images += params[\"batch_size\"]\n",
    "    u_cov /= num_cov_in_avg\n",
    "  np.savez(params[\"out_dir\"]+\"u_cov.npz\", data=u_cov)\n",
    "  np.savez(params[\"out_dir\"]+\"weights.npz\", data=weights)\n",
    "  #import scipy.io as sio\n",
    "  #sio.savemat(params[\"out_dir\"]+\"u_cov.mat\", {\"u_cov\":u_cov})\n",
    "  #sio.savemat(params[\"out_dir\"]+\"phi_weights.mat\", {\"weights\":weights})\n",
    "u_evals, u_evecs = np.linalg.eig(u_cov)\n",
    "u_sort_indices = np.argsort(u_evals)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute pooling filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pooling_filters = compute_pooling_filters(u_cov, params[\"num_pooling_dims\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct analysis plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity covariance matrix summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,10))\n",
    "im = ax.imshow(u_cov, cmap=\"Greys_r\", interpolation=\"nearest\")\n",
    "ax.set_title(\"Covariance matrix computed over \"+str(params[\"cov_num_images\"])+\" image patches\", fontsize=16)\n",
    "pf.add_colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvalues of the activity covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,10))\n",
    "ax.semilogy(u_evals)\n",
    "ax.set_xlim(0, 500) # Ignore first eigenvalue\n",
    "ax.set_ylim(0, 1200)\n",
    "ax.set_yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top connected basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_random_bases = 8\n",
    "num_top_cov_bases = 10\n",
    "bf_indices = np.random.choice(np.arange(u_cov.shape[0]), num_random_bases)\n",
    "pf.plot_top_bases(u_cov, weights, bf_indices, num_top_cov_bases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis function analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pf.plot_hilbert_analysis(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_stats = ip.get_dictionary_stats(weights)\n",
    "num_bf = 10\n",
    "pf.plot_bf_stats(bf_stats, num_bf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling and eigen summary plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pf.plot_pooling_func(pf.plot_lines, weights, u_evecs[u_sort_indices[:50]],\n",
    "  params[\"num_connected_weights\"], params[\"num_pooling_filters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pf.plot_pooling_func(pf.plot_ellipse, weights, u_evecs[u_sort_indices[:50]],\n",
    "  params[\"num_connected_weights\"], params[\"num_pooling_filters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pf.plot_pooling_func(pf.plot_lines, weights, pooling_filters,\n",
    "  params[\"num_connected_weights\"], params[\"num_pooling_filters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pf.plot_pooling_func(pf.plot_ellipse, weights, pooling_filters,\n",
    "  params[\"num_connected_weights\"], params[\"num_pooling_filters\"])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
