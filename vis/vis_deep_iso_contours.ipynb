{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import tensorflow as tf\n",
    "from data.dataset import Dataset\n",
    "import data.data_selector as ds\n",
    "import utils.data_processing as dp\n",
    "import utils.plot_functions as pf\n",
    "import analysis.analysis_picker as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_function():\n",
    "  return tf.identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lambda_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lambda\"\n",
    "    self.model_name = \"lambda_mnist\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "    self.activation_function = get_activation_function()\n",
    "\n",
    "class mlp_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"mlp\"\n",
    "    self.model_name = \"mlp_mnist\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_768_mnist\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class ae_deep_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"ae\"\n",
    "    self.model_name = \"ae_deep_mnist\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "params_list = [lca_params()]\n",
    "for params in params_list:\n",
    "  params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+params.model_name)\n",
    "  \n",
    "analyzer_list = [ap.get_analyzer(params.model_type) for params in params_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analyzer, params in zip(analyzer_list, params_list):\n",
    "  analyzer.setup(params)\n",
    "  if(hasattr(params, \"activation_function\")):\n",
    "    analyzer.model_params.activation_function = params.activation_function\n",
    "  analyzer.setup_model(analyzer.model_params)\n",
    "  analyzer.load_analysis(save_info=params.save_info)\n",
    "  analyzer.model_name = params.model_name"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "images = dp.standardize_data(\n",
    "  np.random.normal(loc=0.0, scale=1.0, size=[2]+analyzer.model.get_input_shape()[1:]),\n",
    "  eps=1e-5)[0]\n",
    "#images = images / np.linalg.norm(images, axis=1, keepdims=True) # this will get done later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analyzer in analyzer_list:\n",
    "  if(analyzer.analysis_params.model_type.lower() != \"lca\"):\n",
    "    pre_images = np.stack([analyzer.neuron_vis_output[\"optimal_stims\"][target_id][-1].reshape(28,28)\n",
    "      for target_id in range(len(analyzer.analysis_params.neuron_vis_targets))], axis=0)\n",
    "    pre_image_fig = pf.plot_weights(pre_images, title=analyzer.model_name+\" pre-images\", figsize=(4,8))\n",
    "    pre_image_fig.savefig(analyzer.analysis_out_dir+\"/vis/pre_images.png\",\n",
    "        transparent=True, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analyzer in analyzer_list:\n",
    "  if(analyzer.analysis_params.model_type.lower() == \"lca\"):\n",
    "    available_indices = np.array(range(analyzer.model.get_num_latent()))\n",
    "  else:\n",
    "    available_indices = np.array([1,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_neuron_idx = 0\n",
    "step_idx = -1\n",
    "\n",
    "for analyzer in analyzer_list:\n",
    "  analyzer.bf_id0 = target_neuron_idx\n",
    "  if(analyzer.analysis_params.model_type.lower() == \"lca\"):\n",
    "    analyzer.bf0 = analyzer.bf_stats[\"basis_functions\"][analyzer.bf_id0]\n",
    "  else:  \n",
    "    analyzer.bf0 = analyzer.neuron_vis_output[\"optimal_stims\"][analyzer.bf_id0][step_idx]\n",
    "  analyzer.bf0 = analyzer.bf0.reshape(np.prod(analyzer.model.get_input_shape()[1:]))\n",
    "  analyzer.bf0 = analyzer.bf0 / np.linalg.norm(analyzer.bf0)\n",
    "  \n",
    "  fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "  \n",
    "  ax = pf.clear_axis(axes[0])\n",
    "  ax.imshow(analyzer.bf0.reshape(28, 28), cmap=\"Greys_r\")#, vmin=0.0, vmax=1.0)\n",
    "  ax.set_title(\"Optimal\\ninput image\")\n",
    "  \n",
    "  if(analyzer.analysis_params.model_type.lower() != \"lca\"):\n",
    "    axes[1].plot(analyzer.neuron_vis_output[\"loss\"][analyzer.bf_id0])\n",
    "    axes[1].set_title(\"Optimization loss\")\n",
    "  \n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_orth_vect(matrix):\n",
    "  rand_vect = np.random.rand(matrix.shape[0], 1)\n",
    "  new_matrix = np.hstack((matrix, rand_vect))\n",
    "  candidate_vect = np.zeros(matrix.shape[1]+1)\n",
    "  candidate_vect[-1] = 1\n",
    "  orth_vect = np.linalg.lstsq(new_matrix.T, candidate_vect, rcond=None)[0] # [0] indexes lst-sqrs solution\n",
    "  orth_vect = np.squeeze((orth_vect / np.linalg.norm(orth_vect)).T) \n",
    "  return orth_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rand_vectors(bf0, num_orth_directions):\n",
    "  rand_vectors = bf0.T[:,None] # matrix of alternate vectors\n",
    "  for orth_idx in range(num_orth_directions):\n",
    "    tmp_bf1 = find_orth_vect(rand_vectors)\n",
    "    rand_vectors = np.append(rand_vectors, tmp_bf1[:,None], axis=1)\n",
    "  return rand_vectors.T[1:, :] # [num_vectors, vector_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alt_vectors(bf0, bf1s):\n",
    "  alt_vectors = bf0.T[:,None] # matrix of alternate vectors\n",
    "  for tmp_bf1 in bf1s:\n",
    "    tmp_bf1 = np.squeeze((tmp_bf1 / np.linalg.norm(tmp_bf1)).T)\n",
    "    alt_vectors = np.append(alt_vectors, tmp_bf1[:,None], axis=1)\n",
    "  return alt_vectors.T[1:, :] # [num_vectors, vector_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_activity(analyzer, alt_vectors, num_imgs):\n",
    "  num_alt_vectors = alt_vectors.shape[0]\n",
    "  # Construct point dataset\n",
    "  x_pts = np.linspace(-0.5, 3.5, int(np.sqrt(num_imgs)))\n",
    "  y_pts = np.linspace(-2.0, 2.0, int(np.sqrt(num_imgs)))\n",
    "  X_mesh, Y_mesh = np.meshgrid(x_pts, y_pts)\n",
    "  proj_datapoints = np.stack([X_mesh.reshape(num_imgs), Y_mesh.reshape(num_imgs)], axis=1)\n",
    "  \n",
    "  out_dict = {\n",
    "    \"norm_activity\": [],\n",
    "    \"proj_neuron0\": [],\n",
    "    \"proj_neuron1\": [],\n",
    "    \"proj_v\": [],\n",
    "    \"v\": [],\n",
    "    \"proj_datapoints\": proj_datapoints,\n",
    "    \"X_mesh\": X_mesh,\n",
    "    \"Y_mesh\": Y_mesh}\n",
    "  \n",
    "  activation_list = []\n",
    "  for alt_vector_idx in range(num_alt_vectors):\n",
    "    alt_vector = alt_vectors[alt_vector_idx, :]\n",
    "    proj_matrix, v = dp.bf_projections(analyzer.bf0, alt_vector)\n",
    "    out_dict[\"proj_neuron0\"].append(np.dot(proj_matrix, analyzer.bf0).T) #project\n",
    "    out_dict[\"proj_neuron1\"].append(np.dot(proj_matrix, alt_vector).T) #project\n",
    "    out_dict[\"proj_v\"].append(np.dot(proj_matrix, v).T) #project\n",
    "    out_dict[\"v\"].append(v)\n",
    "    \n",
    "    datapoints = np.stack([np.dot(proj_matrix.T, proj_datapoints[data_id,:])\n",
    "      for data_id in range(num_imgs)], axis=0) #inject\n",
    "    datapoints = dp.reshape_data(datapoints, flatten=False)[0]\n",
    "    datapoints = {\"test\": Dataset(datapoints, lbls=None, ignore_lbls=None, rand_state=analyzer.rand_state)}\n",
    "    # preprocess_dataset should include rescaling to be between (0,1) for mnist\n",
    "    #datapoints = analyzer.model.preprocess_dataset(datapoints, params=analyzer.model_params)\n",
    "    datapoints = analyzer.model.reshape_dataset(datapoints, analyzer.model_params)\n",
    "    #datapoints[\"test\"].images = (\n",
    "    #  (datapoints[\"test\"].images - np.min(datapoints[\"test\"].images, axis=1, keepdims=True))\n",
    "    #  / (np.max(datapoints[\"test\"].images, axis=1, keepdims=True) - np.min(datapoints[\"test\"].images,\n",
    "    #  axis=1, keepdims=True)))\n",
    "    datapoints[\"test\"].images /= np.max(np.abs(datapoints[\"test\"].images))\n",
    "    #datapoints[\"test\"].images *= analyzer.analysis_params.input_scale*3\n",
    "    activations = analyzer.compute_activations(datapoints[\"test\"].images)#, batch_size=num_imgs//16)\n",
    "    activations = activations[:, analyzer.bf_id0]\n",
    "    activity_max = np.amax(np.abs(activations))\n",
    "    out_dict[\"norm_activity\"].append(activations / (activity_max + 0.00001)) # Rescale between -1 and 1\n",
    "  \n",
    "  return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analyzer = analyzer_list[0]\n",
    "step_idx = -1\n",
    "num_imgs = int(228**2)\n",
    "\n",
    "num_neurons = 3\n",
    "num_orth_directions = 3\n",
    "\n",
    "all_neuron_indices = np.random.choice(available_indices, num_orth_directions+num_neurons, replace=False)#range(analyzer.model.get_num_latent())\n",
    "#all_neuron_indices = np.random.choice(available_indices, num_neurons, replace=False)#range(analyzer.model.get_num_latent())\n",
    "\n",
    "target_neuron_indices = all_neuron_indices[:num_neurons]\n",
    "\n",
    "num_plots_y = num_neurons + 1 # extra dimension for example image\n",
    "num_plots_x = num_orth_directions + 1 # extra dimension for example image\n",
    "\n",
    "gs0 = gridspec.GridSpec(num_plots_y, num_plots_x, wspace=0.1, hspace=0.1)\n",
    "fig = plt.figure(figsize=(28, 28))\n",
    "cmap = plt.get_cmap('viridis')\n",
    "orth_vectors = []\n",
    "target_vectors = []\n",
    "\n",
    "for neuron_loop_index, neuron_id in enumerate(target_neuron_indices): # rows\n",
    "  # Get target neuron pre-image\n",
    "  analyzer.bf_id0 = neuron_id\n",
    "  \n",
    "  if(analyzer.analysis_params.model_type.lower() == \"lca\"):\n",
    "    analyzer.bf0 = analyzer.bf_stats[\"basis_functions\"][analyzer.bf_id0]\n",
    "  else:\n",
    "    analyzer.bf0 = analyzer.neuron_vis_output[\"optimal_stims\"][analyzer.bf_id0][step_idx] # shape=[28,28,1]\n",
    "\n",
    "  target_vectors.append(analyzer.bf0)\n",
    "  analyzer.bf0 = analyzer.bf0.reshape(np.prod(analyzer.model.get_input_shape()[1:])) # shape=[784]\n",
    "  #analyzer.bf0 = (analyzer.bf0 - np.min(analyzer.bf0)) / (np.max(analyzer.bf0) - np.min(analyzer.bf0)) # map to data range (0,1)\n",
    "  analyzer.bf0 = analyzer.bf0 / np.linalg.norm(analyzer.bf0) # normalize length\n",
    "\n",
    "  #TODO: I think this is wrong - don't need to compute every for neuron_idx inside loop\n",
    "  if orth_neuron_indices is None:\n",
    "    if(analyzer.analysis_params.model_type.lower() == \"lca\"):\n",
    "      min_angle = 50\n",
    "      max_angle = 80\n",
    "      vectors = np.squeeze(np.argwhere(np.logical_and(analyzer.neuron_angles[analyzer.bf_id0, :] < max_angle,\n",
    "        analyzer.neuron_angles[analyzer.bf_id0, :] > min_angle)))\n",
    "      vector_ids = np.random.choice(vectors, num_orth_directions, replace=False)\n",
    "      alt_stims =  [analyzer.bf_stats[\"basis_functions\"][vector_id].reshape(28*28)\n",
    "        for vector_id in vector_ids]\n",
    "      comparison_vectors = get_alt_vectors(analyzer.bf0, alt_stims)\n",
    "    else:\n",
    "      comparison_vectors = get_rand_vectors(analyzer.bf0, num_orth_directions)\n",
    "  else:\n",
    "    alt_stims =  [analyzer.neuron_vis_output[\"optimal_stims\"][orth_neuron_idx][step_idx]\n",
    "      for orth_neuron_idx in orth_neuron_indices]\n",
    "    comparison_vectors = get_alt_vectors(analyzer.bf0, alt_stims)\n",
    "\n",
    "  out_dict = get_norm_activity(analyzer, comparison_vectors, num_imgs)\n",
    "  analyzer.X_mesh = out_dict[\"X_mesh\"]\n",
    "  analyzer.Y_mesh = out_dict[\"Y_mesh\"]\n",
    "\n",
    "  for orth_loop_index in range(num_orth_directions): # columns\n",
    "      analyzer.norm_activity = out_dict[\"norm_activity\"][orth_loop_index]\n",
    "      analyzer.norm_activity = analyzer.norm_activity.reshape(int(np.sqrt(num_imgs)), int(np.sqrt(num_imgs)))\n",
    "      analyzer.proj_neuron0 = out_dict[\"proj_neuron0\"][orth_loop_index]\n",
    "      analyzer.proj_neuron1 = out_dict[\"proj_neuron1\"][orth_loop_index]\n",
    "      analyzer.proj_v = out_dict[\"proj_v\"][orth_loop_index]\n",
    "      orth_vectors.append(out_dict[\"v\"][orth_loop_index])\n",
    "      \n",
    "      curve_plot_y_idx = neuron_loop_index + 1\n",
    "      curve_plot_x_idx = orth_loop_index + 1\n",
    "      curve_ax = pf.clear_axis(fig.add_subplot(gs0[curve_plot_y_idx, curve_plot_x_idx]))\n",
    "\n",
    "      # NOTE: each subplot has a renormalized color scale\n",
    "      # TODO: Add scale bar like in the lca inference plots\n",
    "      vmin = np.floor(np.min(analyzer.norm_activity))\n",
    "      vmax = np.ceil(np.max(analyzer.norm_activity))\n",
    "\n",
    "      levels = 5\n",
    "      contsf = curve_ax.contourf(analyzer.X_mesh, analyzer.Y_mesh, analyzer.norm_activity,\n",
    "        levels=levels, vmin=vmin, vmax=vmax, alpha=1.0, antialiased=True, cmap=cmap)\n",
    "\n",
    "      curve_ax.arrow(0, 0, analyzer.proj_neuron0[0].item(), analyzer.proj_neuron0[1].item(),\n",
    "        width=0.05, head_width=0.15, head_length=0.15, fc='r', ec='r')\n",
    "      curve_ax.arrow(0, 0, analyzer.proj_neuron1[0].item(), analyzer.proj_neuron1[1].item(),\n",
    "        width=0.05, head_width=0.15, head_length=0.15, fc='w', ec='w')\n",
    "      curve_ax.arrow(0, 0, analyzer.proj_v[0].item(), analyzer.proj_v[1].item(),\n",
    "        width=0.05, head_width=0.15, head_length=0.15, fc='k', ec='k')\n",
    "\n",
    "      curve_ax.set_xlim([-0.5, 3.5])\n",
    "      curve_ax.set_ylim([-2, 2.0])\n",
    "      #curve_ax.set_aspect(\"equal\")\n",
    "\n",
    "for plot_y_id in range(num_plots_y):\n",
    "  for plot_x_id in range(num_plots_x):\n",
    "      if plot_y_id > 0 and plot_x_id == 0:\n",
    "        bf_ax = pf.clear_axis(fig.add_subplot(gs0[plot_y_id, plot_x_id]))\n",
    "        bf_ax.imshow(target_vectors[plot_y_id-1].reshape((28,28)), cmap=\"Greys_r\")\n",
    "        if plot_y_id == 1:\n",
    "          bf_ax.set_title(\"Target vectors\", color=\"r\", fontsize=16)\n",
    "      if plot_y_id == 0 and plot_x_id > 0:\n",
    "        #comparison_img = comparison_vectors[plot_x_id-1, :].reshape(28,28)\n",
    "        orth_img = orth_vectors[plot_x_id-1].reshape(28,28)\n",
    "        orth_ax = pf.clear_axis(fig.add_subplot(gs0[plot_y_id, plot_x_id]))\n",
    "        orth_ax.imshow(orth_img, cmap=\"Greys_r\")\n",
    "        if plot_x_id == 1:\n",
    "          #orth_ax.set_ylabel(\"Orthogonal vectors\", color=\"k\", fontsize=16)\n",
    "          orth_ax.set_title(\"Orthogonal vectors\", color=\"k\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* Compute a unit vector that is in the same plane as a given basis function pair (B1,B2) and is orthogonal to B1, where B1 is the target basis for comparison and B2 is selected from all other bases.\n",
    "* Construct a line of data points in this plane\n",
    "* Project the data points into image space, compute activations, plot activations\n",
    "\"\"\"\n",
    "for analyzer in analyzer_list:\n",
    "  analyzer.pop_num_imgs = 100\n",
    "  \n",
    "  orthogonal_list = [idx for idx in range(analyzer.bf_stats[\"num_outputs\"])]\n",
    "  num_orthogonal = len(orthogonal_list)\n",
    "  \n",
    "  pop_x_pts = np.linspace(-2.0, 2.0, int(analyzer.pop_num_imgs))\n",
    "  pop_y_pts = np.linspace(-2.0, 2.0, int(analyzer.pop_num_imgs))\n",
    "  pop_X, pop_Y = np.meshgrid(pop_x_pts, pop_y_pts)\n",
    "  pop_proj_datapoints = np.stack([pop_X.reshape(analyzer.pop_num_imgs**2), pop_Y.reshape(analyzer.pop_num_imgs**2)], axis=1) # construct a grid\n",
    "  \n",
    "  #x_target = pop_x_pts[int(6*analyzer.pop_num_imgs/8)] # find a location to take a slice\n",
    "  x_target = pop_x_pts[int(0.75*analyzer.pop_num_imgs)] # find a location to take a slice\n",
    "  \n",
    "  slice_indices = np.where(pop_proj_datapoints[:,0]==x_target)[0]\n",
    "  analyzer.pop_proj_datapoints = pop_proj_datapoints[slice_indices,:] # slice grid\n",
    "  \n",
    "  pop_datapoints = [None,]*num_orthogonal\n",
    "  #pop_proj_neurons = [None,]*num_orthogonal\n",
    "  for pop_idx, tmp_bf_id1 in enumerate(orthogonal_list):\n",
    "    tmp_bf1 = analyzer.bf_stats[\"basis_functions\"][tmp_bf_id1].reshape((analyzer.model_params.num_pixels))\n",
    "    tmp_bf1 /= np.linalg.norm(tmp_bf1)\n",
    "    tmp_proj_matrix, v = analyzer.bf_projections(analyzer.bf0, tmp_bf1) \n",
    "    pop_datapoints[pop_idx] = np.dot(pop_proj_datapoints, tmp_proj_matrix)#[slice_indices,:]\n",
    "  \n",
    "  pop_datapoints = np.reshape(np.stack(pop_datapoints, axis=0),\n",
    "    [num_orthogonal*analyzer.pop_num_imgs, analyzer.model_params.num_pixels])\n",
    "  \n",
    "  pop_datapoints = dp.reshape_data(pop_datapoints, flatten=False)[0]\n",
    "  analyzer.pop_datapoints = {\"test\": Dataset(pop_datapoints, lbls=None, ignore_lbls=None, rand_state=analyzer.rand_state)}\n",
    "  analyzer.pop_datapoints = analyzer.model.preprocess_dataset(analyzer.pop_datapoints,\n",
    "    params={\"whiten_data\":analyzer.model_params.whiten_data,\n",
    "    \"whiten_method\":analyzer.model_params.whiten_method})\n",
    "  analyzer.pop_datapoints = analyzer.model.reshape_dataset(analyzer.pop_datapoints, analyzer.model_params)\n",
    "  analyzer.pop_datapoints[\"test\"].images /= np.max(np.abs(analyzer.pop_datapoints[\"test\"].images))\n",
    "  analyzer.pop_datapoints[\"test\"].images *= analyzer.analysis_params.input_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analyzer in analyzer_list:\n",
    "  pop_activations = analyzer.compute_activations(analyzer.pop_datapoints[\"test\"].images)[:, analyzer.bf_id0]\n",
    "  pop_activations = pop_activations.reshape([num_orthogonal, pop_num_imgs])\n",
    "  analyzer.pop_norm_activity = pop_activations / np.amax(np.abs(pop_activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* Construct the set of unit-length bases that are orthogonal to B0 (there should be B0.size-1 of them)\n",
    "* Construct a line of data points in each plane defined by B0 and a given orthogonal basis\n",
    "* Project the data points into image space, compute activations, plot activations\n",
    "\"\"\"\n",
    "for analyzer in analyzer_list:\n",
    "  analyzer.rand_pop_num_imgs = 100\n",
    "  analyzer.rand_num_orthogonal = analyzer.bf_stats[\"num_inputs\"]-1\n",
    "  \n",
    "  pop_x_pts = np.linspace(-2.0, 2.0, int(analyzer.rand_pop_num_imgs))\n",
    "  pop_y_pts = np.linspace(-2.0, 2.0, int(analyzer.rand_pop_num_imgs))\n",
    "  pop_X, pop_Y = np.meshgrid(pop_x_pts, pop_y_pts)\n",
    "  analyzer.rand_pop_proj_datapoints = np.stack([analyzer.pop_X.reshape(analyzer.rand_pop_num_imgs**2),\n",
    "    analyzer.pop_Y.reshape(analyzer.rand_pop_num_imgs**2)], axis=1) # construct a grid\n",
    "  \n",
    "  #x_target = pop_x_pts[int(6*analyzer.rand_pop_num_imgs/8)] # find a location to take a slice\n",
    "  x_target = pop_x_pts[int(0.25*analyzer.rand_pop_num_imgs)] # find a location to take a slice\n",
    "  \n",
    "  slice_indices = np.where(analyzer.rand_pop_proj_datapoints[:,0]==x_target)[0]\n",
    "  analyzer.rand_pop_proj_datapoints = analyzer.rand_pop_proj_datapoints[slice_indices,:] # slice grid\n",
    "  \n",
    "  analyzer.rand_pop_datapoints = [None,]*analyzer.rand_num_orthogonal\n",
    "  for pop_idx in range(analyzer.rand_num_orthogonal):\n",
    "    v = orth_col_matrix[:, pop_idx]\n",
    "    tmp_proj_matrix = np.stack([analyzer.bf0, v], axis=0)\n",
    "    analyzer.rand_pop_datapoints[pop_idx] = np.dot(analyzer.rand_pop_proj_datapoints,\n",
    "      tmp_proj_matrix)#[slice_indices,:]\n",
    "\n",
    "  analyzer.rand_pop_datapoints = np.reshape(np.stack(analyzer.rand_pop_datapoints, axis=0),\n",
    "    [analyzer.rand_num_orthogonal*analyzer.rand_pop_num_imgs, analyzer.model_params.num_pixels])\n",
    "\n",
    "  analyzer.rand_pop_datapoints = dp.reshape_data(analyzer.rand_pop_datapoints, flatten=False)[0]\n",
    "  analyzer.rand_pop_datapoints = {\"test\": Dataset(analyzer.rand_pop_datapoints, lbls=None, ignore_lbls=None, rand_state=analyzer.rand_state)}\n",
    "  analyzer.rand_pop_datapoints = analyzer.model.preprocess_dataset(analyzer.rand_pop_datapoints,\n",
    "    params={\"whiten_data\":False, \"whiten_method\":None})\n",
    "  analyzer.rand_pop_datapoints = analyzer.model.reshape_dataset(analyzer.rand_pop_datapoints, analyzer.model_params)\n",
    "  analyzer.rand_pop_datapoints[\"test\"].images /= np.max(np.abs(analyzer.rand_pop_datapoints[\"test\"].images))\n",
    "  analyzer.rand_pop_datapoints[\"test\"].images *= analyzer.analysis_params.input_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.analysis_params.input_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analyzer in analyzer_list:\n",
    "  rand_pop_activations = analyzer.compute_activations(analyzer.rand_pop_datapoints[\"test\"].images)[:, analyzer.bf_id0]\n",
    "  rand_pop_activations = rand_pop_activations.reshape([analyzer.rand_num_orthogonal, analyzer.rand_pop_num_imgs])\n",
    "  analyzer.rand_pop_norm_activity = rand_pop_activations / np.amax(np.abs(rand_pop_activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(analyzer.norm_activity))\n",
    "print(np.max(analyzer.norm_activity))\n",
    "print(analyzer.norm_activity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analyzer in analyzer_list:\n",
    "  analyzer.bf_coeffs = [\n",
    "    np.polynomial.polynomial.polyfit(analyzer.pop_proj_datapoints[:,1], analyzer.pop_norm_activity[orthog_idx,:], deg=2)\n",
    "    for orthog_idx in range(num_orthogonal)]\n",
    "  analyzer.bf_fits = [\n",
    "    np.polynomial.polynomial.polyval(analyzer.pop_proj_datapoints[:,1], coeff)\n",
    "    for coeff in analyzer.bf_coeffs]\n",
    "  analyzer.bf_curvatures = [np.polyder(fit, m=2) for fit in analyzer.bf_fits]\n",
    "  \n",
    "  analyzer.rand_coeffs = [np.polynomial.polynomial.polyfit(analyzer.rand_pop_proj_datapoints[:,1],\n",
    "    analyzer.rand_pop_norm_activity[orthog_idx,:], deg=2) for orthog_idx in range(analyzer.rand_num_orthogonal)]\n",
    "  analyzer.rand_fits = [np.polynomial.polynomial.polyval(analyzer.rand_pop_proj_datapoints[:,1], coeff)\n",
    "    for coeff in analyzer.rand_coeffs]\n",
    "  analyzer.rand_curvatures = [np.polyder(fit, m=2) for fit in analyzer.rand_fits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_idx = 0\n",
    "\n",
    "bf_curvatures = np.stack(analyer_list[analyzer_idx].bf_coeffs, axis=0)[:,2]\n",
    "rand_curvatures = np.stack(analyzer_list[analyzer_idx].rand_coeffs, axis=0)[:,2]\n",
    "\n",
    "num_bins = 100\n",
    "bins = np.linspace(-0.2, 0.2, num_bins)\n",
    "bf_hist, bin_edges = np.histogram(bf_curvatures.flatten(), bins)\n",
    "rand_hist, _ = np.histogram(rand_curvatures.flatten(), bins)\n",
    "bin_left, bin_right = bin_edges[:-1], bin_edges[1:]\n",
    "bin_centers = bin_left + (bin_right - bin_left)/2\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(16,9))\n",
    "\n",
    "ax.bar(bin_centers, rand_hist, width=0.0022, log=False, color=\"g\", alpha=0.5, align=\"center\", label=\"Random Projection\")\n",
    "ax.bar(bin_centers, bf_hist, width=0.0022, log=False, color=\"r\", alpha=0.5, align=\"center\", label=\"BF Projection\")\n",
    "\n",
    "ax.set_xticks(bin_left, minor=True)\n",
    "ax.set_xticks(bin_left[::15], minor=False)\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter(\"%0.3f\"))\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "  tick.label.set_fontsize(24) \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "  tick.label.set_fontsize(24) \n",
    "\n",
    "ax.set_title(\"Histogram of Curvatures\", fontsize=32)\n",
    "ax.set_xlabel(\"Curvature\", fontsize=32)\n",
    "ax.set_ylabel(\"Count\", fontsize=32)\n",
    "ax.legend(loc=2, fontsize=32)\n",
    "fig.savefig(analyzer.analysis_out_dir+\"/vis/histogram_of_curvatures_bf0id\"+str(analyzer.bf_id0)+\".png\",\n",
    "  transparent=True, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for analyzer in analyzer_list:\n",
    "  analyzer.orth_col_matrix = analyzer.bf0.T[:,None]\n",
    "  analyzer.rand_num_orthogonal = np.prod(analyzer.model.get_input_shape()[1:])-1\n",
    "\n",
    "  for pop_idx in range(analyzer.rand_num_orthogonal):\n",
    "    v = find_orth(analyzer.orth_col_matrix)\n",
    "    analyzer.orth_col_matrix = np.append(analyzer.orth_col_matrix, v[:,None], axis=1)\n",
    "\n",
    "  if all(np.abs(np.dot(analyzer.bf0, col)) < 1e-9 for col in analyzer.orth_col_matrix[:,1:].T):\n",
    "    print(\"Success\")\n",
    "  else:\n",
    "    count = np.sum([int(np.abs(np.dot(analyzer.bf0, col)) < 1e-9) for col in analyzer.orth_col_matrix[:,1:].T])\n",
    "    print(\"Failure,\", count, \"were non-orthogonal\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_rand_orthogonal = np.prod(analyzer.model.get_input_shape()[1:])\n",
    "num_imgs = int(228**2)\n",
    "set_norm_activity(\n",
    "  analyzer=analyzer_list[0],\n",
    "  target_neuron_idx=0,#np.random.choice(range(analyzer.model.get_num_latent()), 1),\n",
    "  num_rand_orthogonal=num_rand_orthogonal,\n",
    "  orthogonal_idx=np.random.choice(range(1, num_rand_orthogonal), 1),\n",
    "  num_imgs=num_imgs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for analyzer in analyzer_list:\n",
    "  num_plots_y = 1\n",
    "  num_plots_x = 2\n",
    "  gs1 = gridspec.GridSpec(num_plots_y, num_plots_x, wspace=0.3, width_ratios=[4, 1])\n",
    "  fig = plt.figure(figsize=(6,6))\n",
    "  curve_ax = pf.clear_axis(fig.add_subplot(gs1[0]))\n",
    "  #cmap = plt.get_cmap('tab20b')\n",
    "  cmap = plt.get_cmap('viridis')\n",
    "  vmin = np.floor(np.min(analyzer.norm_activity))#0.0\n",
    "  vmax = np.ceil(np.max(analyzer.norm_activity))#1.0\n",
    "  \n",
    "  #name_suffix = \"continuous\"\n",
    "  #pts = curve_ax.scatter(analyzer.proj_datapoints[:,0], analyzer.proj_datapoints[:,1],\n",
    "  #  vmin=vmin, vmax=vmax, cmap=cmap, alpha=0.5, c=analyzer.norm_activity[:, analyzer.bf_id0], s=5.0)\n",
    "  \n",
    "  norm_activity = analyzer.norm_activity[:, analyzer.bf_id0]\n",
    "  norm_activity = norm_activity.reshape(int(np.sqrt(num_imgs)), int(np.sqrt(num_imgs)))\n",
    "  \n",
    "  levels = 5\n",
    "  name_suffix = \"\"\n",
    "  contsf = curve_ax.contourf(analyzer.X_mesh, analyzer.Y_mesh, norm_activity,\n",
    "    levels=levels, vmin=vmin, vmax=vmax, alpha=1.0, antialiased=True, cmap=cmap)\n",
    "  \n",
    "  curve_ax.arrow(0, 0, analyzer.proj_neuron0[0].item(), analyzer.proj_neuron0[1].item(),\n",
    "    width=0.05, head_width=0.15, head_length=0.15, fc='r', ec='r')\n",
    "  curve_ax.arrow(0, 0, analyzer.proj_neuron1[0].item(), analyzer.proj_neuron1[1].item(),\n",
    "    width=0.05, head_width=0.15, head_length=0.15, fc='w', ec='k')\n",
    "  \n",
    "  curve_ax.set_ylim([-2, 2.0])\n",
    "  curve_ax.set_xlim([-2, 2.0])\n",
    "  curve_ax.set_aspect(\"equal\")\n",
    "  curve_ax.set_title(\"Neuron ID \"+str(analyzer.bf_id0), fontsize=16)\n",
    "  \n",
    "  gs2 = gridspec.GridSpecFromSubplotSpec(2, 1, gs1[1], hspace=-0.5)\n",
    "  bf1_ax = pf.clear_axis(fig.add_subplot(gs2[0]))\n",
    "  bf1_ax.imshow(analyzer.neuron_vis_output[\"optimal_stims\"][analyzer.bf_id0][step_idx].reshape((28,28)),\n",
    "    cmap=\"Greys_r\")\n",
    "  bf1_ax.set_title(\"Primary\\n Stimulus\", color='r', fontsize=16)\n",
    "  \n",
    "  activity_ax = fig.add_subplot(gs2[1])\n",
    "  activity_ax.plot(norm_activity[0,:], color='k')\n",
    "  activity_ax.set_aspect(1.0/activity_ax.get_data_ratio())\n",
    "  activity_ax.set_title(\"Activity along\\nstimulus vector\")\n",
    "  \n",
    "  fig.savefig(analyzer.analysis_out_dir+\"/vis/neuron_response_contours_bf0id\"+str(analyzer.bf_id0)+name_suffix+\".png\",\n",
    "    transparent=True, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def set_norm_activity(analyzer, target_neuron_idx, num_orth_directions,\n",
    "  orthogonal_idx, num_imgs, orth_neuron_indices=None):\n",
    "\n",
    "  # Get alt vectors\n",
    "  #for orth_idx in range(num_orth_directions):\n",
    "  #  if alt_neuron_indices is None: # find random orthogonal indices\n",
    "  #    tmp_bf1 = find_orth_vect(alt_vectors)\n",
    "  #  else: # use other pre-images as starting point for orthogonal indices\n",
    "  #    tmp_bf1 = analyzer.neuron_vis_output[\"optimal_stims\"][alt_neuron_indices[pop_idx]][step_idx]\n",
    "  #    tmp_bf1 /= np.linalg.norm(tmp_bf1)\n",
    "  #  alt_vectors = np.append(alt_vectors, tmp_bf1[:,None], axis=1)\n",
    "\n",
    "  # Construct point dataset\n",
    "  x_pts = np.linspace(-0.5, 5.0, int(np.sqrt(num_imgs)))\n",
    "  y_pts = np.linspace(-2.0, 2.0, int(np.sqrt(num_imgs)))\n",
    "  analyzer.X_mesh, analyzer.Y_mesh = np.meshgrid(x_pts, y_pts)\n",
    "  analyzer.proj_datapoints = np.stack([analyzer.X_mesh.reshape(num_imgs),\n",
    "    analyzer.Y_mesh.reshape(num_imgs)], axis=1)\n",
    "  \n",
    "  # Compute projection matrix to inject the point dataset into image space\n",
    "  proj_matrix, v = analyzer.bf_projections(analyzer.bf0, np.squeeze(alt_vectors[:, orthogonal_idx]))\n",
    "  analyzer.proj_neuron0 = np.dot(proj_matrix, analyzer.bf0).T\n",
    "  analyzer.proj_neuron1 = np.dot(proj_matrix, np.squeeze(alt_vectors[:, orthogonal_idx])).T\n",
    "  \n",
    "  orth_datapoints = None#np.stack([np.dot(proj_matrix.T, v)])\n",
    "    #for orth_id in range(num_orth_directions)], axis=0)\n",
    "  \n",
    "  datapoints = np.stack([np.dot(proj_matrix.T, analyzer.proj_datapoints[data_id,:])\n",
    "    for data_id in range(num_imgs)]) #inject\n",
    "  datapoints, orig_shape = dp.reshape_data(datapoints, flatten=False)[:2]\n",
    "  datapoints = {\"test\": Dataset(datapoints, lbls=None,\n",
    "    ignore_lbls=None, rand_state=analyzer.rand_state)}\n",
    "  #params={\"whiten_data\":analyzer.model_params.whiten_data}\n",
    "  #if params[\"whiten_data\"]:\n",
    "  #  params[\"whiten_method\"] = analyzer.model_params.whiten_method\n",
    "  #datapoints = analyzer.model.preprocess_dataset(datapoints, params=params)\n",
    "  datapoints = analyzer.model.reshape_dataset(datapoints, analyzer.model_params)\n",
    "  datapoints[\"test\"].images = (\n",
    "    (datapoints[\"test\"].images - np.min(datapoints[\"test\"].images, axis=1, keepdims=True))\n",
    "    / (np.max(datapoints[\"test\"].images, axis=1, keepdims=True) - np.min(datapoints[\"test\"].images, axis=1, keepdims=True)))\n",
    "  #datapoints[\"test\"].images /= np.max(np.abs(datapoints[\"test\"].images))\n",
    "  #datapoints[\"test\"].images *= analyzer.analysis_params.input_scale*3\n",
    "  \n",
    "  activations = analyzer.compute_activations(datapoints[\"test\"].images)#, batch_size=num_imgs//16)\n",
    "  activity_max = np.amax(np.abs(activations))\n",
    "  analyzer.norm_activity = activations / (activity_max + 0.00001) # Rescale between -1 and 1\n",
    "  \n",
    "  return orth_datapoints"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_norm_activity(analyzer, bf0, alt_vectors, num_imgs):\n",
    "  # Construct point dataset\n",
    "  x_pts = np.linspace(-0.5, 5.0, int(np.sqrt(num_imgs)))\n",
    "  y_pts = np.linspace(-2.0, 2.0, int(np.sqrt(num_imgs)))\n",
    "  X_mesh, Y_mesh = np.meshgrid(x_pts, y_pts)\n",
    "  proj_datapoints = np.stack([X_mesh.reshape(num_imgs), Y_mesh.reshape(num_imgs)], axis=1)\n",
    "  \n",
    "  out_dict = {\n",
    "    \"norm_activity\": [],\n",
    "    \"proj_neuron0\": [],\n",
    "    \"proj_neuron1\": [],\n",
    "    \"proj_v\": [],\n",
    "    \"proj_datapoints\": proj_datapoints,\n",
    "    \"X_mesh\": X_mesh,\n",
    "    \"Y_mesh\": Y_mesh}\n",
    "\n",
    "  for alt_vector_idx in range(alt_vectors.shape[1]):\n",
    "    alt_vector = alt_vectors[:, alt_vector_idx]\n",
    "    proj_matrix, v = dp.bf_projections(bf0, alt_vector)\n",
    "    out_dict[\"proj_neuron0\"].append(np.dot(proj_matrix, bf0).T) #project\n",
    "    out_dict[\"proj_neuron1\"].append(np.dot(proj_matrix, alt_vector).T) #project\n",
    "    out_dict[\"proj_v\"].append(np.dot(proj_matrix, v).T) #project\n",
    "    \n",
    "    datapoints = np.stack([np.dot(proj_matrix.T, proj_datapoints[data_id,:])\n",
    "      for data_id in range(num_imgs)]) #inject\n",
    "    datapoints = dp.reshape_data(datapoints, flatten=False)[0]\n",
    "    datapoints = {\"test\": Dataset(datapoints, lbls=None, ignore_lbls=None,\n",
    "      rand_state=analyzer.rand_state)}\n",
    "    \n",
    "    # preprocess_dataset should include rescaling to be between (0,1) for mnist\n",
    "    params={\"whiten_data\":analyzer.model_params.whiten_data}\n",
    "    if params[\"whiten_data\"]:\n",
    "      params[\"whiten_method\"] = analyzer.model_params.whiten_method\n",
    "    datapoints = analyzer.model.preprocess_dataset(datapoints, params=params)\n",
    "    datapoints = analyzer.model.reshape_dataset(datapoints, analyzer.model_params)\n",
    "    datapoints[\"test\"].images = (\n",
    "      (datapoints[\"test\"].images - np.min(datapoints[\"test\"].images, axis=1, keepdims=True))\n",
    "      / (np.max(datapoints[\"test\"].images, axis=1, keepdims=True) - np.min(datapoints[\"test\"].images, axis=1, keepdims=True)))\n",
    "    #datapoints[\"test\"].images /= np.max(np.abs(datapoints[\"test\"].images))\n",
    "    #datapoints[\"test\"].images *= analyzer.analysis_params.input_scale*3\n",
    "    \n",
    "    activations = analyzer.compute_activations(datapoints[\"test\"].images)#, batch_size=num_imgs//16)\n",
    "    activity_max = np.amax(np.abs(activations))\n",
    "    out_dict[\"norm_activity\"].append(activations / (activity_max + 0.00001)) # Rescale between -1 and 1\n",
    "  \n",
    "  return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
