{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import tensorflow as tf\n",
    "from data.dataset import Dataset\n",
    "import data.data_selector as ds\n",
    "import utils.data_processing as dp\n",
    "import utils.plot_functions as pf\n",
    "import analysis.analysis_picker as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lambda_params(object):\n",
    "  def __init__(self, lamb=None):\n",
    "    self.model_type = \"lambda\"\n",
    "    self.model_name = \"lambda_mnist\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "    self.activation_function = lamb\n",
    "\n",
    "class mlp_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"mlp\"\n",
    "    self.model_name = \"mlp_mnist\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_512_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_512_vh\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_768_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_768_mnist\"\n",
    "    self.version = \"0.0\"\n",
    "    #self.save_info = \"analysis_train_carlini_targeted\" # for vh\n",
    "    self.save_info = \"analysis_test_carlini_targeted\" # for mnist\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_1024_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_1024_vh\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "    \n",
    "class lca_1536_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_1536_mnist\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class ae_deep_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"ae\"\n",
    "    self.model_name = \"ae_deep_mnist\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = lambda x : tf.reduce_sum(tf.square(x), axis=1, keepdims=True)\n",
    "#lamb = lambda x : x / tf.reduce_sum(tf.square(x), axis=1, keepdims=True)\n",
    "\n",
    "params_list = [ae_deep_params()]#lca_768_params(), lca_1536_params()]\n",
    "\n",
    "for params in params_list:\n",
    "  params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+params.model_name)\n",
    "\n",
    "analyzer_list = [ap.get_analyzer(params.model_type) for params in params_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analyzer, params in zip(analyzer_list, params_list):\n",
    "  analyzer.setup(params)\n",
    "  if(hasattr(params, \"activation_function\")):\n",
    "    analyzer.model_params.activation_function = params.activation_function\n",
    "  analyzer.setup_model(analyzer.model_params)\n",
    "  analyzer.load_analysis(save_info=params.save_info)\n",
    "  analyzer.model_name = params.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analyzer in analyzer_list:\n",
    "  if(analyzer.analysis_params.model_type.lower() != \"lca\"\n",
    "    and analyzer.analysis_params.model_type.lower() != \"lambda\"):\n",
    "    pre_images = np.stack([analyzer.neuron_vis_output[\"optimal_stims\"][target_id][-1].reshape(28,28)\n",
    "      for target_id in range(len(analyzer.analysis_params.neuron_vis_targets))], axis=0)\n",
    "    pre_image_fig = pf.plot_weights(pre_images, title=analyzer.model_name+\" pre-images\", figsize=(4,8))\n",
    "    pre_image_fig.savefig(analyzer.analysis_out_dir+\"/vis/pre_images.png\", transparent=True,\n",
    "      bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#available_indices = [  30,   45,  101,  223,  283,  335,  388,  491,  558,  571,  572,\n",
    "#        590,  599,  606,  619,  629,  641,  652,  693,  722,  724,  749,\n",
    "#        769,  787,  812,  819,  824,  906,  914,  927,  987, 1134, 1186,\n",
    "#       1196, 1297, 1376, 1409, 1534]\n",
    "#available_indices = np.array(range(analyzer.model.get_num_latent()))\n",
    "available_indices = [2, 6, 8, 18, 21, 26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step_idx = -1\n",
    "\n",
    "for analyzer in analyzer_list:\n",
    "  analyzer.available_indices = available_indices#np.array(range(analyzer.model.get_num_latent()))\n",
    "  analyzer.target_neuron_idx = analyzer.available_indices[0]\n",
    "  if(analyzer.analysis_params.model_type.lower() == \"lca\"):\n",
    "    bf0 = analyzer.bf_stats[\"basis_functions\"][analyzer.target_neuron_idx]\n",
    "  else:  \n",
    "    bf0 = analyzer.neuron_vis_output[\"optimal_stims\"][analyzer.target_neuron_idx][step_idx]\n",
    "  bf0 = bf0.reshape(np.prod(analyzer.model.get_input_shape()[1:]))\n",
    "  bf0 = bf0 / np.linalg.norm(bf0)\n",
    "  \n",
    "  fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "  \n",
    "  ax = pf.clear_axis(axes[0])\n",
    "  ax.imshow(bf0.reshape(int(np.sqrt(bf0.size)), int(np.sqrt(bf0.size))), cmap=\"Greys_r\")#, vmin=0.0, vmax=1.0)\n",
    "  ax.set_title(\"Optimal\\ninput image\")\n",
    "  \n",
    "  if(analyzer.analysis_params.model_type.lower() != \"lca\"):\n",
    "    axes[1].plot(analyzer.neuron_vis_output[\"loss\"][analyzer.target_neuron_idx])\n",
    "    axes[1].set_title(\"Optimization loss\")\n",
    "  \n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_orth_vect(matrix):\n",
    "  rand_vect = np.random.rand(matrix.shape[0], 1)\n",
    "  new_matrix = np.hstack((matrix, rand_vect))\n",
    "  candidate_vect = np.zeros(matrix.shape[1]+1)\n",
    "  candidate_vect[-1] = 1\n",
    "  orth_vect = np.linalg.lstsq(new_matrix.T, candidate_vect, rcond=None)[0] # [0] indexes lst-sqrs solution\n",
    "  orth_vect = np.squeeze((orth_vect / np.linalg.norm(orth_vect)).T) \n",
    "  return orth_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rand_vectors(bf0, num_orth_directions):\n",
    "  rand_vectors = bf0.T[:,None] # matrix of alternate vectors\n",
    "  for orth_idx in range(num_orth_directions):\n",
    "    tmp_bf1 = find_orth_vect(rand_vectors)\n",
    "    rand_vectors = np.append(rand_vectors, tmp_bf1[:,None], axis=1)\n",
    "  return rand_vectors.T[1:, :] # [num_vectors, vector_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alt_vectors(bf0, bf1s):\n",
    "  alt_vectors = bf0.T[:,None] # matrix of alternate vectors\n",
    "  for tmp_bf1 in bf1s:\n",
    "    tmp_bf1 = np.squeeze((tmp_bf1 / np.linalg.norm(tmp_bf1)).T)\n",
    "    alt_vectors = np.append(alt_vectors, tmp_bf1[:,None], axis=1)\n",
    "  return alt_vectors.T[1:, :] # [num_vectors, vector_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_activity(analyzer, neuron_id_list, stim0_list, stim1_list, num_imgs):\n",
    "  # Construct point dataset\n",
    "  \n",
    "  #x_pts = np.linspace(-0.5, 19.5, int(np.sqrt(num_imgs)))\n",
    "  #y_pts = np.linspace(-10.0, 10.0, int(np.sqrt(num_imgs)))\n",
    "  x_pts = np.linspace(-0.5, 3.5, int(np.sqrt(num_imgs)))\n",
    "  y_pts = np.linspace(-2.0, 2.0, int(np.sqrt(num_imgs)))\n",
    "  #x_pts = np.linspace(0.9, 1.1, int(np.sqrt(num_imgs)))\n",
    "  #y_pts = np.linspace(-0.1, 0.1, int(np.sqrt(num_imgs)))\n",
    "  #x_pts = np.linspace(0.999, 1.001, int(np.sqrt(num_imgs)))\n",
    "  #y_pts = np.linspace(-0.001, 0.001, int(np.sqrt(num_imgs)))\n",
    "  \n",
    "  X_mesh, Y_mesh = np.meshgrid(x_pts, y_pts)\n",
    "  proj_datapoints = np.stack([X_mesh.reshape(num_imgs), Y_mesh.reshape(num_imgs)], axis=1)\n",
    "\n",
    "  out_dict = {\n",
    "    \"norm_activity\": [],\n",
    "    \"proj_neuron0\": [],\n",
    "    \"proj_neuron1\": [],\n",
    "    \"proj_v\": [],\n",
    "    \"v\": [],\n",
    "    \"proj_datapoints\": proj_datapoints,\n",
    "    \"X_mesh\": X_mesh,\n",
    "    \"Y_mesh\": Y_mesh}\n",
    "\n",
    "  # TODO: This can be made to be much faster by compiling all of the stimulus into a single set and computing activations\n",
    "  for neuron_id, stim0 in zip(neuron_id_list, stim0_list):\n",
    "    activity_sub_list = []\n",
    "    proj_neuron0_sub_list = []\n",
    "    proj_neuron1_sub_list = []\n",
    "    proj_v_sub_list = []\n",
    "    v_sub_list = []\n",
    "    for stim1 in stim1_list:\n",
    "      proj_matrix, v = dp.bf_projections(stim0, stim1)\n",
    "      proj_neuron0_sub_list.append(np.dot(proj_matrix, stim0).T) #project\n",
    "      proj_neuron1_sub_list.append(np.dot(proj_matrix, stim1).T) #project\n",
    "      proj_v_sub_list.append(np.dot(proj_matrix, v).T) #project\n",
    "      v_sub_list.append(v)\n",
    "      datapoints = np.stack([np.dot(proj_matrix.T, proj_datapoints[data_id,:])\n",
    "        for data_id in range(num_imgs)], axis=0) #inject\n",
    "      datapoints = dp.reshape_data(datapoints, flatten=False)[0]\n",
    "      datapoints = {\"test\": Dataset(datapoints, lbls=None, ignore_lbls=None, rand_state=analyzer.rand_state)}\n",
    "      datapoints = analyzer.model.reshape_dataset(datapoints, analyzer.model_params)\n",
    "      activations = analyzer.compute_activations(datapoints[\"test\"].images)#, batch_size=int(np.sqrt(num_imgs)))\n",
    "      activations = activations[:, neuron_id]\n",
    "      activity_max = np.amax(np.abs(activations))\n",
    "      activations = activations / (activity_max + 0.00001)\n",
    "      activations = activations.reshape(int(np.sqrt(num_imgs)), int(np.sqrt(num_imgs)))\n",
    "      activity_sub_list.append(activations)\n",
    "    out_dict[\"norm_activity\"].append(activity_sub_list)\n",
    "    out_dict[\"proj_neuron0\"].append(proj_neuron0_sub_list)\n",
    "    out_dict[\"proj_neuron1\"].append(proj_neuron1_sub_list)\n",
    "    out_dict[\"proj_v\"].append(proj_v_sub_list)\n",
    "    out_dict[\"v\"].append(v_sub_list)\n",
    "  return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = analyzer_list[0]\n",
    "step_idx = -1\n",
    "num_imgs = int(300**2)#int(228**2)\n",
    "min_angle = 10\n",
    "use_rand_orth = False\n",
    "\n",
    "num_neurons = 2#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(use_rand_orth):\n",
    "  target_neuron_indices = np.random.choice(analyzer.available_indices, num_neurons, replace=False)\n",
    "  alt_stim_list = get_rand_vectors(stim0, num_neurons)\n",
    "else:\n",
    "  if(analyzer.analysis_params.model_type.lower() == \"lca\"):\n",
    "    target_neuron_indices = np.random.choice(analyzer.available_indices, num_neurons, replace=False)\n",
    "    analyzer.neuron_angles = analyzer.get_neuron_angles(analyzer.bf_stats)[1] * (180/np.pi)\n",
    "    alt_stim_list = []\n",
    "  else:\n",
    "    all_neuron_indices = np.random.choice(analyzer.available_indices, 2*num_neurons, replace=False)\n",
    "    target_neuron_indices = all_neuron_indices[:num_neurons]\n",
    "    orth_neuron_indices = all_neuron_indices[num_neurons:]\n",
    "\n",
    "if(analyzer.analysis_params.model_type.lower() == \"ae\"):\n",
    "  neuron_vis_targets = np.array(analyzer.analysis_params.neuron_vis_targets)\n",
    "  neuron_id_list = neuron_vis_targets[target_neuron_indices]\n",
    "else:\n",
    "  neuron_id_list = target_neuron_indices\n",
    "stim0_list = []\n",
    "stimid0_list = []\n",
    "\n",
    "for neuron_id in target_neuron_indices:\n",
    "  if(analyzer.analysis_params.model_type.lower() == \"lca\"):\n",
    "    stim0 = analyzer.bf_stats[\"basis_functions\"][neuron_id]\n",
    "  else:\n",
    "    stim0 = analyzer.neuron_vis_output[\"optimal_stims\"][neuron_id][step_idx]\n",
    "  stim0 = stim0.reshape(np.prod(analyzer.model.get_input_shape()[1:])) # shape=[784]\n",
    "  stim0 = stim0 / np.linalg.norm(stim0) # normalize length\n",
    "  stim0_list.append(stim0)\n",
    "  stimid0_list.append(neuron_id)\n",
    "  if not use_rand_orth:\n",
    "    if(analyzer.analysis_params.model_type.lower() == \"lca\"):\n",
    "      gt_min_angle_indices = np.argwhere(analyzer.neuron_angles[neuron_id, :] > min_angle)\n",
    "      sorted_angle_indices = np.argsort(analyzer.neuron_angles[neuron_id, gt_min_angle_indices], axis=0)\n",
    "      vector_id = gt_min_angle_indices[sorted_angle_indices[0]].item()\n",
    "      alt_stim =  analyzer.bf_stats[\"basis_functions\"][vector_id]\n",
    "      alt_stim = [np.squeeze(alt_stim.reshape(analyzer.model_params.num_pixels))]\n",
    "      comparison_vector = get_alt_vectors(stim0, alt_stim)[0]\n",
    "      alt_stim_list.append(comparison_vector)\n",
    "    else:\n",
    "      alt_stims = [analyzer.neuron_vis_output[\"optimal_stims\"][orth_neuron_idx][step_idx]\n",
    "        for orth_neuron_idx in orth_neuron_indices]\n",
    "      alt_stim_list = get_alt_vectors(stim0, alt_stims)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "for lambda tests\n",
    "\"\"\"\n",
    "one_hot = np.array([1,1]+[0]*(analyzer.model.get_input_shape()[1]-2))\n",
    "one_hot = one_hot / np.linalg.norm(one_hot)\n",
    "hots = [np.roll(one_hot, shift=shift) for shift in range(2*num_neurons)]\n",
    "stim0_list = hots[:num_neurons]\n",
    "alt_stim_list = hots[num_neurons:]\n",
    "neuron_id_list = [0]*len(stim0_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_dict = get_norm_activity(analyzer, neuron_id_list, stim0_list, alt_stim_list, num_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots_y = num_neurons + 1 # extra dimension for example image\n",
    "num_plots_x = num_neurons + 1 # extra dimension for example image\n",
    "\n",
    "gs0 = gridspec.GridSpec(num_plots_y, num_plots_x, wspace=0.1, hspace=0.1)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "cmap = plt.get_cmap('viridis')\n",
    "\n",
    "orth_vectors = []\n",
    "for neuron_loop_index in range(num_neurons): # rows\n",
    "  for orth_loop_index in range(num_neurons): # columns\n",
    "    norm_activity = out_dict[\"norm_activity\"][neuron_loop_index][orth_loop_index]\n",
    "    proj_neuron0 = out_dict[\"proj_neuron0\"][neuron_loop_index][orth_loop_index]\n",
    "    proj_neuron1 = out_dict[\"proj_neuron1\"][neuron_loop_index][orth_loop_index]\n",
    "    proj_v = out_dict[\"proj_v\"][neuron_loop_index][orth_loop_index]\n",
    "    orth_vectors.append(out_dict[\"v\"][neuron_loop_index][orth_loop_index])\n",
    "\n",
    "    curve_plot_y_idx = neuron_loop_index + 1\n",
    "    curve_plot_x_idx = orth_loop_index + 1\n",
    "    curve_ax = pf.clear_axis(fig.add_subplot(gs0[curve_plot_y_idx, curve_plot_x_idx]))\n",
    "\n",
    "    # NOTE: each subplot has a renormalized color scale\n",
    "    # TODO: Add scale bar like in the lca inference plots\n",
    "    vmin = np.min(norm_activity)\n",
    "    vmax = np.max(norm_activity)\n",
    "\n",
    "    levels = 5\n",
    "    contsf = curve_ax.contourf(out_dict[\"X_mesh\"], out_dict[\"Y_mesh\"], norm_activity,\n",
    "      levels=levels, vmin=vmin, vmax=vmax, alpha=1.0, antialiased=True, cmap=cmap)\n",
    "\n",
    "    curve_ax.arrow(0, 0, proj_neuron0[0].item(), proj_neuron0[1].item(),\n",
    "      width=0.05, head_width=0.15, head_length=0.15, fc='r', ec='r')\n",
    "    curve_ax.arrow(0, 0, proj_neuron1[0].item(), proj_neuron1[1].item(),\n",
    "      width=0.05, head_width=0.15, head_length=0.15, fc='w', ec='w')\n",
    "    curve_ax.arrow(0, 0, proj_v[0].item(), proj_v[1].item(),\n",
    "      width=0.05, head_width=0.15, head_length=0.15, fc='k', ec='k')\n",
    "    #curve_ax.arrow(0, 0, proj_neuron0[0].item(), proj_neuron0[1].item(),\n",
    "    #  width=0.05, head_width=0.15, head_length=0.15, fc='r', ec='r')\n",
    "    #curve_ax.arrow(0, 0, proj_neuron1[0].item(), proj_neuron1[1].item(),\n",
    "    #  width=0.005, head_width=0.15, head_length=0.15, fc='w', ec='w')\n",
    "    #curve_ax.arrow(0, 0, proj_v[0].item(), proj_v[1].item(),\n",
    "    #  width=0.05, head_width=0.05, head_length=0.15, fc='k', ec='k')\n",
    "\n",
    "    #curve_ax.set_xlim([-0.5, 19.5])\n",
    "    #curve_ax.set_ylim([-10, 10.0])\n",
    "    curve_ax.set_xlim([-0.5, 3.5])\n",
    "    curve_ax.set_ylim([-2, 2.0])\n",
    "    #curve_ax.set_xlim([0.999, 1.001])\n",
    "    #curve_ax.set_ylim([-0.001, 0.001])\n",
    "    \n",
    "for plot_y_id in range(num_plots_y):\n",
    "  for plot_x_id in range(num_plots_x):\n",
    "    if plot_y_id > 0 and plot_x_id == 0:\n",
    "      bf_ax = pf.clear_axis(fig.add_subplot(gs0[plot_y_id, plot_x_id]))\n",
    "      bf_resh = stim0_list[plot_y_id-1].reshape((int(np.sqrt(np.prod(analyzer.model.params.data_shape))),\n",
    "        int(np.sqrt(np.prod(analyzer.model.params.data_shape)))))\n",
    "      bf_ax.imshow(bf_resh, cmap=\"Greys_r\")\n",
    "      if plot_y_id == 1:\n",
    "        bf_ax.set_title(\"Target vectors\", color=\"r\", fontsize=16)\n",
    "    if plot_y_id == 0 and plot_x_id > 0:\n",
    "      #comparison_img = comparison_vectors[plot_x_id-1, :].reshape(int(np.sqrt(np.prod(analyzer.model.params.data_shape))),\n",
    "      #  int(np.sqrt(np.prod(analyzer.model.params.data_shape))))\n",
    "      orth_img = orth_vectors[plot_x_id-1].reshape(int(np.sqrt(np.prod(analyzer.model.params.data_shape))),\n",
    "        int(np.sqrt(np.prod(analyzer.model.params.data_shape))))\n",
    "      orth_ax = pf.clear_axis(fig.add_subplot(gs0[plot_y_id, plot_x_id]))\n",
    "      orth_ax.imshow(orth_img, cmap=\"Greys_r\")\n",
    "      if plot_x_id == 1:\n",
    "        #orth_ax.set_ylabel(\"Orthogonal vectors\", color=\"k\", fontsize=16)\n",
    "        orth_ax.set_title(\"Orthogonal vectors\", color=\"k\", fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(analyzer.analysis_out_dir+\"/vis/iso_contour_grid_04.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curvature comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = [1, 1]#, 3]\n",
    "for analyzer, list_index in zip(analyzer_list, id_list):\n",
    "  analyzer.bf0 = stim0_list[list_index]\n",
    "  analyzer.bf_id0 = stimid0_list[list_index]\n",
    "  analyzer.bf0_slice_scale = 0.80 # between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* Compute a unit vector that is in the same plane as a given basis function pair (B1,B2) and\n",
    "  is orthogonal to B1, where B1 is the target basis for comparison and B2 is selected from all other bases.\n",
    "* Construct a line of data points in this plane\n",
    "* Project the data points into image space, compute activations, plot activations\n",
    "\"\"\"\n",
    "for analyzer in analyzer_list:\n",
    "  analyzer.pop_num_imgs = 100\n",
    "  \n",
    "  #orthogonal_list = [idx for idx in range(analyzer.bf_stats[\"num_outputs\"])]\n",
    "  orthogonal_list = [idx for idx in range(analyzer.bf_stats[\"num_outputs\"]) if idx != analyzer.bf_id0]\n",
    "  analyzer.num_orthogonal = len(orthogonal_list)\n",
    "  \n",
    "  pop_x_pts = np.linspace(-2.0, 2.0, int(analyzer.pop_num_imgs))\n",
    "  pop_y_pts = np.linspace(-2.0, 2.0, int(analyzer.pop_num_imgs))\n",
    "  pop_X, pop_Y = np.meshgrid(pop_x_pts, pop_y_pts)\n",
    "  full_pop_proj_datapoints = np.stack([pop_X.reshape(analyzer.pop_num_imgs**2),\n",
    "    pop_Y.reshape(analyzer.pop_num_imgs**2)], axis=1) # construct a grid\n",
    "  \n",
    "  # find a location to take a slice\n",
    "  # to avoid having to exactly find a point we use a relative position\n",
    "  x_target = pop_x_pts[int(analyzer.bf0_slice_scale*analyzer.pop_num_imgs)]\n",
    "  \n",
    "  slice_indices = np.where(full_pop_proj_datapoints[:,0]==x_target)[0]\n",
    "  analyzer.pop_proj_datapoints = full_pop_proj_datapoints[slice_indices,:] # slice grid\n",
    "  \n",
    "  analyzer.pop_datapoints = [None,]*analyzer.num_orthogonal\n",
    "  for pop_idx, tmp_bf_id1 in enumerate(orthogonal_list):\n",
    "    tmp_bf1 = analyzer.bf_stats[\"basis_functions\"][tmp_bf_id1].reshape((analyzer.model_params.num_pixels))\n",
    "    tmp_bf1 /= np.linalg.norm(tmp_bf1)\n",
    "    tmp_proj_matrix, v = analyzer.bf_projections(analyzer.bf0, tmp_bf1) \n",
    "    analyzer.pop_datapoints[pop_idx] = np.dot(analyzer.pop_proj_datapoints, tmp_proj_matrix)#[slice_indices,:]\n",
    "  \n",
    "  analyzer.pop_datapoints = np.reshape(np.stack(analyzer.pop_datapoints, axis=0),\n",
    "    [analyzer.num_orthogonal*analyzer.pop_num_imgs, analyzer.model_params.num_pixels])\n",
    "  \n",
    "  analyzer.pop_datapoints = dp.reshape_data(analyzer.pop_datapoints, flatten=False)[0]\n",
    "  analyzer.pop_datapoints = {\"test\": Dataset(analyzer.pop_datapoints, lbls=None,\n",
    "    ignore_lbls=None, rand_state=analyzer.rand_state)}\n",
    "  #analyzer.pop_datapoints = analyzer.model.preprocess_dataset(analyzer.pop_datapoints,\n",
    "  #  params={\"whiten_data\":analyzer.model_params.whiten_data,\n",
    "  #  \"whiten_method\":analyzer.model_params.whiten_method,\n",
    "  #  \"whiten_batch_size\":10})\n",
    "  analyzer.pop_datapoints = analyzer.model.reshape_dataset(analyzer.pop_datapoints, analyzer.model_params)\n",
    "  #analyzer.pop_datapoints[\"test\"].images /= np.max(np.abs(analyzer.pop_datapoints[\"test\"].images))\n",
    "  #analyzer.pop_datapoints[\"test\"].images *= 10#analyzer.analysis_params.input_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analyzer in analyzer_list:\n",
    "  pop_activations = analyzer.compute_activations(analyzer.pop_datapoints[\"test\"].images)[:, analyzer.bf_id0]\n",
    "  pop_activations = pop_activations.reshape([analyzer.num_orthogonal, analyzer.pop_num_imgs])\n",
    "  analyzer.pop_norm_activity = pop_activations / (np.amax(np.abs(pop_activations)) + 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* Construct the set of unit-length bases that are orthogonal to B0 (there should be B0.size-1 of them)\n",
    "* Construct a line of data points in each plane defined by B0 and a given orthogonal basis\n",
    "* Project the data points into image space, compute activations, plot activations\n",
    "\"\"\"\n",
    "for analyzer in analyzer_list:\n",
    "  analyzer.rand_pop_num_imgs = 100\n",
    "  analyzer.rand_num_orthogonal = analyzer.bf_stats[\"num_inputs\"]-1\n",
    "  \n",
    "  pop_x_pts = np.linspace(-2.0, 2.0, int(analyzer.rand_pop_num_imgs))\n",
    "  pop_y_pts = np.linspace(-2.0, 2.0, int(analyzer.rand_pop_num_imgs))\n",
    "  pop_X, pop_Y = np.meshgrid(pop_x_pts, pop_y_pts)\n",
    "  full_rand_pop_proj_datapoints = np.stack([pop_X.reshape(analyzer.rand_pop_num_imgs**2),\n",
    "    pop_Y.reshape(analyzer.rand_pop_num_imgs**2)], axis=1) # construct a grid\n",
    "  \n",
    "  # find a location to take a slice\n",
    "  x_target = pop_x_pts[int(analyzer.bf0_slice_scale*np.sqrt(analyzer.rand_pop_num_imgs))]\n",
    "  \n",
    "  slice_indices = np.where(full_rand_pop_proj_datapoints[:,0]==x_target)[0]\n",
    "  analyzer.rand_pop_proj_datapoints = full_rand_pop_proj_datapoints[slice_indices,:] # slice grid\n",
    "  \n",
    "  orth_col_matrix = analyzer.bf0.T[:,None]\n",
    "  analyzer.rand_pop_datapoints = [None,]*analyzer.rand_num_orthogonal\n",
    "  for pop_idx in range(analyzer.rand_num_orthogonal):\n",
    "    v = find_orth_vect(orth_col_matrix)\n",
    "    orth_col_matrix = np.append(orth_col_matrix, v[:,None], axis=1)\n",
    "    tmp_proj_matrix = np.stack([analyzer.bf0, v], axis=0)\n",
    "    analyzer.rand_pop_datapoints[pop_idx] = np.dot(analyzer.rand_pop_proj_datapoints,\n",
    "      tmp_proj_matrix)\n",
    "\n",
    "  analyzer.rand_pop_datapoints = np.reshape(np.stack(analyzer.rand_pop_datapoints, axis=0),\n",
    "    [analyzer.rand_num_orthogonal*analyzer.rand_pop_num_imgs, analyzer.model_params.num_pixels])\n",
    "\n",
    "  analyzer.rand_pop_datapoints = dp.reshape_data(analyzer.rand_pop_datapoints, flatten=False)[0]\n",
    "  analyzer.rand_pop_datapoints = {\"test\": Dataset(analyzer.rand_pop_datapoints, lbls=None,\n",
    "    ignore_lbls=None, rand_state=analyzer.rand_state)}\n",
    "  #analyzer.rand_pop_datapoints = analyzer.model.preprocess_dataset(analyzer.rand_pop_datapoints,\n",
    "  #  params={\"whiten_data\":analyzer.model.params.whiten_data,\n",
    "  #  \"whiten_method\":analyzer.model.params.whiten_method,\n",
    "  #  \"whiten_batch_size\":10})\n",
    "  analyzer.rand_pop_datapoints = analyzer.model.reshape_dataset(analyzer.rand_pop_datapoints,\n",
    "    analyzer.model_params)\n",
    "  #analyzer.rand_pop_datapoints[\"test\"].images /= np.max(np.abs(analyzer.rand_pop_datapoints[\"test\"].images))\n",
    "  #analyzer.rand_pop_datapoints[\"test\"].images *= 10# analyzer.analysis_params.input_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analyzer in analyzer_list:\n",
    "  rand_pop_activations = analyzer.compute_activations(analyzer.rand_pop_datapoints[\"test\"].images)[:,\n",
    "    analyzer.bf_id0]\n",
    "  rand_pop_activations = rand_pop_activations.reshape([analyzer.rand_num_orthogonal, analyzer.rand_pop_num_imgs])\n",
    "  analyzer.rand_pop_norm_activity = rand_pop_activations / (np.amax(np.abs(rand_pop_activations)) + 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analyzer in analyzer_list:\n",
    "  analyzer.bf_coeffs = [\n",
    "    np.polynomial.polynomial.polyfit(analyzer.pop_proj_datapoints[:,1],\n",
    "    analyzer.pop_norm_activity[orthog_idx,:], deg=2)\n",
    "    for orthog_idx in range(analyzer.num_orthogonal)]\n",
    "  analyzer.bf_fits = [\n",
    "    np.polynomial.polynomial.polyval(analyzer.pop_proj_datapoints[:,1], coeff)\n",
    "    for coeff in analyzer.bf_coeffs]\n",
    "  analyzer.bf_curvatures = [np.polyder(fit, m=2) for fit in analyzer.bf_fits]\n",
    "  \n",
    "  analyzer.rand_coeffs = [np.polynomial.polynomial.polyfit(analyzer.rand_pop_proj_datapoints[:,1],\n",
    "    analyzer.rand_pop_norm_activity[orthog_idx,:], deg=2)\n",
    "    for orthog_idx in range(analyzer.rand_num_orthogonal)]\n",
    "  analyzer.rand_fits = [np.polynomial.polynomial.polyval(analyzer.rand_pop_proj_datapoints[:,1], coeff)\n",
    "    for coeff in analyzer.rand_coeffs]\n",
    "  analyzer.rand_curvatures = [np.polyder(fit, m=2) for fit in analyzer.rand_fits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_idx = 0\n",
    "\n",
    "bf_curvatures = np.stack(analyzer_list[analyzer_idx].bf_coeffs, axis=0)[:,2]\n",
    "rand_curvatures = np.stack(analyzer_list[analyzer_idx].rand_coeffs, axis=0)[:,2]\n",
    "\n",
    "num_bins = 100\n",
    "bins = np.linspace(-0.2, 0.01, num_bins)\n",
    "bar_width = np.diff(bins).min()\n",
    "bf_hist, bin_edges = np.histogram(bf_curvatures.flatten(), bins)\n",
    "rand_hist, _ = np.histogram(rand_curvatures.flatten(), bins)\n",
    "bin_left, bin_right = bin_edges[:-1], bin_edges[1:]\n",
    "bin_centers = bin_left + (bin_right - bin_left)/2\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(16,9))\n",
    "\n",
    "ax.bar(bin_centers, rand_hist, width=bar_width, log=False, color=\"g\", alpha=0.5, align=\"center\",\n",
    "  label=\"Random Projection\")\n",
    "ax.bar(bin_centers, bf_hist, width=bar_width, log=False, color=\"r\", alpha=0.5, align=\"center\",\n",
    "  label=\"BF Projection\")\n",
    "\n",
    "ax.set_xticks(bin_left, minor=True)\n",
    "ax.set_xticks([bin_left[0], bin_left[int(len(bin_left)/2)], 0.0], minor=False)\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter(\"%0.3f\"))\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "  tick.label.set_fontsize(24) \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "  tick.label.set_fontsize(24) \n",
    "\n",
    "ax.set_title(\"Histogram of Curvatures\", fontsize=32)\n",
    "ax.set_xlabel(\"Curvature\", fontsize=32)\n",
    "ax.set_ylabel(\"Count\", fontsize=32)\n",
    "ax.legend(loc=2, fontsize=32)\n",
    "fig.savefig(analyzer.analysis_out_dir+\"/vis/histogram_of_curvatures_bf0id\"+str(analyzer.bf_id0)+\".png\",\n",
    "  transparent=True, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for analyzer in analyzer_list:\n",
    "  analyzer.orth_col_matrix = analyzer.bf0.T[:,None]\n",
    "  analyzer.rand_num_orthogonal = np.prod(analyzer.model.get_input_shape()[1:])-1\n",
    "\n",
    "  for pop_idx in range(analyzer.rand_num_orthogonal):\n",
    "    v = find_orth(analyzer.orth_col_matrix)\n",
    "    analyzer.orth_col_matrix = np.append(analyzer.orth_col_matrix, v[:,None], axis=1)\n",
    "\n",
    "  if all(np.abs(np.dot(analyzer.bf0, col)) < 1e-9 for col in analyzer.orth_col_matrix[:,1:].T):\n",
    "    print(\"Success\")\n",
    "  else:\n",
    "    count = np.sum([int(np.abs(np.dot(analyzer.bf0, col)) < 1e-9) for col in analyzer.orth_col_matrix[:,1:].T])\n",
    "    print(\"Failure,\", count, \"were non-orthogonal\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_rand_orthogonal = np.prod(analyzer.model.get_input_shape()[1:])\n",
    "num_imgs = int(228**2)\n",
    "set_norm_activity(\n",
    "  analyzer=analyzer_list[0],\n",
    "  target_neuron_idx=0,#np.random.choice(range(analyzer.model.get_num_latent()), 1),\n",
    "  num_rand_orthogonal=num_rand_orthogonal,\n",
    "  orthogonal_idx=np.random.choice(range(1, num_rand_orthogonal), 1),\n",
    "  num_imgs=num_imgs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for analyzer in analyzer_list:\n",
    "  num_plots_y = 1\n",
    "  num_plots_x = 2\n",
    "  gs1 = gridspec.GridSpec(num_plots_y, num_plots_x, wspace=0.3, width_ratios=[4, 1])\n",
    "  fig = plt.figure(figsize=(6,6))\n",
    "  curve_ax = pf.clear_axis(fig.add_subplot(gs1[0]))\n",
    "  #cmap = plt.get_cmap('tab20b')\n",
    "  cmap = plt.get_cmap('viridis')\n",
    "  vmin = np.floor(np.min(analyzer.norm_activity))#0.0\n",
    "  vmax = np.ceil(np.max(analyzer.norm_activity))#1.0\n",
    "  \n",
    "  #name_suffix = \"continuous\"\n",
    "  #pts = curve_ax.scatter(analyzer.proj_datapoints[:,0], analyzer.proj_datapoints[:,1],\n",
    "  #  vmin=vmin, vmax=vmax, cmap=cmap, alpha=0.5, c=analyzer.norm_activity[:, analyzer.bf_id0], s=5.0)\n",
    "  \n",
    "  norm_activity = analyzer.norm_activity[:, analyzer.bf_id0]\n",
    "  norm_activity = norm_activity.reshape(int(np.sqrt(num_imgs)), int(np.sqrt(num_imgs)))\n",
    "  \n",
    "  levels = 5\n",
    "  name_suffix = \"\"\n",
    "  contsf = curve_ax.contourf(analyzer.X_mesh, analyzer.Y_mesh, norm_activity,\n",
    "    levels=levels, vmin=vmin, vmax=vmax, alpha=1.0, antialiased=True, cmap=cmap)\n",
    "  \n",
    "  curve_ax.arrow(0, 0, analyzer.proj_neuron0[0].item(), analyzer.proj_neuron0[1].item(),\n",
    "    width=0.05, head_width=0.15, head_length=0.15, fc='r', ec='r')\n",
    "  curve_ax.arrow(0, 0, analyzer.proj_neuron1[0].item(), analyzer.proj_neuron1[1].item(),\n",
    "    width=0.05, head_width=0.15, head_length=0.15, fc='w', ec='k')\n",
    "  \n",
    "  curve_ax.set_ylim([-2, 2.0])\n",
    "  curve_ax.set_xlim([-2, 2.0])\n",
    "  curve_ax.set_aspect(\"equal\")\n",
    "  curve_ax.set_title(\"Neuron ID \"+str(analyzer.bf_id0), fontsize=16)\n",
    "  \n",
    "  gs2 = gridspec.GridSpecFromSubplotSpec(2, 1, gs1[1], hspace=-0.5)\n",
    "  bf1_ax = pf.clear_axis(fig.add_subplot(gs2[0]))\n",
    "  bf1_ax.imshow(analyzer.neuron_vis_output[\"optimal_stims\"][analyzer.bf_id0][step_idx].reshape((28,28)),\n",
    "    cmap=\"Greys_r\")\n",
    "  bf1_ax.set_title(\"Primary\\n Stimulus\", color='r', fontsize=16)\n",
    "  \n",
    "  activity_ax = fig.add_subplot(gs2[1])\n",
    "  activity_ax.plot(norm_activity[0,:], color='k')\n",
    "  activity_ax.set_aspect(1.0/activity_ax.get_data_ratio())\n",
    "  activity_ax.set_title(\"Activity along\\nstimulus vector\")\n",
    "  \n",
    "  fig.savefig(analyzer.analysis_out_dir+\"/vis/neuron_response_contours_bf0id\"+str(analyzer.bf_id0)+name_suffix+\".png\",\n",
    "    transparent=True, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def set_norm_activity(analyzer, target_neuron_idx, num_orth_directions,\n",
    "  orthogonal_idx, num_imgs, orth_neuron_indices=None):\n",
    "\n",
    "  # Get alt vectors\n",
    "  #for orth_idx in range(num_orth_directions):\n",
    "  #  if alt_neuron_indices is None: # find random orthogonal indices\n",
    "  #    tmp_bf1 = find_orth_vect(alt_vectors)\n",
    "  #  else: # use other pre-images as starting point for orthogonal indices\n",
    "  #    tmp_bf1 = analyzer.neuron_vis_output[\"optimal_stims\"][alt_neuron_indices[pop_idx]][step_idx]\n",
    "  #    tmp_bf1 /= np.linalg.norm(tmp_bf1)\n",
    "  #  alt_vectors = np.append(alt_vectors, tmp_bf1[:,None], axis=1)\n",
    "\n",
    "  # Construct point dataset\n",
    "  x_pts = np.linspace(-0.5, 5.0, int(np.sqrt(num_imgs)))\n",
    "  y_pts = np.linspace(-2.0, 2.0, int(np.sqrt(num_imgs)))\n",
    "  analyzer.X_mesh, analyzer.Y_mesh = np.meshgrid(x_pts, y_pts)\n",
    "  analyzer.proj_datapoints = np.stack([analyzer.X_mesh.reshape(num_imgs),\n",
    "    analyzer.Y_mesh.reshape(num_imgs)], axis=1)\n",
    "  \n",
    "  # Compute projection matrix to inject the point dataset into image space\n",
    "  proj_matrix, v = analyzer.bf_projections(analyzer.bf0, np.squeeze(alt_vectors[:, orthogonal_idx]))\n",
    "  analyzer.proj_neuron0 = np.dot(proj_matrix, analyzer.bf0).T\n",
    "  analyzer.proj_neuron1 = np.dot(proj_matrix, np.squeeze(alt_vectors[:, orthogonal_idx])).T\n",
    "  \n",
    "  orth_datapoints = None#np.stack([np.dot(proj_matrix.T, v)])\n",
    "    #for orth_id in range(num_orth_directions)], axis=0)\n",
    "  \n",
    "  datapoints = np.stack([np.dot(proj_matrix.T, analyzer.proj_datapoints[data_id,:])\n",
    "    for data_id in range(num_imgs)]) #inject\n",
    "  datapoints, orig_shape = dp.reshape_data(datapoints, flatten=False)[:2]\n",
    "  datapoints = {\"test\": Dataset(datapoints, lbls=None,\n",
    "    ignore_lbls=None, rand_state=analyzer.rand_state)}\n",
    "  #params={\"whiten_data\":analyzer.model_params.whiten_data}\n",
    "  #if params[\"whiten_data\"]:\n",
    "  #  params[\"whiten_method\"] = analyzer.model_params.whiten_method\n",
    "  #datapoints = analyzer.model.preprocess_dataset(datapoints, params=params)\n",
    "  datapoints = analyzer.model.reshape_dataset(datapoints, analyzer.model_params)\n",
    "  datapoints[\"test\"].images = (\n",
    "    (datapoints[\"test\"].images - np.min(datapoints[\"test\"].images, axis=1, keepdims=True))\n",
    "    / (np.max(datapoints[\"test\"].images, axis=1, keepdims=True) - np.min(datapoints[\"test\"].images, axis=1, keepdims=True)))\n",
    "  #datapoints[\"test\"].images /= np.max(np.abs(datapoints[\"test\"].images))\n",
    "  #datapoints[\"test\"].images *= analyzer.analysis_params.input_scale*3\n",
    "  \n",
    "  activations = analyzer.compute_activations(datapoints[\"test\"].images)#, batch_size=num_imgs//16)\n",
    "  activity_max = np.amax(np.abs(activations))\n",
    "  analyzer.norm_activity = activations / (activity_max + 0.00001) # Rescale between -1 and 1\n",
    "  \n",
    "  return orth_datapoints"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_norm_activity(analyzer, bf0, alt_vectors, num_imgs):\n",
    "  # Construct point dataset\n",
    "  x_pts = np.linspace(-0.5, 5.0, int(np.sqrt(num_imgs)))\n",
    "  y_pts = np.linspace(-2.0, 2.0, int(np.sqrt(num_imgs)))\n",
    "  X_mesh, Y_mesh = np.meshgrid(x_pts, y_pts)\n",
    "  proj_datapoints = np.stack([X_mesh.reshape(num_imgs), Y_mesh.reshape(num_imgs)], axis=1)\n",
    "  \n",
    "  out_dict = {\n",
    "    \"norm_activity\": [],\n",
    "    \"proj_neuron0\": [],\n",
    "    \"proj_neuron1\": [],\n",
    "    \"proj_v\": [],\n",
    "    \"proj_datapoints\": proj_datapoints,\n",
    "    \"X_mesh\": X_mesh,\n",
    "    \"Y_mesh\": Y_mesh}\n",
    "\n",
    "  for alt_vector_idx in range(alt_vectors.shape[1]):\n",
    "    alt_vector = alt_vectors[:, alt_vector_idx]\n",
    "    proj_matrix, v = dp.bf_projections(bf0, alt_vector)\n",
    "    out_dict[\"proj_neuron0\"].append(np.dot(proj_matrix, bf0).T) #project\n",
    "    out_dict[\"proj_neuron1\"].append(np.dot(proj_matrix, alt_vector).T) #project\n",
    "    out_dict[\"proj_v\"].append(np.dot(proj_matrix, v).T) #project\n",
    "    \n",
    "    datapoints = np.stack([np.dot(proj_matrix.T, proj_datapoints[data_id,:])\n",
    "      for data_id in range(num_imgs)]) #inject\n",
    "    datapoints = dp.reshape_data(datapoints, flatten=False)[0]\n",
    "    datapoints = {\"test\": Dataset(datapoints, lbls=None, ignore_lbls=None,\n",
    "      rand_state=analyzer.rand_state)}\n",
    "    \n",
    "    # preprocess_dataset should include rescaling to be between (0,1) for mnist\n",
    "    params={\"whiten_data\":analyzer.model_params.whiten_data}\n",
    "    if params[\"whiten_data\"]:\n",
    "      params[\"whiten_method\"] = analyzer.model_params.whiten_method\n",
    "    datapoints = analyzer.model.preprocess_dataset(datapoints, params=params)\n",
    "    datapoints = analyzer.model.reshape_dataset(datapoints, analyzer.model_params)\n",
    "    datapoints[\"test\"].images = (\n",
    "      (datapoints[\"test\"].images - np.min(datapoints[\"test\"].images, axis=1, keepdims=True))\n",
    "      / (np.max(datapoints[\"test\"].images, axis=1, keepdims=True) - np.min(datapoints[\"test\"].images, axis=1, keepdims=True)))\n",
    "    #datapoints[\"test\"].images /= np.max(np.abs(datapoints[\"test\"].images))\n",
    "    #datapoints[\"test\"].images *= analyzer.analysis_params.input_scale*3\n",
    "    \n",
    "    activations = analyzer.compute_activations(datapoints[\"test\"].images)#, batch_size=num_imgs//16)\n",
    "    activity_max = np.amax(np.abs(activations))\n",
    "    out_dict[\"norm_activity\"].append(activations / (activity_max + 0.00001)) # Rescale between -1 and 1\n",
    "  \n",
    "  return out_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
