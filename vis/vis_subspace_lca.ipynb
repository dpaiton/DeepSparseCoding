{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                                              \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy\n",
    "import tensorflow as tf                                                         \n",
    "import data.data_selector as ds                                                   \n",
    "import analysis.analysis_picker as ap\n",
    "import utils.plot_functions as pf                                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class analysis_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca_subspace\"\n",
    "    self.model_name = \"lca_subspace_vh\"\n",
    "    self.version = \"2.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "# Computed params\n",
    "analysis_params = analysis_params()\n",
    "analysis_params.project_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\")\n",
    "analysis_params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+analysis_params.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analyzer = ap.get_analyzer(analysis_params.model_type)\n",
    "analyzer.setup(analysis_params)\n",
    "analyzer.setup_model(analyzer.model_params)\n",
    "analyzer.load_analysis(save_info=analysis_params.save_info)\n",
    "analyzer.model_name = analysis_params.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_angles(init, num_steps, step_size, momentum_weight=0.0):\n",
    "  init = np.asarray(init)\n",
    "  prev_step = 0.0\n",
    "  angles = [init]\n",
    "  for step in range(1, num_steps):\n",
    "    delta = np.random.normal(0.0, 1.0, size=init.shape)\n",
    "    delta_angle = momentum_weight * prev_step + step_size * delta\n",
    "    new_angle = angles[step-1] + delta_angle\n",
    "    for dim in range(new_angle.ndim-1):\n",
    "      if new_angle[dim] > np.pi:\n",
    "        new_angle[dim] = new_angle[dim] - np.pi\n",
    "    if new_angle[-1] > 2*np.pi:\n",
    "      new_angle[-1] = new_angle[-1] - np.pi\n",
    "    angles.append(new_angle)\n",
    "  return np.stack(angles, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 500\n",
    "step_size = np.pi/10\n",
    "momentum_weight = 2.0\n",
    "init = [0, 0]\n",
    "angles = random_angles(init, num_steps, step_size, momentum_weight)\n",
    "points = np.zeros((num_steps, 3))\n",
    "for angle_id, angle in enumerate(angles):\n",
    "  points[angle_id, 0] = np.cos(angle[0])\n",
    "  points[angle_id, 1] = np.sin(angle[0])*np.cos(angle[1])\n",
    "  points[angle_id, 2] = np.sin(angle[0])*np.sin(angle[1])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "line, = ax.plot(points[:2,0], points[:2,1], zs=points[:2,2], color='k', linewidth=0.5, alpha=0.8)\n",
    "\n",
    "ax.set_xlim([-1, 1])\n",
    "ax.set_ylim([-1, 1])\n",
    "ax.set_zlim([-1, 1])\n",
    "\n",
    "def init():\n",
    "  line.set_xdata([np.nan])\n",
    "  line.set_ydata([np.nan])\n",
    "  line.set_3d_properties([np.nan])\n",
    "  return line,\n",
    "\n",
    "def animate(i):\n",
    "  line.set_xdata(points[:i, 0])\n",
    "  line.set_ydata(points[:i, 1])\n",
    "  line.set_3d_properties(points[:i, 2])\n",
    "  return line,\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=num_steps, init_func=init, interval=25, blit=True)\n",
    "ani.save(analyzer.analysis_out_dir+\"/vis/\"+analysis_params.model_name+\"_test_plot.mp4\")\n",
    "\n",
    "ax.scatter(points[:,0], points[:,1], points[:,2], color='r', s=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/N-sphere#Spherical_coordinates\n",
    "import itertools\n",
    "\n",
    "target_group = 1\n",
    "num_examples_per_dimension = 10\n",
    "num_steps = 1000\n",
    "step_size = np.pi/10\n",
    "momentum_weight = 2.0\n",
    "\n",
    "num_groups = analyzer.model_params.num_groups\n",
    "num_neurons = analyzer.model_params.num_neurons\n",
    "num_neurons_per_group = num_neurons // num_groups\n",
    "\n",
    "# Traversal of the space one axis at a time\n",
    "#angles = [np.linspace(0, np.pi, num_examples_per_dimension) for _ in range(num_neurons_per_group-2)]\n",
    "#angles += [np.linspace(0, 2*np.pi, num_examples_per_dimension)]\n",
    "#angles = [angle for angle in itertools.product(*angles)]\n",
    "\n",
    "# Traversal of the space via random walk\n",
    "init = [0]*num_neurons_per_group\n",
    "angles = random_angles(init, num_steps, step_size, momentum_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = []\n",
    "for angle in angles:\n",
    "  z = np.zeros((num_groups, num_neurons_per_group))\n",
    "  for group_neuron_idx in range(num_neurons_per_group):\n",
    "    if group_neuron_idx == 0:\n",
    "      z[target_group, group_neuron_idx] = np.cos(angle[group_neuron_idx])\n",
    "    elif group_neuron_idx > 0 and group_neuron_idx <= num_neurons_per_group-2:\n",
    "      prev_group_angles = [np.sin(angle[prev_group_neuron_idx])\n",
    "        for prev_group_neuron_idx in range(group_neuron_idx)]\n",
    "      z[target_group, group_neuron_idx] = np.prod(prev_group_angles)*np.cos(angle[group_neuron_idx])\n",
    "    else: # group_neuron_idx == num_neurons_per_group - 1\n",
    "      prev_group_angles = [np.sin(angle[prev_group_neuron_idx])\n",
    "        for prev_group_neuron_idx in range(group_neuron_idx)]\n",
    "      z[target_group, group_neuron_idx] = np.prod(prev_group_angles)\n",
    "  zs.append(z)\n",
    "zs = np.stack(zs, axis=0)\n",
    "\n",
    "sigmas = np.zeros((zs.shape[0], analyzer.model_params.num_groups))\n",
    "sigmas[:, target_group] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [zs.shape[0]]+ analyzer.model.get_input_shape()[1:]\n",
    "feed_dict = analyzer.model.get_feed_dict(np.zeros(input_shape), is_test=True)\n",
    "feed_dict[analyzer.sigmas] = sigmas\n",
    "feed_dict[analyzer.zs] = zs\n",
    "recons = analyzer.evaluate_tf_tensor(analyzer.group_recons, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = pf.clear_axis(fig.add_subplot())\n",
    "ims = []\n",
    "for i in range(recons.shape[0]):\n",
    "  recon = recons[i].reshape([analyzer.model_params.patch_edge_size]*2)\n",
    "  im = ax.imshow(recon, animated=True, cmap=\"Greys_r\")\n",
    "  ims.append([im])\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=50)\n",
    "ani.save(analyzer.analysis_out_dir+\"/vis/\"+analysis_params.model_name+\"_group_\"+str(target_group)+\"_recons.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full image recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_image = (\n",
    "  (analyzer.full_image - np.min(analyzer.full_image))\n",
    "  / (np.max(analyzer.full_image) - np.min(analyzer.full_image))).astype(np.float32)\n",
    "\n",
    "normed_recon = (\n",
    "  (analyzer.full_recon - np.min(analyzer.full_recon))\n",
    "  / (np.max(analyzer.full_recon) - np.min(analyzer.full_recon))).astype(np.float32)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,12))\n",
    "ax[0] = pf.clear_axis(ax[0])\n",
    "ax[0].imshow(np.squeeze(normed_image), cmap=\"Greys_r\")\n",
    "ax[0].set_title(\"Input Image\", fontsize=16)\n",
    "ax[1] = pf.clear_axis(ax[1])\n",
    "ax[1].imshow(np.squeeze(normed_recon), cmap=\"Greys_r\")\n",
    "percent_active = \"{:.2f}\".format(analyzer.recon_frac_act*100)\n",
    "psnr = \"{:.2f}\".format(compare_psnr(normed_image, normed_recon, data_range=1))\n",
    "ax[1].set_title(\"Reconstruction\\n\"+percent_active+\" percent active\"+\"\\n\"+\"PSNR = \"+psnr, fontsize=16)\n",
    "plt.show()\n",
    "fig.savefig(analyzer.analysis_out_dir+\"/vis/\"+analysis_params.model_name+\"_image_recon.png\", transparent=True,\n",
    "  bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data = ds.get_data(analyzer.model_params)\n",
    "data = analyzer.model.preprocess_dataset(data, analyzer.model_params)\n",
    "data = analyzer.model.reshape_dataset(data, analyzer.model_params)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_imgs = 10000#data[\"train\"].images.shape[0]\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config, graph=analyzer.model.graph) as sess:\n",
    "  feed_dict = analyzer.model.get_feed_dict(data[\"train\"].images[0:num_imgs,...])\n",
    "  sess.run(analyzer.model.init_op, feed_dict)\n",
    "  analyzer.model.load_full_model(sess, analyzer.analysis_params.cp_loc)\n",
    "  run_list = [analyzer.model.a, analyzer.model.module.group_activity, analyzer.model.module.group_angles]\n",
    "  neuron_activations, group_activations, group_angles = sess.run(run_list, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "labels = np.array([np.nonzero(data[\"train\"].labels[label_index,:])[0].item()\n",
    "  for label_index in range(data[\"train\"].labels.shape[0])]).T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# rank groupings by label category\n",
    "# grab input samples where labels match a category\n",
    "# sort activations of groups for those samples with the most active first\n",
    "# compute mode of top groups\n",
    "# repeat, excluding previous mode\n",
    "\n",
    "#np.argwhere(labels==0)\n",
    "\n",
    "#get_mode = lambda x : scipy.stats.mode(x)[0].item()\n",
    "\n",
    "# Need to exclude groups that were never active for this label\n",
    "\n",
    "#include_indices = np.arange(analyzer.model.num_groups, dtype=int) # startout looking across all groups\n",
    "#modes = []\n",
    "#for neuron_index in range(analyzer.model.num_groups):\n",
    "#  mode = get_mode([np.argsort(np.squeeze(group_activations)[batch_index, include_indices])[-1]\n",
    "#    for batch_index in np.argwhere(labels==0)])\n",
    "#  modes.append(mode)\n",
    "#  print(include_indices.size)\n",
    "#  include_indices = include_indices[include_indices!=mode]\n",
    "#print(modes)\n",
    "\n",
    "#get_mode([sorted_activations(group_activations[batch_index, include_indices])[0]\n",
    "#  for batch_index in np.argwhere(labels==0)])\n",
    "#[np.argsort(np.squeeze(group_activations[batch_index,:]))[::-1]\n",
    "#  for batch_index in np.argwhere(labels==0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VanHateren"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cent_group_activations = np.zeros_like(group_activations)\n",
    "for group_id in range(analyzer.model.num_groups):\n",
    "  cent_group_activations[:,group_id] = group_activations[:,group_id] - np.mean(group_activations[:,group_id])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#cov = 1/num_imgs * np.dot(group_activations.T, group_activations)\n",
    "cov = 1/num_imgs * np.dot(np.squeeze(cent_group_activations).T, np.squeeze(cent_group_activations))\n",
    "np.fill_diagonal(cov, 0.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(cov, cmap=\"Greys_r\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from matplotlib.ticker import FormatStrFormatter\n",
    "group_idx = 1\n",
    "num_bins = 1000\n",
    "figsize = None\n",
    "indiv_group_act = group_activations[:, group_idx]\n",
    "bins = np.linspace(np.min(indiv_group_act), np.max(indiv_group_act), num_bins)\n",
    "hist, bin_edges = np.histogram(indiv_group_act.flatten(), bins)\n",
    "bin_left, bin_right = bin_edges[:-1], bin_edges[1:]\n",
    "bin_centers = bin_left + (bin_right - bin_left)/2\n",
    "fig, ax = plt.subplots(1, figsize=figsize)\n",
    "ax.bar(bin_centers, hist, width=2.0, log=True, align=\"center\")\n",
    "ax.set_xticks(bin_left, minor=True)\n",
    "ax.set_xticks(bin_left[::10], minor=False)\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter(\"%0.0f\"))\n",
    "ax.set_xlim([np.min(indiv_group_act), np.max(indiv_group_act)])\n",
    "ax.set_title(\"Group \"+str(group_idx))\n",
    "ax.set_xlabel(\"Activation\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.scatter(np.arange(neuron_activations.size), neuron_activations.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.count_nonzero(neuron_activations.flatten())/neuron_activations.size"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(np.min(group_angles), np.mean(group_angles), np.max(group_angles))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "weights = analyzer.evals[\"lca_subspace/weights/w:0\"]\n",
    "active_idx = np.random.choice(analyzer.model.params.num_groups)\n",
    "group_activity_vector = np.zeros((1, analyzer.model.params.num_groups))\n",
    "group_activity_vector[0, active_idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct analysis plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pf.plot_loc_freq_summary(analyzer.bf_stats)\n",
    "fig.savefig(analyzer.analysis_out_dir+\"vis/location_frequency_centers.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_angles = analyzer.get_neuron_angles(analyzer.bf_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_hist_fig = pf.plot_weight_angle_histogram(neuron_angles[1], num_bins=50, angle_min=0, angle_max=180, figsize=(8,8))\n",
    "angle_hist_fig.savefig(analyzer.analysis_out_dir+\"/vis/neuron_angle_histogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_indices = np.stack([np.array(id_list) for id_list in analyzer.model.module.group_ids], axis=0)\n",
    "pooling_weights = np.stack([analyzer.evals[\"lca_subspace/weights/w:0\"][:, id_list] for id_list in analyzer.model.module.group_ids], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from mpl_toolkits import axes_grid1\n",
    "import utils.data_processing as dp\n",
    "def plot_group_weights(weights, group_ids, title=\"\", figsize=None,  save_filename=None):\n",
    "  \"\"\"\n",
    "    weights: [np.ndarray] of shape [num_neurons, num_input_y, num_input_x]\n",
    "    group_ids: [list of lists] containing ids for each group [[,]*neurons_per_group,]*num_groups\n",
    "  \"\"\"\n",
    "  num_neurons = weights.shape[0]\n",
    "  for weight_id in range(num_neurons):\n",
    "    weights[weight_id,...] = weights[weight_id,...] - weights[weight_id,...].mean()\n",
    "    weights[weight_id,...] = weights[weight_id,...] / (weights[weight_id,...].max()-weights[weight_id,...].min())\n",
    "  vmin = np.min(weights)\n",
    "  vmax = np.max(weights)\n",
    "  indices = [idx for id_list in group_ids for idx in id_list]\n",
    "  num_groups = len(group_ids)\n",
    "  num_groups_x = int(np.floor(np.sqrt(num_groups)))\n",
    "  num_groups_y = int(np.ceil(np.sqrt(num_groups)))\n",
    "  num_neurons_per_group = len(group_ids[0])\n",
    "  num_neurons_x = int(np.floor(np.sqrt(num_neurons_per_group)))\n",
    "  num_neurons_y = int(np.ceil(np.sqrt(num_neurons_per_group)))\n",
    "  outer_spacing = 0.20\n",
    "  inner_spacing = 0.1\n",
    "  fig = plt.figure(figsize=figsize)\n",
    "  gs1 = gridspec.GridSpec(num_groups_y, num_groups_x,\n",
    "    hspace=outer_spacing*num_groups_y/(num_groups_x+num_groups_y),\n",
    "    wspace=outer_spacing*num_groups_x/(num_groups_x+num_groups_y))\n",
    "  neuron_index = 0\n",
    "  for group_plot_id in np.ndindex((num_groups_y, num_groups_x)):\n",
    "    gs_inner = gridspec.GridSpecFromSubplotSpec(num_neurons_y, num_neurons_x, gs1[group_plot_id],\n",
    "      hspace=inner_spacing*num_neurons_y/(num_neurons_x+num_neurons_y),\n",
    "      wspace=inner_spacing*num_neurons_x/(num_neurons_x+num_neurons_y))\n",
    "    for inner_plot_id in np.ndindex((num_neurons_y, num_neurons_x)):\n",
    "      ax = pf.clear_axis(fig.add_subplot(gs_inner[inner_plot_id]))\n",
    "      ax.set_aspect(\"equal\")\n",
    "      if neuron_index < num_neurons:\n",
    "        ax.imshow(weights[indices[neuron_index], ...], cmap=\"Greys_r\", vmin=vmin, vmax=vmax)\n",
    "        neuron_index += 1\n",
    "  fig.suptitle(title, y=0.9, x=0.5, fontsize=20)\n",
    "  if save_filename is not None:\n",
    "    fig.savefig(save_filename)\n",
    "    plt.close(fig)\n",
    "    return None\n",
    "  plt.show()\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.reshape(analyzer.evals[\"lca_subspace/weights/w:0\"].T, [analyzer.model.params.num_neurons,\n",
    "      int(np.sqrt(analyzer.model.params.num_pixels)), int(np.sqrt(analyzer.model.params.num_pixels))])\n",
    "weight_fig = plot_group_weights(np.squeeze(weights), analyzer.model.module.group_ids,\n",
    "  title=\"Dictionary\", figsize=(18,18))\n",
    "weight_fig.savefig(analyzer.analysis_out_dir+\"/vis/\"+\"group_phi.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
