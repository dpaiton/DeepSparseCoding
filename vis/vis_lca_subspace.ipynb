{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from skimage.measure import compare_psnr\n",
    "import tensorflow as tf\n",
    "import data.data_selector as ds\n",
    "from data.dataset import Dataset\n",
    "import analysis.analysis_picker as ap\n",
    "import analysis.iso_response_analysis as iso\n",
    "import utils.plot_functions as pf\n",
    "import utils.data_processing as dp\n",
    "import utils.neural_comp_funcs as nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_width = 506.295 #pt\n",
    "dpi = 800\n",
    "fontsize = 12\n",
    "figsize = (12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lca_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_1280_vh\"\n",
    "    self.version = \"5x_0.55\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_subspace_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca_subspace\"\n",
    "    self.model_name = \"lca_subspace_vh\"\n",
    "    self.version = \"5x_4_1.0_0.2\"#\"3.0\"\n",
    "    self.save_info = \"analysis_train\"#\"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#analysis_params = lca_params()\n",
    "#analysis_params.project_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\")\n",
    "#analysis_params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+analysis_params.model_name)\n",
    "\n",
    "analysis_params = lca_subspace_params()\n",
    "analysis_params.project_dir = (os.path.expanduser(\"~\")+\"/Work/ryan_Projects/\")\n",
    "analysis_params.model_dir = (os.path.expanduser(\"~\")+\"/Work/ryan_Projects/\"+analysis_params.model_name)\n",
    "\n",
    "analysis_params.do_group_recons = True\n",
    "analysis_params.do_neuron_visualization = False\n",
    "\n",
    "analyzer = ap.get_analyzer(analysis_params.model_type)\n",
    "analyzer.setup(analysis_params)\n",
    "analyzer.setup_model(analyzer.model_params)\n",
    "analyzer.load_analysis(save_info=analysis_params.save_info)\n",
    "analyzer.model_name = analysis_params.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iso-Response Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_angles, plot_matrix = analyzer.get_neuron_angles(analyzer.bf_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_hist_fig = pf.plot_weight_angle_histogram(neuron_angles, num_bins=50, angle_min=0, angle_max=180,\n",
    "  y_max=1e6, figsize=(7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(plot_matrix[plot_matrix>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_angle = 10\n",
    "num_above_min = np.count_nonzero(plot_matrix<min_angle) # many angles are -1 or 0\n",
    "sorted_angle_indices = np.stack(np.unravel_index(np.argsort(plot_matrix.ravel()),\n",
    "  plot_matrix.shape), axis=1)[num_above_min:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_angle_indices[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_id0 = 1274#sorted_angle_indices[14,0]#39\n",
    "target_neuron_locs = np.argwhere(sorted_angle_indices[:,0] == bf_id0)\n",
    "low_angle_neuron_ids = np.squeeze(sorted_angle_indices[target_neuron_locs, 1])\n",
    "bf_id1 = 680#low_angle_neuron_ids[1]\n",
    "\n",
    "print(\"BF indices = [\",bf_id0,\", \",bf_id1,\"]\")\n",
    "fig, ax = plt.subplots(2)\n",
    "ax[0] = pf.clear_axis(ax[0])\n",
    "ax[0].imshow(analyzer.bf_stats[\"basis_functions\"][bf_id0], cmap=\"Greys_r\")\n",
    "ax[0].set_title(str(bf_id0))\n",
    "ax[1] = pf.clear_axis(ax[1])\n",
    "ax[1].imshow(analyzer.bf_stats[\"basis_functions\"][bf_id1], cmap=\"Greys_r\")\n",
    "ax[1].set_title(str(bf_id1))\n",
    "plt.show()\n",
    "print(\"vector angle\\t= \", plot_matrix[bf_id0, bf_id1]*(np.pi/180), \" rad\\n\\t\\t= \", plot_matrix[bf_id0, bf_id1], \" deg\")\n",
    "bf1 = analyzer.bf_stats[\"basis_functions\"][bf_id0].reshape((analyzer.model_params.patch_edge_size**2))\n",
    "bf2 = analyzer.bf_stats[\"basis_functions\"][bf_id1].reshape((analyzer.model_params.patch_edge_size**2))\n",
    "bf1_norm = np.linalg.norm(bf1)\n",
    "bf2_norm = np.linalg.norm(bf2)\n",
    "print(\"bf1 norm = \", bf1_norm)\n",
    "print(\"bf2 norm = \", bf2_norm)\n",
    "bf1 /= bf1_norm\n",
    "bf2 /= bf2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import Dataset\n",
    "num_imgs = 2500#10000\n",
    "\n",
    "x_min = -4.0\n",
    "x_max = 4.0\n",
    "y_min = -4.0\n",
    "y_max = 4.0\n",
    "\n",
    "x_pts = np.linspace(x_min, x_max, int(np.sqrt(num_imgs)))\n",
    "y_pts = np.linspace(y_min, y_max, int(np.sqrt(num_imgs)))\n",
    "X, Y = np.meshgrid(x_pts, y_pts)\n",
    "proj_datapoints = np.stack([X.reshape(num_imgs), Y.reshape(num_imgs)], axis=1)\n",
    "\n",
    "proj_matrix, v = analyzer.bf_projections(bf1, bf2)\n",
    "proj_neuron1 = np.dot(proj_matrix, bf1).T\n",
    "proj_neuron2 = np.dot(proj_matrix, bf2).T\n",
    "proj_v = np.dot(proj_matrix, v).T\n",
    "\n",
    "datapoints = np.stack([np.dot(proj_matrix.T, proj_datapoints[data_id,:]) for data_id in range(num_imgs)]) #inject\n",
    "datapoints, orig_shape = dp.reshape_data(datapoints, flatten=False)[:2]\n",
    "datapoints = {\"test\": Dataset(datapoints, lbls=None, ignore_lbls=None, rand_state=analyzer.rand_state)}\n",
    "datapoints = analyzer.model.preprocess_dataset(datapoints,\n",
    "  params={\"whiten_data\":analyzer.model_params.whiten_data,\n",
    "  \"whiten_method\":analyzer.model_params.whiten_method})\n",
    "datapoints = analyzer.model.reshape_dataset(datapoints, analyzer.model_params)\n",
    "#datapoints[\"test\"].images /= np.max(np.abs(datapoints[\"test\"].images))\n",
    "#datapoints[\"test\"].images *= 2.0#analyzer.analysis_params.input_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "activation_operation=analyzer.model.get_reshaped_group_activity\n",
    "neuron_index = bf_id0\n",
    "\n",
    "num_images, data_size = datapoints[\"test\"].images.shape\n",
    "analyzer.comp_activations = analyzer.compute_activations(datapoints[\"test\"].images, batch_size, activation_operation)\n",
    "analyzer.comp_activations = analyzer.comp_activations[:, neuron_index]\n",
    "activity_max = np.amax(np.abs(analyzer.comp_activations))\n",
    "analyzer.comp_activations = analyzer.comp_activations / (activity_max + 0.00001)\n",
    "analyzer.comp_activations = analyzer.comp_activations.reshape(int(np.sqrt(num_images)), int(np.sqrt(num_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_levels = 10\n",
    "num_plots_y = 1\n",
    "num_plots_x = 2\n",
    "gs1 = gridspec.GridSpec(num_plots_y, num_plots_x, wspace=0.5, width_ratios=[4, 1])\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "curve_ax = pf.clear_axis(fig.add_subplot(gs1[0]))\n",
    "\n",
    "vmin = np.floor(np.min(analyzer.comp_activations))#0.0\n",
    "vmax = np.ceil(np.max(analyzer.comp_activations))#1.0\n",
    "levels = np.linspace(vmin, vmax, num_levels)\n",
    "\n",
    "#cmap = plt.get_cmap('tab20b')\n",
    "#cmap = plt.get_cmap('viridis')\n",
    "#cmap = plt.get_cmap('jet')\n",
    "cmap = plt.get_cmap(\"cividis\")\n",
    "cNorm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "scalarMap = matplotlib.cm.ScalarMappable(norm=cNorm, cmap=cmap)\n",
    "\n",
    "#pts = curve_ax.scatter(proj_datapoints[:,0], proj_datapoints[:,1],\n",
    "#  vmin=vmin, vmax=vmax, cmap=cmap, c=analyzer.comp_activations.reshape(num_images), s=5.0)\n",
    "x_pts = np.linspace(x_min/2, x_max/2, int(np.sqrt(num_imgs))) # compress points into smaller area\n",
    "y_pts = np.linspace(y_min/2, y_max/2, int(np.sqrt(num_imgs)))\n",
    "X_mesh, Y_mesh = np.meshgrid(x_pts, y_pts)\n",
    "contsf = curve_ax.contourf(X_mesh, Y_mesh,\n",
    "  analyzer.comp_activations, levels=levels, vmin=vmin, vmax=vmax, alpha=1.0, antialiased=True, cmap=cmap)\n",
    "\n",
    "\n",
    "curve_ax.arrow(0, 0, proj_neuron1[0].item(), proj_neuron1[1].item(), width=0.05, head_width=0.15,\n",
    "  head_length=0.15, fc='k', ec='k')\n",
    "curve_ax.arrow(0, 0, proj_neuron2[0].item(), proj_neuron2[1].item(), width=0.05, head_width=0.15,\n",
    "  head_length=0.15, fc='k', ec='k')\n",
    "#curve_ax.set_title(\"Angle = \"+\"{:.2f}\".format(neuron_angles[bf_id0, bf_id1])+\" deg\")\n",
    "#curve_ax.set_title(\"Response from pooling neuron \"+\"{:.0f}\".format(pooling_filter_id))\n",
    "curve_ax.set_ylim([-2, 2.0])\n",
    "curve_ax.set_xlim([-2, 2.0])\n",
    "curve_ax.set_aspect(\"equal\")\n",
    "#cbar = pf.add_colorbar_to_im(pts, aspect=20, pad_fraction=0.5, labelsize=16, ticks=[vmin, vmax])\n",
    "#cbar.ax.set_yticklabels([\"{:.0f}\".format(vmin), \"{:.0f}\".format(vmax)])\n",
    "\n",
    "gs2 = gridspec.GridSpecFromSubplotSpec(2, 1, gs1[1], wspace=2, hspace=-0.2)\n",
    "bf1_ax = pf.clear_axis(fig.add_subplot(gs2[0]))\n",
    "bf1_ax.imshow(analyzer.bf_stats[\"basis_functions\"][bf_id0], cmap=\"Greys_r\")\n",
    "bf1_ax.set_title(\"Input\\nNeuron {:.0f}\".format(bf_id0), color='b')\n",
    "bf2_ax = pf.clear_axis(fig.add_subplot(gs2[1]))\n",
    "bf2_ax.imshow(analyzer.bf_stats[\"basis_functions\"][bf_id1], cmap=\"Greys_r\")\n",
    "bf2_ax.set_title(\"Input\\nNeuron {:.0f}\".format(bf_id1), color='k')\n",
    "#fig.savefig(analyzer.analysis_out_dir+\"/vis/l2_neuron_response_contours_pid\"+str(pooling_filter_id)+\"_bf0id\"+str(bf_id0)+\"_bf1id\"+str(bf_id1)+\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_neuron_id = 61\n",
    "in_group_comp_neuron_id = 62\n",
    "out_group_comp_neuron_id = 170#137#35\n",
    "x_range = [-2.0, 2.0]\n",
    "y_range = [-2.0, 2.0]\n",
    "num_images = int(30**2)\n",
    "batch_size = 100\n",
    "\n",
    "target_vector = analyzer.bf_stats[\"basis_functions\"][target_neuron_id]\n",
    "target_vector = target_vector.reshape(analyzer.model_params.num_pixels)\n",
    "target_vector = target_vector / np.linalg.norm(target_vector)\n",
    "\n",
    "in_comparison_vector = analyzer.bf_stats[\"basis_functions\"][in_group_comp_neuron_id]\n",
    "in_comparison_vector = in_comparison_vector.reshape(analyzer.model_params.num_pixels)\n",
    "in_comparison_vector = np.squeeze((in_comparison_vector / np.linalg.norm(in_comparison_vector)).T)\n",
    "out_comparison_vector = analyzer.bf_stats[\"basis_functions\"][out_group_comp_neuron_id]\n",
    "out_comparison_vector = out_comparison_vector.reshape(analyzer.model_params.num_pixels)\n",
    "out_comparison_vector = np.squeeze((out_comparison_vector / np.linalg.norm(out_comparison_vector)).T)\n",
    "\n",
    "analyzer.target_neuron_ids = [target_neuron_id]\n",
    "analyzer.comparison_neuron_ids = [in_group_comp_neuron_id, out_group_comp_neuron_id]\n",
    "analyzer.target_vectors = [target_vector]\n",
    "analyzer.comparison_vectors = [np.stack([in_comparison_vector, out_comparison_vector], axis=0)]\n",
    "\n",
    "num_comparison_vects = None\n",
    "use_random_orth_vects = False\n",
    "analyzer.comp_contour_dataset, datapoints = iso.get_contour_dataset(analyzer, num_comparison_vects,\n",
    "        use_random_orth_vects, x_range, y_range, num_images)\n",
    "\n",
    "analyzer.comp_activations = iso.get_normalized_activations(analyzer, datapoints, batch_size,\n",
    "  activation_operation=analyzer.model.get_reshaped_group_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_list = [analyzer]*2\n",
    "neuron_indices = [0]*2#[0, 0, 0, 0]\n",
    "orth_indices = [0, 1]\n",
    "num_levels = 10\n",
    "show_contours = True\n",
    "\n",
    "contour_fig, contour_handles = nc.plot_group_iso_contours(analyzer_list, neuron_indices, orth_indices,\n",
    "  num_levels, x_range, y_range, show_contours, text_width, 1.0, dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_vals = dict(zip([\"blk\", \"lt_green\", \"md_green\", \"dk_green\", \"lt_blue\", \"md_blue\", \"dk_blue\", \"lt_red\", \"md_red\", \"dk_red\"],\n",
    "  [\"#000000\", \"#A9DFBF\", \"#196F3D\", \"#27AE60\", \"#AED6F1\", \"#3498DB\", \"#21618C\", \"#F5B7B1\", \"#E74C3C\", \"#943126\"]))\n",
    "\n",
    "#save_name = \"iso_curvature_min85_max95_\"\n",
    "save_name = \"iso_curvature_min10_max60_\"\n",
    "run_params = np.load(analyzer.analysis_out_dir+\"savefiles/iso_params_\"+save_name\n",
    "  +analyzer.analysis_params.save_info+\".npz\", allow_pickle=True)[\"data\"].item()\n",
    "min_angle = run_params[\"min_angle\"]\n",
    "max_angle = run_params[\"max_angle\"]\n",
    "num_neurons = run_params[\"num_neurons\"]\n",
    "use_bf_stats = run_params[\"use_bf_stats\"]\n",
    "analyzer.num_comparison_vects = run_params[\"num_comparison_vects\"]\n",
    "x_range = run_params[\"x_range\"]\n",
    "y_range = run_params[\"y_range\"]\n",
    "num_images = run_params[\"num_images\"]\n",
    "\n",
    "iso_vectors = np.load(analyzer.analysis_out_dir+\"savefiles/iso_vectors_\"+save_name\n",
    "  +analyzer.analysis_params.save_info+\".npz\", allow_pickle=True)[\"data\"].item()\n",
    "analyzer.target_neuron_ids = iso_vectors[\"target_neuron_ids\"]\n",
    "analyzer.comparison_neuron_ids = iso_vectors[\"comparison_neuron_ids\"]\n",
    "analyzer.target_vectors = iso_vectors[\"target_vectors\"]\n",
    "analyzer.rand_orth_vectors = iso_vectors[\"rand_orth_vectors\"]\n",
    "analyzer.comparison_vectors = iso_vectors[\"comparison_vectors\"]\n",
    "\n",
    "analyzer.comp_activations = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_activations_\"+save_name\n",
    "  +analyzer.analysis_params.save_info+\".npz\", allow_pickle=True)[\"data\"]\n",
    "analyzer.comp_contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_contour_dataset_\"+save_name\n",
    "  +analyzer.analysis_params.save_info+\".npz\", allow_pickle=True)[\"data\"].item()\n",
    "analyzer.rand_activations = np.load(analyzer.analysis_out_dir+\"savefiles/iso_rand_activations_\"+save_name\n",
    "  +analyzer.analysis_params.save_info+\".npz\", allow_pickle=True)[\"data\"]\n",
    "analyzer.rand_contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_rand_contour_dataset_\"+save_name\n",
    "  +analyzer.analysis_params.save_info+\".npz\", allow_pickle=True)[\"data\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_list = [analyzer]*analyzer.num_comparison_vects\n",
    "neuron_indices = [0]*analyzer.num_comparison_vects\n",
    "orth_indices = list(range(analyzer.num_comparison_vects))\n",
    "num_levels = 10\n",
    "show_contours = True\n",
    "\n",
    "contour_fig, contour_handles = plot_group_iso_contours(analyzer_list, neuron_indices, orth_indices,\n",
    "  num_levels, x_range, y_range, show_contours, text_width, 1.0, dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group invariance reconstructions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import itertools\n",
    "def grid_angles(num_examples_per_dimension, num_neurons_per_group):\n",
    "  angles = [np.linspace(0, np.pi, num_examples_per_dimension) for _ in range(num_neurons_per_group-2)]\n",
    "  angles += [np.linspace(0, 2*np.pi, num_examples_per_dimension)]\n",
    "  angles = [angle for angle in itertools.product(*angles)]\n",
    "  return np.stack(angles, axis=0)\n",
    "\n",
    "def random_angles(init, num_steps, step_size, momentum_weight=0.0):\n",
    "  init = np.asarray(init)\n",
    "  prev_step = 0.0\n",
    "  angles = [init]\n",
    "  for step in range(1, num_steps):\n",
    "    delta = np.random.normal(0.0, 1.0, size=init.shape)\n",
    "    delta_angle = momentum_weight * prev_step + step_size * delta\n",
    "    new_angle = angles[step-1] + delta_angle\n",
    "    for dim in range(new_angle.ndim-1):\n",
    "      if new_angle[dim] > np.pi:\n",
    "        new_angle[dim] = new_angle[dim] - np.pi\n",
    "    if new_angle[-1] > 2*np.pi:\n",
    "      new_angle[-1] = new_angle[-1] - (2 * np.pi)\n",
    "    angles.append(new_angle)\n",
    "  return np.stack(angles, axis=0)\n",
    "\n",
    "def less_random_angles(init, steps_per_direction, num_directions, step_size):\n",
    "  init = np.asarray(init)\n",
    "  angles = [init]\n",
    "  for direction in range(num_directions):\n",
    "    delta = np.random.normal(0.0, 1.0, size=init.shape)\n",
    "    for step in range(steps_per_direction):\n",
    "      delta_angle = step_size * delta\n",
    "      new_angle = angles[step-1] + delta_angle\n",
    "      for dim in range(new_angle.ndim-1):\n",
    "        if new_angle[dim] > np.pi:\n",
    "          new_angle[dim] = new_angle[dim] - np.pi\n",
    "      if new_angle[-1] > 2*np.pi:\n",
    "        new_angle[-1] = new_angle[-1] - (2 *np.pi)\n",
    "      angles.append(new_angle)\n",
    "  return np.stack(angles, axis=0)\n",
    "\n",
    "def angles_to_vectors(angles, target_group, num_groups, num_neurons_per_group):\n",
    "  # https://en.wikipedia.org/wiki/N-sphere#Spherical_coordinates\n",
    "  zs = []\n",
    "  for angle in angles:\n",
    "    z = np.zeros((num_groups, num_neurons_per_group))\n",
    "    for group_neuron_idx in range(num_neurons_per_group):\n",
    "      if group_neuron_idx == 0:\n",
    "        z[target_group, group_neuron_idx] = np.cos(angle[group_neuron_idx])\n",
    "      elif group_neuron_idx > 0 and group_neuron_idx <= num_neurons_per_group-2:\n",
    "        prev_group_angles = [np.sin(angle[prev_group_neuron_idx])\n",
    "          for prev_group_neuron_idx in range(group_neuron_idx)]\n",
    "        z[target_group, group_neuron_idx] = np.prod(prev_group_angles)*np.cos(angle[group_neuron_idx])\n",
    "      else: # group_neuron_idx == num_neurons_per_group - 1\n",
    "        prev_group_angles = [np.sin(angle[prev_group_neuron_idx])\n",
    "          for prev_group_neuron_idx in range(group_neuron_idx)]\n",
    "        z[target_group, group_neuron_idx] = np.prod(prev_group_angles)\n",
    "    zs.append(z)\n",
    "  zs = np.stack(zs, axis=0)\n",
    "  return zs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Regarding fig 4, he suggested that instead of the method that you are currently using to generate images, to first find a set of images that evoke the maximal group/complex cell response and then find the z values that correspond to these images. With these z values we then should synthesize the images with a fixed sigma (e.g. 1) and compare the 'equivalent images'. He said that this way you can be sure the be exploring the z-space that is actually occupied by natural images, vs just randomly walking around in the space which is likely not to be completely occupied by natural images. He basically thought that some of the generated images didn't look like real natural image patches, so was concerned that it was because the latent space that some of them were generated from isn't typically within the domain that natural images span in that 4-dimensional space (in the case of group size 4). "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "step_size = np.pi/10\n",
    "momentum_weight = 2.0\n",
    "num_directions = 100\n",
    "steps_per_direction = 10\n",
    "\n",
    "num_groups = analyzer.model.params.num_groups\n",
    "num_neurons = analyzer.model.params.num_neurons\n",
    "num_neurons_per_group = num_neurons // num_groups\n",
    "init = [0]*num_neurons_per_group\n",
    "\n",
    "# Traversal of the space one axis at a time\n",
    "#num_examples_per_dimension = 10\n",
    "#angles = grid_angles(num_examples_per_dimension, num_neurons_per_group)\n",
    "\n",
    "# Traversal of the space via random walk w/ momentum\n",
    "#num_steps = num_directions * steps_per_direction\n",
    "#angles = random_angles(init, num_steps, step_size, momentum_weight)\n",
    "\n",
    "# Traversal of the space via endpoint-finding\n",
    "#angles = less_random_angles(init, steps_per_direction, num_directions, step_size)\n",
    "\n",
    "#from scipy.interpolate import interp1d\n",
    "\n",
    "#first_max_images = data[\"train\"].images[sort_args[0, :], ...]\n",
    "#first_angle_vector = analyzer.compute_activations(first_max_images, batch_size=1,\n",
    "#  activation_operation=analyzer.model.get_group_angle)\n",
    "#\n",
    "#second_max_images = data[\"train\"].images[sort_args[1, :], ...]\n",
    "#second_angle_vector = analyzer.compute_activations(second_max_images, batch_size=1,\n",
    "#  activation_operation=analyzer.model.get_group_angle)\n",
    "\n",
    "#angle_vectors = []\n",
    "#for group_index in range(num_plot_groups):\n",
    "#  angle_matrix = np.stack([first_angle_vector[group_index, :], second_angle_vector[group_index, :]], axis=0)\n",
    "#  linfit = interp1d([0, num_plot_groups], angle_matrix, axis=0)\n",
    "#  angle_vectors.append(linfit(list(range(num_plot_groups+1))))\n",
    "\n",
    "recons = []\n",
    "for group_id, target_group in enumerate(group_indices):\n",
    "  #zs = angles_to_vectors(angles, target_group, num_groups, num_neurons_per_group)\n",
    "  zs = angle_vectors[group_id, ...].reshape((num_plot_groups+1, num_groups, num_neurons_per_group))\n",
    "  sigmas = np.zeros((zs.shape[0], analyzer.model_params.num_groups))\n",
    "  sigmas[:, target_group] = 1\n",
    "  input_shape = [zs.shape[0]]+ analyzer.model.get_input_shape()[1:]\n",
    "  feed_dict = analyzer.model.get_feed_dict(np.zeros(input_shape), is_test=True)\n",
    "  feed_dict[analyzer.sigmas] = sigmas\n",
    "  feed_dict[analyzer.zs] = zs\n",
    "  recons.append(analyzer.evaluate_tf_tensor(analyzer.group_recons, feed_dict))\n",
    "\n",
    "num_plots_y = len(recons)\n",
    "num_plots_x = 2\n",
    "fig_aspect = num_plots_y / num_plots_x\n",
    "fig = plt.figure(figsize=[figsize[0] / fig_aspect, figsize[1]])\n",
    "gs = gridspec.GridSpec(num_plots_y, num_plots_x, wspace=0.01, hspace=0.02)\n",
    "for recon_ax in range(num_plots_y):\n",
    "  ax0 = pf.clear_axis(fig.add_subplot(gs[recon_ax, 0]))\n",
    "  img = max_images[recon_ax, ...].reshape([analyzer.model.params.patch_edge_size]*2)\n",
    "  img = dp.normalize_data_with_max(img)[0]\n",
    "  ax0.imshow(img, cmap=\"Greys_r\", vmin=-1, vmax=1)\n",
    "  \n",
    "  ax1 = pf.clear_axis(fig.add_subplot(gs[recon_ax, 1]))\n",
    "  img = recons[recon_ax][0].reshape([analyzer.model_params.patch_edge_size]*2)\n",
    "  img = dp.normalize_data_with_max(img)[0]\n",
    "  ax1.imshow(img, cmap=\"Greys_r\", vmin=-1, vmax=1)\n",
    "plt.show()\n",
    "\n",
    "#fig.savefig(analyzer.analysis_out_dir+\"/vis/\"+analysis_params.model_name+\"_group_invariance_tiled.png\",\n",
    "#  transparent=True, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "\n",
    "for recon_id in range(len(recons)):\n",
    "  fig = plt.figure(figsize=figsize)\n",
    "  ax = pf.clear_axis(fig.add_subplot())\n",
    "  ims = []\n",
    "  for i in range(recons[recon_id].shape[0]):\n",
    "    recon = recons[recon_id][i].reshape([analyzer.model_params.patch_edge_size]*2)\n",
    "    recon = dp.normalize_data_with_max(recon)[0]\n",
    "    im = ax.imshow(recon, animated=True, cmap=\"Greys_r\", vmin=-1, vmax=1)\n",
    "    ims.append([im])\n",
    "  ani = animation.ArtistAnimation(fig, ims, interval=50)\n",
    "  ani.save(analyzer.analysis_out_dir+\"/vis/\"+analysis_params.model_name+\"_group_\"+str(target_groups[recon_id])+\"_recons.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.expanduser(\"~\")+\"/Work/Datasets/\"\n",
    "data = ds.get_data_from_string(\"vanhateren\", data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(np.squeeze(data[\"train\"].images[29, ...]), cmap=\"Greys_r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_indices = [0, 4, 5, 6, 7, 11, 17, 22, 23, 25]#, 28]\n",
    "data_subset = {\"train\": Dataset(data[\"train\"].images[image_indices, ...], lbls=None, ignore_lbls=None,\n",
    "  rand_state=analyzer.rand_state)}\n",
    "analyzer.model_params.batch_size = 10\n",
    "analyzer.model_params.whiten_batch_size = 10\n",
    "analyzer.model_params.num_patches = 500\n",
    "data_subset = analyzer.model.preprocess_dataset(data_subset, params=analyzer.model_params)\n",
    "data_subset = analyzer.model.reshape_dataset(data_subset, analyzer.model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_images = 4\n",
    "\n",
    "#num_plot_groups = 5\n",
    "#select_group_indices = np.random.randint(0, analyzer.model.params.num_groups, num_plot_groups)\n",
    "#print(select_group_indices)\n",
    "\n",
    "#[6, 18, 41, 57, 78, 102, 128, 176, 275, 295, 315]\n",
    "select_group_indices = np.array([6, 41, 57, 18, 176])\n",
    "num_plot_groups = select_group_indices.size\n",
    "\n",
    "group_activations, group_angles = analyzer.evaluate_tf_tensor(\n",
    "  tensor=analyzer.model.get_group_encodings(),\n",
    "  feed_dict=analyzer.model.get_feed_dict(data_subset[\"train\"].images, is_test=True))\n",
    "group_activations = group_activations[:, select_group_indices]\n",
    "new_shape = (analyzer.model_params.num_patches, analyzer.model.params.num_groups,\n",
    "  analyzer.model.params.num_neurons_per_group)\n",
    "group_angles = group_angles.reshape(new_shape)[:, select_group_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_images = []\n",
    "angle_vectors = []\n",
    "recons = []\n",
    "for group_index in range(num_plot_groups):\n",
    "  data_activity_vector = group_activations[:, group_index]\n",
    "  max_image_indices = np.argsort(data_activity_vector)\n",
    "  max_images.append(data_subset[\"train\"].images[max_image_indices[-num_top_images:], ...])\n",
    "  angle_vectors.append(group_angles[max_image_indices[-num_top_images:], group_index, :])\n",
    "  sigmas = np.zeros((num_top_images, analyzer.model.params.num_groups))\n",
    "  sigmas[:, select_group_indices[group_index]] = 1\n",
    "  zs = np.zeros((num_top_images, analyzer.model.params.num_groups, analyzer.model.params.num_neurons_per_group))\n",
    "  zs[:, select_group_indices[group_index], :] = angle_vectors[-1]\n",
    "  input_shape = [num_top_images] + analyzer.model.get_input_shape()[1:]\n",
    "  feed_dict = analyzer.model.get_feed_dict(np.zeros(input_shape), is_test=True)\n",
    "  feed_dict[analyzer.sigmas] = sigmas\n",
    "  feed_dict[analyzer.zs] = zs\n",
    "  recons.append(analyzer.evaluate_tf_tensor(analyzer.group_recons, feed_dict))\n",
    "max_images = np.stack(max_images, axis=0)\n",
    "angle_vectors = np.stack(angle_vectors, axis=0)\n",
    "recons = np.stack(recons, axis=0)\n",
    "group_weights = analyzer.evaluate_tf_tensor(analyzer.model.module.group_weights,\n",
    "  analyzer.model.get_feed_dict(np.zeros(input_shape), is_test=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_plots_y = recons.shape[0]\n",
    "num_plots_x = 2 * recons.shape[1] + 1\n",
    "vmin = np.min(angle_vectors)\n",
    "vmax = np.max(angle_vectors)\n",
    "fig = plt.figure(figsize=[figsize[0], figsize[1]/2])\n",
    "gs = gridspec.GridSpec(num_plots_y, num_plots_x)#, hspace=-0.5, wspace=0.2)\n",
    "for group_idx in range(num_plots_y):\n",
    "  group_gs = gridspec.GridSpecFromSubplotSpec(2, 2, gs[group_idx, 0], wspace=-0.01)\n",
    "  group_order = [0, 1, 3, 2] # clockwise from top-left\n",
    "  for group_neuron_id in range(analyzer.model.params.num_neurons_per_group):\n",
    "    weight_ax = pf.clear_axis(fig.add_subplot(group_gs[group_order[group_neuron_id]]))\n",
    "    weight = group_weights[:, select_group_indices[group_idx], group_neuron_id]\n",
    "    weight = dp.normalize_data_with_max(weight)[0].reshape([analyzer.model_params.patch_edge_size]*2)\n",
    "    weight_ax.imshow(weight, cmap=\"Greys_r\", vmin=-1, vmax=1)\n",
    "  for image_idx, gs_x_idx in enumerate(range(1, num_plots_x, 2)):\n",
    "    recon_ax = pf.clear_axis(fig.add_subplot(gs[group_idx, gs_x_idx]))\n",
    "    recon = recons[group_idx, image_idx, ...]\n",
    "    recon = dp.normalize_data_with_max(recon)[0].reshape([analyzer.model_params.patch_edge_size]*2)\n",
    "    recon_ax.imshow(recon, cmap=\"Greys_r\", vmin=-1, vmax=1)\n",
    "    activity_ax = pf.clear_axis(fig.add_subplot(gs[group_idx, gs_x_idx+1]))\n",
    "    activity_ax.bar(\n",
    "      range(analyzer.model.params.num_neurons_per_group),\n",
    "      angle_vectors[group_idx, image_idx, :],\n",
    "      width=0.8, color=\"k\", align=\"center\")\n",
    "    activity_ax.set_ylim(vmin, vmax)\n",
    "    activity_ax.set_yticks(np.linspace(vmin, vmax, 6))\n",
    "    if gs_x_idx == 1 and group_idx == 0:\n",
    "      activity_ax.text(-1.6, 0.9, \"1\", fontsize=fontsize)\n",
    "      activity_ax.text(-1.6, 0.0, \"0\", fontsize=fontsize)\n",
    "      activity_ax.text(-1.6, -0.9, \"-1\", fontsize=fontsize)\n",
    "    activity_ax.get_yaxis().set_visible(True)\n",
    "    activity_ax.yaxis.grid(True, linestyle='--', which='major',\n",
    "      color='k', alpha=.25)\n",
    "    yrange = max(activity_ax.get_ylim()) - min(activity_ax.get_ylim())\n",
    "    xrange = max(activity_ax.get_xlim()) - min(activity_ax.get_xlim())\n",
    "    aspect = xrange/yrange\n",
    "    activity_ax.set_aspect(aspect)\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(analyzer.analysis_out_dir+\"/vis/\"+analysis_params.model_name+\"_group_invariance_tiled.png\",\n",
    "  transparent=True, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots_y = recons.shape[0]\n",
    "num_plots_x = 2 * recons.shape[1] + 1\n",
    "fig = plt.figure(figsize=[figsize[0], figsize[1]/2])\n",
    "gs = gridspec.GridSpec(num_plots_y, num_plots_x)#, hspace=-0.5, wspace=0.2)\n",
    "for group_idx in range(num_plots_y):\n",
    "  group_gs = gridspec.GridSpecFromSubplotSpec(2, 2, gs[group_idx, 0], wspace=-0.01)\n",
    "  for group_neuron_id in range(analyzer.model.params.num_neurons_per_group):\n",
    "    weight_ax = pf.clear_axis(fig.add_subplot(group_gs[group_neuron_id]))\n",
    "    weight = group_weights[:, select_group_indices[group_idx], group_neuron_id]\n",
    "    weight = dp.normalize_data_with_max(weight)[0].reshape([analyzer.model_params.patch_edge_size]*2)\n",
    "    weight_ax.imshow(weight, cmap=\"Greys_r\", vmin=-1, vmax=1)\n",
    "  for image_idx, gs_x_idx in enumerate(range(1, num_plots_x, 2)):\n",
    "    recon_ax = pf.clear_axis(fig.add_subplot(gs[group_idx, gs_x_idx]))\n",
    "    recon = recons[group_idx, image_idx, ...]\n",
    "    recon = dp.normalize_data_with_max(recon)[0].reshape([analyzer.model_params.patch_edge_size]*2)\n",
    "    recon_ax.imshow(recon, cmap=\"Greys_r\", vmin=-1, vmax=1)\n",
    "    inner_gs = gridspec.GridSpecFromSubplotSpec(2, 2, gs[group_idx, gs_x_idx+1])#, wspace=0.05)\n",
    "    vmin = np.min(angle_vectors)\n",
    "    vmax = np.max(angle_vectors)\n",
    "    angle_images = np.ones((analyzer.model_params.num_neurons_per_group,\n",
    "      analyzer.model_params.patch_edge_size, analyzer.model_params.patch_edge_size))\n",
    "    for group_neuron_id in range(analyzer.model.params.num_neurons_per_group):\n",
    "      activity_ax = pf.clear_axis(fig.add_subplot(inner_gs[group_neuron_id]))\n",
    "      angle_images[group_neuron_id, ...] *= angle_vectors[group_idx, image_idx, group_neuron_id]\n",
    "      activity_ax.imshow(angle_images[group_neuron_id, ...], cmap=\"Greys_r\", vmin=np.min(angle_vectors), vmax=np.max(angle_vectors))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group phase & orientation invariance measurements"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_groups = 10#analyzer.model.params.num_groups\n",
    "num_phases = 32\n",
    "phases = np.linspace(-np.pi, np.pi, num_phases)\n",
    "contrast = 1.0\n",
    "diameter = 0\n",
    "\n",
    "phase_gratings = []\n",
    "for group_neuron_idx in range(num_groups):\n",
    "  neuron_indices = analyzer.model.module.group_ids[group_neuron_idx] # list of neurons in group\n",
    "  orientations = []\n",
    "  spatial_frequencies = []\n",
    "  for neuron_idx in neuron_indices:\n",
    "    orientation = np.pi - analyzer.bf_stats[\"ellipse_orientations\"][neuron_idx]\n",
    "    orientations.append(orientation)\n",
    "    spatial_frequency = analyzer.bf_stats[\"spatial_frequencies\"][neuron_idx]\n",
    "    spatial_frequencies.append(spatial_frequency) \n",
    "  group_orientation = np.mean(orientations)\n",
    "  group_sf = np.mean(spatial_frequencies)\n",
    "  group_phases = []\n",
    "  for phase in phases:\n",
    "      params = list(dp.get_grating_params(analyzer.bf_stats, neuron_idx, orientation=group_orientation,\n",
    "        phase=phase, contrast=contrast, diameter=diameter))\n",
    "      #params[3] = group_orientation\n",
    "      params[4] = group_sf\n",
    "      grating = dp.generate_grating(*params)\n",
    "      group_phases.append(grating)\n",
    "  phase_gratings.append(np.stack(group_phases, axis=0))\n",
    "phase_gratings = np.stack(phase_gratings, axis=0) #[num_groups, num_phases, 16, 16]\n",
    "all_gratings = np.reshape(phase_gratings, [num_groups * num_phases, 16, 16]).reshape([num_groups*num_phases, 256])\n",
    "\n",
    "#set up session\n",
    "group_activations = analyzer.evaluate_tf_tensor(\n",
    "  tensor=analyzer.model.get_group_activity(),\n",
    "  feed_dict=analyzer.model.get_feed_dict(all_gratings, is_test=True))\n",
    "group_activations = group_activations.reshape((num_groups, num_phases, analyzer.model.params.num_groups))[:, :, :num_groups]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_plot_groups = 5\n",
    "group_ids = list(range(num_plot_groups))\n",
    "#max_phase_id = np.argmax(group_activations[group_ids, :, group_ids])\n",
    "#mean_activations = np.mean(group_activations, axis=1)\n",
    "\n",
    "group_id = 3\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(phases, group_activations[group_id, :, group_id])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_file_loc = analyzer.analysis_out_dir+\"savefiles/group_ot_responses_\"+analyzer.analysis_params.save_info+\".npz\"\n",
    "ot_grating_responses = np.load(tuning_file_loc, allow_pickle=True)[\"data\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_id = 1\n",
    "\n",
    "best_or_id = np.argwhere(ot_grating_responses[\"orientations\"] == ot_grating_responses[\"best_orientations\"][group_id, -1]).item()\n",
    "#or_diffs = ot_grating_responses[\"orientations\"] - ot_grating_responses[\"orientations\"][best_or_id]\n",
    "num_or = len(ot_grating_responses[\"orientations\"])\n",
    "or_x_vals = np.linspace(-num_or/2, num_or/2, num_or)\n",
    "\n",
    "best_ph_id = np.argwhere(ot_grating_responses[\"phases\"] == ot_grating_responses[\"best_phases\"][group_id, -1, best_or_id]).item()\n",
    "#ph_diffs = ot_grating_responses[\"phases\"] - ot_grating_responses[\"phases\"][best_ph_id]\n",
    "num_ph = len(ot_grating_responses[\"phases\"])\n",
    "ph_x_vals = np.linspace(-num_ph/2, num_ph/2, num_ph)\n",
    "\n",
    "or_resp = np.roll(ot_grating_responses[\"responses\"][group_id, -1, :, best_ph_id], best_or_id)\n",
    "ph_resp = np.roll(ot_grating_responses[\"responses\"][group_id, -1, best_or_id, :], best_ph_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"ICA\", \"ISA\", \"SLCA\"]\n",
    "num_plots_y = len(models)\n",
    "num_plots_x = 2 # Phase, Orientation\n",
    "\n",
    "fig = plt.figure(figsize=[figsize[0], figsize[1]])\n",
    "gs = gridspec.GridSpec(num_plots_y, num_plots_x)#, hspace=-0.5, wspace=0.2)\n",
    "for y_idx, model_name in enumerate(models):\n",
    "  ax = fig.add_subplot(gs[y_idx, 0])\n",
    "  ax.plot(ph_x_vals, ph_resp, color='k')\n",
    "  #ax.set_xticks(or_x_vals)\n",
    "  if y_idx == 0:\n",
    "    ax.set_title(\"Varying phase\", fontsize=fontsize)\n",
    "\n",
    "  ax = fig.add_subplot(gs[y_idx, 1])\n",
    "  ax.plot(or_x_vals, or_resp, color='k')\n",
    "  #ax.set_xticks(ph_x_vals)\n",
    "  ax.set_ylabel(model_name, rotation=\"vertical\", fontsize=fontsize)\n",
    "  if y_idx == 0:\n",
    "    ax.set_title(\"Varying orientation\", fontsize=fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full image recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_image = (\n",
    "  (analyzer.full_image - np.min(analyzer.full_image))\n",
    "  / (np.max(analyzer.full_image) - np.min(analyzer.full_image))).astype(np.float32)\n",
    "\n",
    "normed_recon = (\n",
    "  (analyzer.full_recon - np.min(analyzer.full_recon))\n",
    "  / (np.max(analyzer.full_recon) - np.min(analyzer.full_recon))).astype(np.float32)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=figsize)\n",
    "ax[0] = pf.clear_axis(ax[0])\n",
    "ax[0].imshow(np.squeeze(normed_image), cmap=\"Greys_r\")\n",
    "ax[0].set_title(\"Input Image\", fontsize=fontsize)\n",
    "ax[1] = pf.clear_axis(ax[1])\n",
    "ax[1].imshow(np.squeeze(normed_recon), cmap=\"Greys_r\")\n",
    "percent_active = \"{:.2f}\".format(analyzer.recon_frac_act*100)\n",
    "psnr = \"{:.2f}\".format(compare_psnr(normed_image, normed_recon, data_range=1))\n",
    "ax[1].set_title(\"Reconstruction\\n\"+percent_active+\" percent active\"+\"\\n\"+\"PSNR = \"+psnr, fontsize=fontsize)\n",
    "plt.show()\n",
    "fig.savefig(analyzer.analysis_out_dir+\"/vis/\"+analysis_params.model_name+\"_image_recon.png\",\n",
    "  transparent=True, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group coactivation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ds.get_data(analyzer.model_params)\n",
    "data = analyzer.model.preprocess_dataset(data, analyzer.model_params)\n",
    "data = analyzer.model.reshape_dataset(data, analyzer.model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs = 10000#data[\"train\"].images.shape[0]\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config, graph=analyzer.model.graph) as sess:\n",
    "  feed_dict = analyzer.model.get_feed_dict(data[\"train\"].images[0:num_imgs,...])\n",
    "  sess.run(analyzer.model.init_op, feed_dict)\n",
    "  analyzer.model.load_full_model(sess, analyzer.analysis_params.cp_loc)\n",
    "  #run_list = [analyzer.model.a]\n",
    "  #neuron_activations = sess.run(run_list, feed_dict)\n",
    "  run_list = [analyzer.model.a, analyzer.model.module.group_activity, analyzer.model.module.group_angles]\n",
    "  neuron_activations, group_activations, group_angles = sess.run(run_list, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cent_activations = neuron_activations - np.mean(neuron_activations)\n",
    "\n",
    "cent_group_activations = np.zeros_like(group_activations)\n",
    "for group_id in range(analyzer.model.module.num_groups):\n",
    "  cent_group_activations[:,group_id] = group_activations[:,group_id] - np.mean(group_activations[:,group_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cov = 1/num_imgs * np.dot(group_activations.T, group_activations)\n",
    "\n",
    "cov = 1/num_imgs * np.dot(np.squeeze(cent_group_activations).T, np.squeeze(cent_group_activations))\n",
    "np.fill_diagonal(cov, 0.0)\n",
    "\n",
    "#cov = 1/num_imgs * np.dot(np.squeeze(cent_activations).T, np.squeeze(cent_activations))\n",
    "#np.fill_diagonal(cov, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=figsize)\n",
    "ax = pf.clear_axis(fig.add_subplot())\n",
    "im = ax.imshow(cov, cmap=\"Greys_r\")\n",
    "vmin = np.min(cov)\n",
    "vmax = np.max(cov)\n",
    "vmean = vmax - ((vmax - vmin) / 2)\n",
    "pf.add_colorbar_to_im(im, aspect=20, ticks=[vmin, vmean, vmax], labelsize=fontsize)\n",
    "ax.set_title(\"Covariance matrix for group activations\", fontsize=fontsize)\n",
    "#ax.set_title(\"Covariance matrix for activations\", fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(analyzer.analysis_out_dir+\"/vis/\"+analysis_params.model_name+\"_group_activity_covariance.png\",\n",
    "  transparent=True, bbox_inches=\"tight\", pad_inches=0.01)\n",
    "#fig.savefig(analyzer.analysis_out_dir+\"/vis/\"+analysis_params.model_name+\"_activity_covariance.png\",\n",
    "#  transparent=True, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_idx = 1\n",
    "num_bins = 1000\n",
    "indiv_group_act = group_activations[:, group_idx]\n",
    "bins = np.linspace(np.min(indiv_group_act), np.max(indiv_group_act), num_bins)\n",
    "hist, bin_edges = np.histogram(indiv_group_act.flatten(), bins)\n",
    "bin_left, bin_right = bin_edges[:-1], bin_edges[1:]\n",
    "bin_centers = bin_left + (bin_right - bin_left)/2\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=figsize)\n",
    "ax.bar(bin_centers, hist, width=2.0, log=True, align=\"center\", color='k')\n",
    "ax.set_xticks(bin_left, minor=True)\n",
    "ax.set_xticks(bin_left[::10], minor=False)\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter(\"%0.0f\"))\n",
    "ax.set_xlim([np.min(indiv_group_act), np.max(indiv_group_act)])\n",
    "vmin = np.min(indiv_group_act)\n",
    "vmax = np.max(indiv_group_act)\n",
    "vmean = vmax - ((vmax - vmin)/2)\n",
    "ax.set_xticks([vmin, vmean, vmax])\n",
    "ax.set_title(\"Activity histogram of group \"+str(group_idx)+\" for \"+str(len(indiv_group_act))+\" images\", fontsize=fontsize)\n",
    "ax.set_xlabel(\"Activation\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"Log Count\", fontsize=fontsize)\n",
    "ax.tick_params(\"both\", labelsize=fontsize)\n",
    "plt.show()\n",
    "fig.savefig(analyzer.analysis_out_dir+\"/vis/\"+analysis_params.model_name+\"_group\"+str(group_idx)+\"_activity_hist.png\",\n",
    "  transparent=True, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 500\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_subplot()\n",
    "for neuron_id in range(neuron_activations.shape[1]):\n",
    "  ax.scatter([neuron_id]*num_images, neuron_activations[:num_images, neuron_id], s=0.8, alpha=0.01, color='k')\n",
    "ax.set_title(\"Individual Neuron Activity\", fontsize=fontsize)\n",
    "ax.set_xlabel(\"Neuron Index\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"Activity\", fontsize=fontsize)\n",
    "ax.tick_params(\"both\", labelsize=fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight analysis plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pf.plot_loc_freq_summary(analyzer.bf_stats, spotsize=9, figsize=[figsize[0], figsize[1]/3], fontsize=fontsize)\n",
    "fig.savefig(analyzer.analysis_out_dir+\"/vis/\"+analysis_params.model_name+\"_location_frequency_centers.png\",\n",
    "  transparent=True, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.neurn_angles, analyzer.plot_matrix = analyzer.get_neuron_angles(analyzer.bf_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_hist_fig = pf.plot_weight_angle_histogram(analyzer.neuron_angles, num_bins=50, angle_min=0, angle_max=180, figsize=(8,8))\n",
    "angle_hist_fig.savefig(analyzer.analysis_out_dir+\"/vis/neuron_angle_histogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = analyzer.evals[\"lca_subspace/weights/w:0\"]\n",
    "weight_indices = np.stack([np.array(id_list) for id_list in analyzer.model.module.group_ids], axis=0)\n",
    "pooling_weights = np.stack([weights[:, id_list] for id_list in analyzer.model.module.group_ids], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.reshape(analyzer.evals[\"lca_subspace/weights/w:0\"].T, [analyzer.model.params.num_neurons,\n",
    "      int(np.sqrt(analyzer.model.params.num_pixels)), int(np.sqrt(analyzer.model.params.num_pixels))])\n",
    "weight_fig = pf.plot_group_weights(np.squeeze(weights), analyzer.model.module.group_ids,\n",
    "  title=\"Dictionary\", figsize=(18,18))\n",
    "weight_fig.savefig(analyzer.analysis_out_dir+\"/vis/\"+\"group_phi.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debugging code for bf_stats spatial freq & orientation\n",
    "\n",
    "todo: make a wavelett dictionary, compute the bf stats in that"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bleh = np.random.randint(0, 20, (5,5))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(bleh)\n",
    "N = 5\n",
    "#f = (2/N) * np.pi * np.linspace(-N/2.0, N/2.0, N)\n",
    "fx = np.linspace(-2.0, 2.0, N)\n",
    "fy = np.linspace(2.0, -2.0, N)\n",
    "print(\"x_vals:\", fx, \"\\ny_vals:\", fy)\n",
    "max_ys = np.argmax(np.abs(bleh), axis=0) # Returns row index for each col\n",
    "print(\"max_ys:\",max_ys)\n",
    "max_x = np.argmax(np.max(np.abs(bleh), axis=0))\n",
    "print(\"max_x:\",max_x)\n",
    "print(\"max_y:\",max_ys[max_x])\n",
    "# Convert peak amplitude location into angle in freq domain\n",
    "fx_max = fx[max_x]\n",
    "fy_max = fy[max_ys[max_x]]\n",
    "print((fx_max, fy_max))\n",
    "theta_max = np.arctan(fy_max/fx_max) * 180 / np.pi\n",
    "print(theta_max)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def hilbert_amplitude(weights, padding=None):\n",
    "  \"\"\"\n",
    "  Compute Hilbert amplitude envelope of weight matrix\n",
    "  Inputs:\n",
    "    weights: [np.ndarray] of shape [num_inputs, num_outputs]\n",
    "      num_inputs must have an even square root\n",
    "    padding: [int] specifying how much 0-padding to use for FFT\n",
    "      default is the closest power of 2 of sqrt(num_inputs)\n",
    "  Outputs:\n",
    "    env: [np.ndarray] of shape [num_outputs, num_inputs]\n",
    "      Hilbert envelope\n",
    "    bff_filt: [np.ndarray] of shape [num_outputs, padded_num_inputs]\n",
    "      Filtered Fourier transform of basis function\n",
    "    hil_filt: [np.ndarray] of shape [num_outputs, sqrt(num_inputs), sqrt(num_inputs)]\n",
    "      Hilbert filter to be applied in Fourier space\n",
    "    bffs: [np.ndarray] of shape [num_outputs, padded_num_inputs, padded_num_inputs]\n",
    "      Fourier transform of input weights\n",
    "  \"\"\"\n",
    "  cart2pol = lambda x,y: (np.arctan2(y,x), np.hypot(x, y))\n",
    "  num_inputs, num_outputs = weights.shape\n",
    "  assert np.sqrt(num_inputs) == np.floor(np.sqrt(num_inputs)), (\n",
    "    \"weights.shape[0] must have an even square root.\")\n",
    "  patch_edge_size = int(np.sqrt(num_inputs))\n",
    "  if padding is None or padding <= patch_edge_size:\n",
    "    # Amount of zero padding for fft2 (closest power of 2)\n",
    "    N = np.int(2**(np.ceil(np.log2(patch_edge_size))))\n",
    "  else:\n",
    "    N = np.int(padding)\n",
    "  # Analytic signal envelope for weights\n",
    "  # (Hilbet transform of each basis function)\n",
    "  env = np.zeros((num_outputs, num_inputs), dtype=complex)\n",
    "  # Fourier transform of weights\n",
    "  bffs = np.zeros((num_outputs, N, N), dtype=complex)\n",
    "  # Filtered Fourier transform of weights\n",
    "  bff_filt = np.zeros((num_outputs, N**2), dtype=complex)\n",
    "  # Hilbert filters\n",
    "  hil_filt = np.zeros((num_outputs, N, N))\n",
    "  # Grid for creating filter\n",
    "  fx = (2/N) * np.pi * np.linspace(-N/2.0, N/2.0, N)\n",
    "  fy = (2/N) * np.pi * np.linspace(N/2.0, -N/2.0, N)\n",
    "  (fx_mesh, fy_mesh) = np.meshgrid(fx, fy)\n",
    "  (theta, r) = cart2pol(fx_mesh, fy_mesh)\n",
    "  for neuron_idx in range(num_outputs):\n",
    "    # Grab single basis function, reshape to a square image\n",
    "    bf = weights[:, neuron_idx].reshape(patch_edge_size, patch_edge_size)\n",
    "    # Convert basis function into DC-centered Fourier domain\n",
    "    bff = np.fft.fftshift(np.fft.fft2(bf-np.mean(bf), [N, N]))\n",
    "    bffs[neuron_idx, ...] = bff\n",
    "    # Find indices of the peak amplitude\n",
    "    max_ys = np.argmax(np.abs(bff), axis=0) # Returns row index for each col\n",
    "    max_x = np.argmax(np.max(np.abs(bff), axis=0))\n",
    "    # Convert peak amplitude location into angle in freq domain\n",
    "    fx_max = fx[max_x]\n",
    "    fy_max = fy[max_ys[max_x]]\n",
    "    theta_max = np.arctan2(fy_max, fx_max)\n",
    "    # Define the half-plane with respect to the maximum\n",
    "    ang_diff = np.abs(theta-theta_max)\n",
    "    idx = (ang_diff>np.pi).nonzero()\n",
    "    ang_diff[idx] = 2.0 * np.pi - ang_diff[idx]\n",
    "    hil_filt[neuron_idx, ...] = (ang_diff < np.pi/2.0).astype(int)\n",
    "    # Create analytic signal from the inverse FT of the half-plane filtered bf\n",
    "    abf = np.fft.ifft2(np.fft.fftshift(hil_filt[neuron_idx, ...]*bff))\n",
    "    env[neuron_idx, ...] = abf[0:patch_edge_size, 0:patch_edge_size].reshape(num_inputs)\n",
    "    bff_filt[neuron_idx, ...] = (hil_filt[neuron_idx, ...]*bff).reshape(N**2)\n",
    "  return (env, bff_filt, hil_filt, bffs)\n",
    "\n",
    "def get_dictionary_stats(weights, padding=None, num_gauss_fits=20, gauss_thresh=0.2):\n",
    "  \"\"\"\n",
    "  Compute summary statistics on dictionary elements using Hilbert amplitude envelope\n",
    "  Inputs:\n",
    "    weights: [np.ndarray] of shape [num_inputs, num_outputs]\n",
    "    padding: [int] total image size to pad out to in the FFT computation\n",
    "    num_gauss_fits: [int] total number of attempts to make when fitting the BFs\n",
    "    gauss_thresh: All probability values below gauss_thresh*mean(gauss_fit) will be\n",
    "      considered outliers for repeated fits\n",
    "  Outputs:\n",
    "    The function output is a dictionary containing the keys for each type of analysis\n",
    "    Each key dereferences a list of len num_outputs (i.e. one entry for each weight vector)\n",
    "    The keys and their list entries are as follows:\n",
    "      basis_functions: [np.ndarray] of shape [patch_edge_size, patch_edge_size]\n",
    "      envelopes: [np.ndarray] of shape [N, N], where N is the amount of padding\n",
    "        for the hilbert_amplitude function\n",
    "      envelope_centers: [tuples of ints] indicating the (y, x) position of the\n",
    "        center of the Hilbert envelope\n",
    "      gauss_fits: [list of np.ndarrays] containing (gaussian_fit, grid) where gaussian_fit\n",
    "        is returned from get_gauss_fit and specifies the 2D Gaussian PDF fit to the Hilbert\n",
    "        envelope and grid is a tuple containing (y,x) points with which the Gaussian PDF\n",
    "        can be plotted\n",
    "      gauss_centers: [list of ints] containing the (y,x) position of the center of\n",
    "        the Gaussian fit\n",
    "      gauss_orientations: [list of np.ndarrays] containing the (eigenvalues, eigenvectors) of\n",
    "        the covariance matrix for the Gaussian fit of the Hilbert amplitude envelope. They are\n",
    "        both sorted according to the highest to lowest Eigenvalue.\n",
    "      fourier_centers: [list of ints] containing the (y,x) position of the center (max) of\n",
    "        the Fourier amplitude map\n",
    "      num_inputs: [int] dim[0] of input weights\n",
    "      num_outputs: [int] dim[1] of input weights\n",
    "      patch_edge_size: [int] int(floor(sqrt(num_inputs)))\n",
    "      areas: [list of floats] area of enclosed ellipse\n",
    "      spatial_frequncies: [list of floats] dominant spatial frequency for basis function\n",
    "  \"\"\"\n",
    "  envelope, bff_filt, hil_filter, bffs = hilbert_amplitude(weights, padding)\n",
    "  num_inputs, num_outputs = weights.shape\n",
    "  patch_edge_size = np.int(np.floor(np.sqrt(num_inputs)))\n",
    "  basis_funcs = [None]*num_outputs\n",
    "  envelopes = [None]*num_outputs\n",
    "  gauss_fits = [None]*num_outputs\n",
    "  gauss_centers = [None]*num_outputs\n",
    "  diameters = [None]*num_outputs\n",
    "  gauss_orientations = [None]*num_outputs\n",
    "  envelope_centers = [None]*num_outputs\n",
    "  fourier_centers = [None]*num_outputs\n",
    "  ellipse_orientations = [None]*num_outputs\n",
    "  fourier_maps = [None]*num_outputs\n",
    "  spatial_frequencies = [None]*num_outputs\n",
    "  areas = [None]*num_outputs\n",
    "  phases = [None]*num_outputs\n",
    "  for bf_idx in range(num_outputs):\n",
    "    # Reformatted individual basis function\n",
    "    basis_funcs[bf_idx] = np.squeeze(dp.reshape_data(weights.T[bf_idx,...],\n",
    "      flatten=False)[0])\n",
    "    # Reformatted individual envelope filter\n",
    "    envelopes[bf_idx] = np.squeeze(dp.reshape_data(np.abs(envelope[bf_idx,...]),\n",
    "      flatten=False)[0])\n",
    "    # Basis function center\n",
    "    max_ys = envelopes[bf_idx].argmax(axis=0) # Returns row index for each col\n",
    "    max_x = np.argmax(envelopes[bf_idx].max(axis=0))\n",
    "    y_cen = max_ys[max_x]\n",
    "    x_cen = max_x\n",
    "    envelope_centers[bf_idx] = (y_cen, x_cen)\n",
    "    # Gaussian fit to Hilbet amplitude envelope\n",
    "    gauss_fit, grid, gauss_mean, gauss_cov = dp.get_gauss_fit(envelopes[bf_idx],\n",
    "      num_gauss_fits, gauss_thresh)\n",
    "    gauss_fits[bf_idx] = (gauss_fit, grid)\n",
    "    gauss_centers[bf_idx] = gauss_mean\n",
    "    evals, evecs = np.linalg.eigh(gauss_cov)\n",
    "    sort_indices = np.argsort(evals)[::-1]\n",
    "    gauss_orientations[bf_idx] = (evals[sort_indices], evecs[:,sort_indices])\n",
    "    width, height = evals[sort_indices] # Width & height are relative to orientation\n",
    "    diameters[bf_idx] = np.sqrt(width**2+height**2)\n",
    "    # Fourier function center, spatial frequency, orientation\n",
    "    fourier_map = np.sqrt(np.real(bffs[bf_idx, ...])**2+np.imag(bffs[bf_idx, ...])**2)\n",
    "    fourier_maps[bf_idx] = fourier_map\n",
    "    N = fourier_map.shape[0]\n",
    "    center_freq = int(np.floor(N/2))\n",
    "    fourier_map[center_freq, center_freq] = 0 # remove DC component\n",
    "    max_fys = np.argmax(fourier_map, axis=0)\n",
    "    max_fx = np.argmax(np.max(fourier_map, axis=0))\n",
    "    ##fx = (2/N) * np.pi * np.linspace(-N/2.0, N/2.0, N)\n",
    "    ##fy = (2/N) * np.pi * np.linspace(N/2.0, -N/2.0, N)\n",
    "    ##fx_max = fx[max_x]\n",
    "    ##fy_max = fy[max_ys[max_x]]\n",
    "    ##theta_max = np.arctan2(fy_max, fx_max)\n",
    "    #fx = np.arange(0, N, 1)\n",
    "    #fy = np.arange(N, 0, -1)\n",
    "    fx = (2/N) * np.pi * np.linspace(-N/2.0, N/2.0, N)\n",
    "    fy = (2/N) * np.pi * np.linspace(N/2.0, -N/2.0, N)\n",
    "    fy_cen = fx[max_fx]\n",
    "    fx_cen = fy[max_fys[max_fx]]\n",
    "    fourier_centers[bf_idx] = [fy_cen, fx_cen]\n",
    "    #fy_cen = (max_fys[max_fx] - (N/2)) * (patch_edge_size/N)\n",
    "    #fx_cen = (max_fx - (N/2)) * (patch_edge_size/N)\n",
    "    #fourier_centers[bf_idx] = [fy_cen, fx_cen]\n",
    "    # NOTE: we flip fourier_centers because fx_cen is the peak of the x frequency, which would be an y coordinate\n",
    "    #ellipse_orientations[bf_idx] = np.arctan2(*fourier_centers[bf_idx][::-1])\n",
    "    ellipse_orientations[bf_idx] = np.arctan2(fx_cen, fy_cen)\n",
    "    #ellipse_orientations[bf_idx] = np.arctan(fx_cen/fy_cen)\n",
    "    spatial_frequencies[bf_idx] = np.sqrt(fy_cen**2 + fx_cen**2)\n",
    "    areas[bf_idx] = np.pi * np.prod(evals)\n",
    "    phases[bf_idx] = np.angle(bffs[bf_idx])[y_cen, x_cen]\n",
    "  output = {\"basis_functions\":basis_funcs, \"envelopes\":envelopes, \"gauss_fits\":gauss_fits,\n",
    "    \"gauss_centers\":gauss_centers, \"gauss_orientations\":gauss_orientations, \"areas\":areas,\n",
    "    \"fourier_centers\":fourier_centers, \"fourier_maps\":fourier_maps, \"num_inputs\":num_inputs,\n",
    "    \"spatial_frequencies\":spatial_frequencies, \"envelope_centers\":envelope_centers,\n",
    "    \"num_outputs\":num_outputs, \"patch_edge_size\":patch_edge_size, \"phases\":phases,\n",
    "    \"ellipse_orientations\":ellipse_orientations, \"diameters\":diameters}\n",
    "  return output\n",
    "\n",
    "def plot_loc_freq_summary(bf_stats, spotsize=10, figsize=(15, 5), fontsize=16):\n",
    "  plt.rc('text', usetex=True)\n",
    "  fig = plt.figure(figsize=figsize)\n",
    "  gs = fig.add_gridspec(1, 3, wspace=0.3)\n",
    "  ax = fig.add_subplot(gs[0])\n",
    "  x_pos = [x for (y,x) in bf_stats[\"gauss_centers\"]]\n",
    "  y_pos = [y for (y,x) in bf_stats[\"gauss_centers\"]]\n",
    "  ax.scatter(x_pos, y_pos, color='k', s=spotsize)\n",
    "  ax.set_xlim([0, bf_stats[\"patch_edge_size\"]-1])\n",
    "  ax.set_ylim([bf_stats[\"patch_edge_size\"]-1, 0])\n",
    "  ax.xaxis.set_major_locator(matplotlib.ticker.MaxNLocator(integer=True))\n",
    "  ax.yaxis.set_major_locator(matplotlib.ticker.MaxNLocator(integer=True))\n",
    "  ax.set_aspect(\"equal\")\n",
    "  ax.set_ylabel(\"Pixels\", fontsize=fontsize)\n",
    "  ax.set_xlabel(\"Pixels\", fontsize=fontsize)\n",
    "  ax.set_title(\"Centers\", fontsize=fontsize, pad=32)\n",
    "  ax = fig.add_subplot(gs[1])\n",
    "  num_bins = 15\n",
    "  bins = np.linspace(0, np.max(bf_stats[\"spatial_frequencies\"]), num_bins)\n",
    "  bin_width = bins[1] - bins[0] / 2\n",
    "  ax.hist(bf_stats[\"spatial_frequencies\"], bins=bins, align=\"mid\", rwidth=bin_width, color=\"k\")\n",
    "  ax.set_xlabel(\"Cycles / Patch\", fontsize=fontsize)\n",
    "  ax.set_ylabel(\"Count\", fontsize=fontsize)\n",
    "  yrange = max(ax.get_ylim()) - min(ax.get_ylim())\n",
    "  xrange = max(ax.get_xlim()) - min(ax.get_xlim())\n",
    "  aspect = xrange/yrange\n",
    "  ax.set_aspect(aspect)\n",
    "  #x_sf = [x for (y,x) in bf_stats[\"fourier_centers\"]]\n",
    "  #y_sf = [y for (y,x) in bf_stats[\"fourier_centers\"]]\n",
    "  #max_sf = np.max(np.abs(x_sf+y_sf))\n",
    "  #ax.scatter(y_sf, x_sf, color='k', s=spotsize)\n",
    "  ##ax.set_xlim([-bf_stats[\"patch_edge_size\"]/2, bf_stats[\"patch_edge_size\"]/2])\n",
    "  ##ax.set_ylim([-bf_stats[\"patch_edge_size\"]/2, bf_stats[\"patch_edge_size\"]/2])\n",
    "  #ax.set_xlim([-max_sf, max_sf])\n",
    "  #ax.set_ylim([-max_sf, max_sf])\n",
    "  #ax.xaxis.set_major_locator(matplotlib.ticker.MaxNLocator(integer=True))\n",
    "  #ax.yaxis.set_major_locator(matplotlib.ticker.MaxNLocator(integer=True))\n",
    "  #ax.set_aspect(\"equal\")\n",
    "  #ax.set_ylabel(\"Cycles / Patch\", fontsize=fontsize)\n",
    "  #ax.set_xlabel(\"Cycles / Patch\", fontsize=fontsize)\n",
    "  ax.set_title(\"Spatial Frequencies\", fontsize=fontsize, pad=32)\n",
    "  num_bins = 360\n",
    "  fyx = bf_stats[\"fourier_centers\"]\n",
    "  #orientations = [np.arctan2(fy, fx) for (fy, fx) in fyx]\n",
    "  orientations = [np.arctan2(fx, fy) for (fy, fx) in fyx]\n",
    "  #orientations = [np.arctan(fy/fx) for (fy, fx) in fyx]\n",
    "  #orientations = [np.arctan(fx/fy) for (fy, fx) in fyx]\n",
    "  #orientations = bf_stats[\"ellipse_orientations\"]\n",
    "  #orientations = [np.pi + orientation\n",
    "  #  for orientation in [np.arctan2(*fyx[::-1]) for fyx in bf_stats[\"fourier_centers\"]]]\n",
    "  bins = np.linspace(0, 2*np.pi, num_bins)\n",
    "  count, bin_edges = np.histogram(orientations, bins)\n",
    "  count = count / np.max(count)\n",
    "  bin_left, bin_right = bin_edges[:-1], bin_edges[1:]\n",
    "  bin_centers = bin_left + (bin_right - bin_left)/2\n",
    "  ax = fig.add_subplot(gs[2], polar=True)\n",
    "  ax.plot(bin_centers, count, linewidth=3, color='k')\n",
    "  ax.set_yticks([])\n",
    "  ax.set_thetamin(0)\n",
    "  ax.set_thetamax(2*np.pi)\n",
    "  ax.set_xticks([0, np.pi/4, 2*np.pi/4, 3*np.pi/4, 4*np.pi/4,\n",
    "    5*np.pi/4, 6*np.pi/4, 7*np.pi/4, 2*np.pi])\n",
    "  ax.set_xticklabels([r\"0\", r\"$\\frac{\\pi}{4}$\", r\"$\\frac{\\pi}{2}$\",\n",
    "    r\"$\\frac{3\\pi}{4}$\", r\"$\\pi$\", r\"$\\frac{5\\pi}{4}$\", r\"$\\frac{3\\pi}{2}$\",\n",
    "    r\"$\\frac{7\\pi}{4}$\"], fontsize=fontsize)\n",
    "  ax.set_title(\"Orientaitons\", fontsize=fontsize, pad=23)\n",
    "  plt.show()\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "weights = np.stack(analyzer.bf_stats[\"basis_functions\"],\n",
    "  axis=-1).reshape(analyzer.model.params.num_pixels, analyzer.model.params.num_neurons)\n",
    "new_bf_stats = get_dictionary_stats(weights,\n",
    "  padding=analyzer.analysis_params.ft_padding,\n",
    "  num_gauss_fits=analyzer.analysis_params.num_gauss_fits,\n",
    "  gauss_thresh=analyzer.analysis_params.gauss_thresh)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = plot_loc_freq_summary(new_bf_stats, spotsize=10,\n",
    "  figsize=(figsize[0], figsize[1]/3), fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = pf.plot_loc_freq_summary(analyzer.bf_stats, spotsize=10,\n",
    "  figsize=(figsize[0], figsize[1]/3), fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plot_ellipse(axis, center, shape, angle, color_val=\"auto\", alpha=1.0, lines=False,\n",
    "  fill_ellipse=False):\n",
    "  \"\"\"\n",
    "  Add an ellipse to given axis\n",
    "  Inputs:\n",
    "    axis [matplotlib.axes._subplots.AxesSubplot] axis on which ellipse should be drawn\n",
    "    center [tuple or list] specifying [y, x] center coordinates\n",
    "    shape [tuple or list] specifying [width, height] shape of ellipse\n",
    "    angle [float] specifying angle of ellipse\n",
    "    color_val [matplotlib color spec] specifying the color of the edge & face of the ellipse\n",
    "    alpha [float] specifying the transparency of the ellipse\n",
    "    lines [bool] if true, output will be a line, where the secondary axis of the ellipse\n",
    "      is collapsed\n",
    "    fill_ellipse [bool] if true and lines is false then a filled ellipse will be plotted\n",
    "  Outputs:\n",
    "    ellipse [matplotlib.patches.ellipse] ellipse object\n",
    "  \"\"\"\n",
    "  if fill_ellipse:\n",
    "    face_color_val = \"none\" if color_val==\"auto\" else color_val\n",
    "  else:\n",
    "    face_color_val = \"none\"\n",
    "  y_cen, x_cen = center\n",
    "  width, height = shape\n",
    "  if lines:\n",
    "    min_length = 0.1\n",
    "    if width < height:\n",
    "      width = min_length\n",
    "    elif width > height:\n",
    "      height = min_length\n",
    "  ellipse = matplotlib.patches.Ellipse(xy=[x_cen, y_cen], width=width,\n",
    "    height=height, angle=angle, edgecolor=color_val, facecolor=face_color_val,\n",
    "    alpha=alpha, fill=True)\n",
    "  axis.add_artist(ellipse)\n",
    "  ellipse.set_clip_box(axis.bbox)\n",
    "  return ellipse\n",
    "\n",
    "def plot_ellipse_summaries(bf_stats, num_bf=-1, lines=False, rand_bf=False):\n",
    "  \"\"\"\n",
    "  Plot basis functions with summary ellipses drawn over them\n",
    "  Inputs:\n",
    "    bf_stats [dict] output of dp.get_dictionary_stats()\n",
    "    num_bf [int] number of basis functions to plot (<=0 is all; >total is all)\n",
    "    lines [bool] If true, will plot lines instead of ellipses\n",
    "    rand_bf [bool] If true, will choose a random set of basis functions\n",
    "  \"\"\"\n",
    "  tot_num_bf = len(bf_stats[\"basis_functions\"])\n",
    "  if num_bf <= 0 or num_bf > tot_num_bf:\n",
    "    num_bf = tot_num_bf\n",
    "  SFs = np.asarray([np.sqrt(fcent[0]**2 + fcent[1]**2)\n",
    "    for fcent in bf_stats[\"fourier_centers\"]], dtype=np.float32)\n",
    "  sf_sort_indices = np.argsort(SFs)\n",
    "  if rand_bf:\n",
    "    bf_range = np.random.choice([i for i in range(tot_num_bf)], num_bf, replace=False)\n",
    "  num_plots_y = int(np.ceil(np.sqrt(num_bf)))\n",
    "  num_plots_x = int(np.ceil(np.sqrt(num_bf)))\n",
    "  gs = gridspec.GridSpec(num_plots_y, num_plots_x)\n",
    "  fig = plt.figure(figsize=(17,17))\n",
    "  filter_idx = 0\n",
    "  for plot_id in  np.ndindex((num_plots_y, num_plots_x)):\n",
    "    ax = pf.clear_axis(fig.add_subplot(gs[plot_id]))\n",
    "    if filter_idx < tot_num_bf and filter_idx < num_bf:\n",
    "      if rand_bf:\n",
    "        bf_idx = bf_range[filter_idx]\n",
    "      else:\n",
    "        bf_idx = filter_idx\n",
    "      bf = bf_stats[\"basis_functions\"][bf_idx]\n",
    "      ax.imshow(bf, interpolation=\"Nearest\", cmap=\"Greys_r\")\n",
    "      ax.set_title(str(bf_idx), fontsize=\"8\")\n",
    "      center = bf_stats[\"gauss_centers\"][bf_idx]\n",
    "      evals, evecs = bf_stats[\"gauss_orientations\"][bf_idx]\n",
    "      orientations = bf_stats[\"fourier_centers\"][bf_idx]\n",
    "      angle = np.rad2deg(np.pi/2 + np.arctan2(*orientations))\n",
    "      alpha = 1.0\n",
    "      ellipse = plot_ellipse(ax, center, evals, angle, color_val=\"b\", alpha=alpha, lines=lines)\n",
    "      filter_idx += 1\n",
    "    ax.set_aspect(\"equal\")\n",
    "  plt.show()\n",
    "  return fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
