{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Computation Paper Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from skimage import measure\n",
    "from skimage.io import imread\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "import matplotlib.font_manager\n",
    "import tensorflow as tf\n",
    "from data.dataset import Dataset\n",
    "import data.data_selector as ds\n",
    "import analysis.analysis_picker as ap\n",
    "import utils.data_processing as dp\n",
    "import utils.plot_functions as pf\n",
    "import utils.neural_comp_funcs as nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (16, 16)\n",
    "fontsize = 20\n",
    "dpi = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lca_512_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_512_vh\"\n",
    "    self.display_name = \"Sparse Coding 512\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_768_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_768_vh\"\n",
    "    self.display_name = \"Sparse Coding 768\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_1024_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_1024_vh\"\n",
    "    self.display_name = \"Sparse Coding 1024\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_2560_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_2560_vh\"\n",
    "    self.display_name = \"Sparse Coding\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class sae_768_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"sae\"\n",
    "    self.model_name = \"sae_768_vh\"\n",
    "    self.display_name = \"Sparse Autoencoder\"\n",
    "    self.version = \"1.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class rica_768_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"rica\"\n",
    "    self.model_name = \"rica_768_vh\"\n",
    "    self.display_name = \"Linear Autoencoder\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class ae_768_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"ae\"\n",
    "    self.model_name = \"ae_768_vh\"\n",
    "    self.display_name = \"ReLU\"\n",
    "    self.version = \"1.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_768_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_768_mnist\"\n",
    "    self.display_name = \"Sparse Coding 768\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_1536_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_1536_mnist\"\n",
    "    self.display_name = \"Sparse Coding 1536\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class ae_768_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"ae\"\n",
    "    self.model_name = \"ae_768_mnist\"\n",
    "    self.display_name = \"Leaky ReLU\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class sae_768_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"sae\"\n",
    "    self.model_name = \"sae_768_mnist\"\n",
    "    self.display_name = \"Sparse Autoencoder\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class rica_768_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"rica\"\n",
    "    self.model_name = \"rica_768_mnist\"\n",
    "    self.display_name = \"Linear Autoencoder\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class ae_deep_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"ae\"\n",
    "    self.model_name = \"ae_deep_mnist\"\n",
    "    self.display_name = \"Leaky ReLU\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_levels = 10\n",
    "color_vals = dict(zip([\"lt_green\", \"md_green\", \"dk_green\", \"lt_blue\", \"md_blue\", \"dk_blue\", \"lt_red\", \"md_red\", \"dk_red\"],\n",
    "  [\"#A9DFBF\", \"#196F3D\", \"#27AE60\", \"#AED6F1\", \"#3498DB\", \"#21618C\", \"#F5B7B1\", \"#E74C3C\", \"#943126\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iso-contour activations comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = [rica_768_vh_params(), ae_768_vh_params(), sae_768_vh_params(), lca_2560_vh_params()]\n",
    "for params in params_list:\n",
    "  params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+params.model_name)\n",
    "analyzer_list = [ap.get_analyzer(params.model_type) for params in params_list]\n",
    "for analyzer, params in zip(analyzer_list, params_list):\n",
    "  analyzer.setup(params)\n",
    "  analyzer.model.setup(analyzer.model_params)\n",
    "  analyzer.load_analysis(save_info=params.save_info)\n",
    "  analyzer.model_name = params.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = \"\"\n",
    "for analyzer in analyzer_list:\n",
    "  run_params = np.load(analyzer.analysis_out_dir+\"savefiles/iso_params_\"+save_name\n",
    "    +analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()\n",
    "  min_angle = run_params[\"min_angle\"]\n",
    "  max_angle = run_params[\"max_angle\"]\n",
    "  num_neurons = run_params[\"num_neurons\"]\n",
    "  use_bf_stats = run_params[\"use_bf_stats\"]\n",
    "  num_comparison_vectors = run_params[\"num_comparison_vects\"]\n",
    "  x_range = run_params[\"x_range\"]\n",
    "  y_range = run_params[\"y_range\"]\n",
    "  num_images = run_params[\"num_images\"]\n",
    "\n",
    "  iso_vectors = np.load(analyzer.analysis_out_dir+\"savefiles/iso_vectors_\"+save_name\n",
    "    +analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()\n",
    "  analyzer.target_neuron_ids = iso_vectors[\"target_neuron_ids\"]\n",
    "  analyzer.comparison_neuron_ids = iso_vectors[\"comparison_neuron_ids\"]\n",
    "  analyzer.target_vectors = iso_vectors[\"target_vectors\"]\n",
    "  analyzer.rand_orth_vectors = iso_vectors[\"rand_orth_vectors\"]\n",
    "  analyzer.comparison_vectors = iso_vectors[\"comparison_vectors\"]\n",
    "\n",
    "  analyzer.comp_activations = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_activations_\"+save_name\n",
    "    +analyzer.analysis_params.save_info+\".npz\")[\"data\"]\n",
    "  analyzer.comp_contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_contour_dataset_\"+save_name\n",
    "    +analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()\n",
    "  analyzer.rand_activations = np.load(analyzer.analysis_out_dir+\"savefiles/iso_rand_activations_\"+save_name\n",
    "    +analyzer.analysis_params.save_info+\".npz\")[\"data\"]\n",
    "  analyzer.rand_contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_rand_contour_dataset_\"+save_name\n",
    "    +analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "neuron_indices = [0, 0, 0, 0]\n",
    "orth_indices = [0, 0, 0, 1]\n",
    "num_plots_y = 2\n",
    "num_plots_x = 2\n",
    "show_contours = True\n",
    "\n",
    "fig, contour_handles = nc.plot_goup_iso_contours(analyzer_list, neuron_indices, orth_indices,\n",
    "  num_levels, x_range, y_range, show_contours, figsize, dpi, fontsize)\n",
    "for analyzer, neuron_index, orth_index in zip(analyzer_list, neuron_indices, orth_indices):\n",
    "  for ext in [\".png\", \".eps\"]:\n",
    "    neuron_str = str(analyzer.target_neuron_ids[neuron_index])\n",
    "    orth_str = str(analyzer.comparison_neuron_ids[neuron_index][orth_index])\n",
    "    save_name = analyzer.analysis_out_dir+\"/vis/iso_contour_comparison_\"\n",
    "    if not show_contours:\n",
    "      save_name += \"continuous_\"\n",
    "    save_name += \"bf0id\"+neuron_str+\"_bf1id\"+orth_str+\"_\"+analyzer.analysis_params.save_info+ext\n",
    "    fig.savefig(save_name, dpi=dpi, transparent=True, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "line_alpha = 0.08\n",
    "for analyzer in analyzer_list:\n",
    "  target_neuron_index = 0\n",
    "  fig = nc.plot_bf_curvature(analyzer, target_neuron_index, line_alpha=line_alpha,\n",
    "    figsize=(figsize[0], 2*figsize[1]), dpi=dpi, fontsize=fontsize)\n",
    "  neuron_str = str(analyzer.target_neuron_ids[target_neuron_index])\n",
    "  save_name = (analyzer.analysis_out_dir+\"/vis/bf_curvatures_bfid0\"+neuron_str\n",
    "    +\"_\"+analyzer.analysis_params.save_info+\".png\")\n",
    "  fig.savefig(save_name, transparent=True, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for analyzer in analyzer_list:\n",
    "  target_neuron_index = 0\n",
    "  fig = nc.plot_fit_curvature(analyzer, target_neuron_index=0, line_alpha=line_alpha,\n",
    "    figsize=(figsize[0], 2*figsize[1]), dpi=dpi, fontsize=fontsize)\n",
    "  neuron_str = str(analyzer.target_neuron_ids[target_neuron_index])\n",
    "  save_name = (analyzer.analysis_out_dir+\"/vis/bf_fit_curvatures_bfid0\"+neuron_str\n",
    "    +\"_\"+analyzer.analysis_params.save_info+\".png\")\n",
    "  fig.savefig(save_name, transparent=True, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curvature histogram"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class Arrow3D(FancyArrowPatch):\n",
    "    def __init__(self, xs, ys, zs, *args, **kwargs):\n",
    "        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\n",
    "        self._verts3d = xs, ys, zs\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        xs3d, ys3d, zs3d = self._verts3d\n",
    "        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n",
    "        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n",
    "        FancyArrowPatch.draw(self, renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = [lca_512_vh_params(), lca_768_vh_params(), lca_2560_vh_params()]\n",
    "iso_save_name = \"iso_curvature_xrange1.3_yrange-2.2_\"\n",
    "attn_save_name = \"1d_\"\n",
    "\n",
    "for params in params_list:\n",
    "  params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+params.model_name)\n",
    "\n",
    "analyzer_list = [ap.get_analyzer(params.model_type) for params in params_list]\n",
    "for analyzer, params in zip(analyzer_list, params_list):\n",
    "  analyzer.setup(params)\n",
    "  analyzer.model.setup(analyzer.model_params)\n",
    "  analyzer.load_analysis(save_info=params.save_info)\n",
    "  analyzer.model_name = params.model_name\n",
    "\n",
    "  iso_run_params = np.load(analyzer.analysis_out_dir+\"savefiles/iso_params_\"+iso_save_name+analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()\n",
    "  analyzer.iso_comp_activations = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_activations_\"+iso_save_name+analyzer.analysis_params.save_info+\".npz\")[\"data\"]\n",
    "  analyzer.iso_comp_contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_contour_dataset_\"+iso_save_name+analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()\n",
    "  analyzer.iso_rand_activations = np.load(analyzer.analysis_out_dir+\"savefiles/iso_rand_activations_\"+iso_save_name+analyzer.analysis_params.save_info+\".npz\")[\"data\"]\n",
    "  analyzer.iso_rand_contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_rand_contour_dataset_\"+iso_save_name+analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()\n",
    "\n",
    "  analyzer.iso_num_target_neurons = iso_run_params[\"num_neurons\"]\n",
    "  analyzer.iso_num_comparison_vectors = iso_run_params[\"num_comparison_vects\"]\n",
    "  \n",
    "  attn_run_params = np.load(analyzer.analysis_out_dir+\"savefiles/iso_params_\"+attn_save_name+analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()\n",
    "  analyzer.attn_comp_activations = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_activations_\"+attn_save_name+analyzer.analysis_params.save_info+\".npz\")[\"data\"]\n",
    "  analyzer.attn_comp_contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_contour_dataset_\"+attn_save_name+analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()\n",
    "  analyzer.attn_rand_activations = np.load(analyzer.analysis_out_dir+\"savefiles/iso_rand_activations_\"+attn_save_name+analyzer.analysis_params.save_info+\".npz\")[\"data\"]\n",
    "  analyzer.attn_rand_contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_rand_contour_dataset_\"+attn_save_name+analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()\n",
    "  \n",
    "  analyzer.attn_num_target_neurons = attn_run_params[\"num_neurons\"]\n",
    "  analyzer.attn_num_comparison_vectors = attn_run_params[\"num_comparison_vects\"]\n",
    "  \n",
    "mesh_save_name = \"iso_curvature_ryan_\"\n",
    "contour_activity = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_activations_\"+mesh_save_name\n",
    "  +analyzer.analysis_params.save_info+\".npz\")[\"data\"][0, 1, ...]\n",
    "comp_contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_contour_dataset_\"+mesh_save_name\n",
    "  +analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()\n",
    "x = range(comp_contour_dataset[\"x_pts\"].size)\n",
    "y = range(comp_contour_dataset[\"y_pts\"].size)\n",
    "contour_pts = (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_act = 0.3 # target activity spot between min & max value of normalized activity (btwn 0 and 1)\n",
    "for analyzer in analyzer_list:\n",
    "  analyzer.iso_comp_curvatures = []\n",
    "  analyzer.iso_rand_curvatures = []\n",
    "  activations_and_curvatures = ((analyzer.iso_comp_activations, analyzer.iso_comp_curvatures),\n",
    "    (analyzer.iso_rand_activations, analyzer.iso_rand_curvatures))\n",
    "  for activations, curvatures in activations_and_curvatures:\n",
    "    (num_neurons, num_planes, num_points_y, num_points_x) = activations.shape\n",
    "    for neuron_id in range(num_neurons):\n",
    "      sub_curvatures = []\n",
    "      for plane_id in range(num_planes):\n",
    "        activity = activations[neuron_id, plane_id, ...]\n",
    "        ## mirror top half of activations to bottom half to only measure curvature in the upper quadrant\n",
    "        num_y, num_x = activity.shape \n",
    "        activity[:int(num_y/2), :] = activity[int(num_y/2):, :][::-1,:]\n",
    "        ## compute curvature\n",
    "        contours = measure.find_contours(activity, target_act)[0]\n",
    "        x_vals = contours[:,1]\n",
    "        y_vals = contours[:,0]\n",
    "        coeffs = np.polynomial.polynomial.polyfit(y_vals, x_vals, deg=2)\n",
    "        sub_curvatures.append(coeffs[-1])\n",
    "      curvatures.append(sub_curvatures)\n",
    "\n",
    "  comp_x_pts = analyzer.attn_comp_contour_dataset[\"x_pts\"]\n",
    "  rand_x_pts = analyzer.attn_rand_contour_dataset[\"x_pts\"]\n",
    "  assert(np.all(comp_x_pts == rand_x_pts)) # This makes sure we don't need to recompute proj_datapoints for each case\n",
    "  num_x_imgs = len(comp_x_pts)\n",
    "  x_target = comp_x_pts[num_x_imgs-1] # find a location to take a slice\n",
    "  proj_datapoints = analyzer.attn_comp_contour_dataset[\"proj_datapoints\"]\n",
    "  slice_indices = np.where(proj_datapoints[:, 0] == x_target)[0]\n",
    "  analyzer.sliced_datapoints = proj_datapoints[slice_indices, :][:, :] # slice grid\n",
    "\n",
    "  analyzer.attn_comp_curvatures = []\n",
    "  analyzer.attn_comp_fits = []\n",
    "  analyzer.attn_comp_sliced_activity = []\n",
    "  analyzer.attn_rand_curvatures = []\n",
    "  analyzer.attn_rand_fits = []\n",
    "  analyzer.attn_rand_sliced_activity = []\n",
    "  for neuron_index in range(analyzer.attn_num_target_neurons):\n",
    "    sub_comp_curvatures = []\n",
    "    sub_comp_fits = []\n",
    "    sub_comp_sliced_activity = []\n",
    "    sub_comp_delta_activity = []\n",
    "    sub_rand_curvatures = []\n",
    "    sub_rand_fits = []\n",
    "    sub_rand_sliced_activity = []\n",
    "    for orth_index in range(analyzer.attn_num_comparison_vectors):\n",
    "      comp_activity = analyzer.attn_comp_activations[neuron_index, orth_index, ...].reshape([-1])\n",
    "      sub_comp_sliced_activity.append(comp_activity[slice_indices][:])\n",
    "      coeff = np.polynomial.polynomial.polyfit(analyzer.sliced_datapoints[:, 1],\n",
    "        sub_comp_sliced_activity[-1], deg=2) # [c0, c1, c2], where p = c0 + c1x + c2x^2\n",
    "      sub_comp_curvatures.append(coeff[2])\n",
    "      sub_comp_fits.append(np.polynomial.polynomial.polyval(analyzer.sliced_datapoints[:, 1], coeff))\n",
    "      \n",
    "    num_rand_vectors = np.minimum(analyzer.bf_stats[\"num_inputs\"]-1, analyzer.attn_num_comparison_vectors)\n",
    "    for orth_index in range(num_rand_vectors):\n",
    "      rand_activity = analyzer.attn_rand_activations[neuron_index, orth_index, ...].reshape([-1])\n",
    "      sub_rand_sliced_activity.append(rand_activity[slice_indices][:])\n",
    "      coeff = np.polynomial.polynomial.polyfit(analyzer.sliced_datapoints[:, 1],\n",
    "        sub_rand_sliced_activity[-1], deg=2)\n",
    "      sub_rand_curvatures.append(coeff[2])\n",
    "      sub_rand_fits.append(np.polynomial.polynomial.polyval(analyzer.sliced_datapoints[:, 1], coeff))\n",
    "\n",
    "    analyzer.attn_comp_curvatures.append(sub_comp_curvatures)\n",
    "    analyzer.attn_comp_fits.append(sub_comp_fits)\n",
    "    analyzer.attn_comp_sliced_activity.append(sub_comp_sliced_activity)\n",
    "    analyzer.attn_rand_curvatures.append(sub_rand_curvatures)\n",
    "    analyzer.attn_rand_fits.append(sub_rand_fits)\n",
    "    analyzer.attn_rand_sliced_activity.append(sub_rand_sliced_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 50\n",
    "def get_bins(all_curvatures, num_bins=50):\n",
    "  max_curvature = np.amax(all_curvatures)\n",
    "  min_curvature = np.amin(all_curvatures)\n",
    "  bin_width = (max_curvature - min_curvature) / (num_bins-1) # subtract 1 to leave room for the zero bin\n",
    "  bin_centers = [0.0]\n",
    "  while min(bin_centers) > min_curvature:\n",
    "    bin_centers.append(bin_centers[-1]-bin_width)\n",
    "  bin_centers = bin_centers[::-1]\n",
    "  while max(bin_centers) < max_curvature:\n",
    "    bin_centers.append(bin_centers[-1]+bin_width)\n",
    "  bin_lefts = bin_centers - (bin_width / 2)\n",
    "  bin_rights = bin_centers + (bin_width / 2)\n",
    "  bins = np.append(bin_lefts, bin_rights[-1])\n",
    "  return bins\n",
    "\n",
    "iso_all_curvatures = []\n",
    "for analyzer in analyzer_list:\n",
    "  for neuron_index in range(num_neurons):\n",
    "    iso_all_curvatures += analyzer.iso_comp_curvatures[neuron_index]\n",
    "    iso_all_curvatures += analyzer.iso_rand_curvatures[neuron_index]\n",
    "iso_bins = get_bins(iso_all_curvatures, num_bins)\n",
    "\n",
    "attn_all_curvatures = []\n",
    "for analyzer in analyzer_list:\n",
    "  for neuron_index in range(analyzer.attn_num_target_neurons):\n",
    "    attn_all_curvatures += analyzer.attn_comp_curvatures[neuron_index]\n",
    "    attn_all_curvatures += analyzer.attn_rand_curvatures[neuron_index]\n",
    "attn_bins = get_bins(attn_all_curvatures, num_bins)\n",
    "  \n",
    "for analyzer in analyzer_list:\n",
    "  flat_comp_curvatures = [item for sub_list in analyzer.iso_comp_curvatures for item in sub_list]\n",
    "  comp_hist, analyzer.iso_bin_edges = np.histogram(flat_comp_curvatures, iso_bins, density=False)\n",
    "  analyzer.iso_comp_hist = comp_hist / np.sum(comp_hist)\n",
    "  flat_rand_curvatures = [item for sub_list in analyzer.iso_rand_curvatures for item in sub_list]\n",
    "  rand_hist, _ = np.histogram(flat_rand_curvatures, iso_bins, density=False)\n",
    "  analyzer.iso_rand_hist = rand_hist / np.sum(rand_hist)\n",
    "\n",
    "  flat_comp_curvatures = [item for sub_list in analyzer.attn_comp_curvatures for item in sub_list]\n",
    "  comp_hist, analyzer.attn_bin_edges = np.histogram(flat_comp_curvatures, attn_bins, density=False)\n",
    "  analyzer.attn_comp_hist = comp_hist / np.sum(comp_hist)\n",
    "  flat_rand_curvatures = [item for sub_list in analyzer.attn_rand_curvatures for item in sub_list]\n",
    "  rand_hist, _ = np.histogram(flat_rand_curvatures, attn_bins, density=False)\n",
    "  analyzer.attn_rand_hist = rand_hist / np.sum(rand_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_hist_list = [[analyzer.iso_comp_hist for analyzer in analyzer_list],\n",
    "  [analyzer.iso_rand_hist for analyzer in analyzer_list]]\n",
    "label_list = [[\"Comparison vectors, 2x\", \"Comparison vectors, 4x\", \"Comparison vectors, 10x\"],\n",
    "  [\"Random vectors, 2x\", \"Random vectors, 4x\", \"Random vectors, 10x\"]]\n",
    "color_list = [[color_vals[\"lt_red\"], color_vals[\"md_red\"], color_vals[\"dk_red\"]],\n",
    "  [color_vals[\"lt_blue\"], color_vals[\"md_blue\"], color_vals[\"dk_blue\"]]]\n",
    "\n",
    "plot_bin_lefts, plot_bin_rights = analyzer_list[0].iso_bin_edges[:-1], analyzer_list[0].iso_bin_edges[1:]\n",
    "iso_plot_bin_centers = plot_bin_lefts + (plot_bin_rights - plot_bin_lefts)\n",
    "\n",
    "label_loc = [0.5, 0.3]\n",
    "iso_title = \"Iso-Response\"\n",
    "iso_xlabel = \"Curvature of Iso-Response Contours\"\n",
    "\n",
    "attn_hist_list = [[analyzer.attn_comp_hist for analyzer in analyzer_list],\n",
    "  [analyzer.attn_rand_hist for analyzer in analyzer_list]]\n",
    "\n",
    "plot_bin_lefts, plot_bin_rights = analyzer_list[0].attn_bin_edges[:-1], analyzer_list[0].attn_bin_edges[1:]\n",
    "attn_plot_bin_centers = plot_bin_lefts + (plot_bin_rights - plot_bin_lefts)\n",
    "\n",
    "label_loc = [0.08, 0.30]\n",
    "attn_title = \"Response Attenuation\"\n",
    "attn_xlabel = \"Curvature of Response Attenuation\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plot_curvature_histograms(activity, contour_pts, contour_angle, contour_text_loc, hist_list, label_list, color_list, bin_centers, title, xlabel, figsize=None, dpi=100, fontsize=12):\n",
    "  fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "  num_y_plots = 4\n",
    "  num_x_plots = 4\n",
    "  gs0 = gridspec.GridSpec(num_y_plots, num_x_plots, wspace=0.5)\n",
    "\n",
    "  x_mesh, y_mesh = np.meshgrid(*contour_pts)\n",
    "  curve_ax = fig.add_subplot(gs0[:, 0:2], projection='3d')\n",
    "  curve_ax.set_zlim(0,1)\n",
    "  curve_ax.set_xlim3d(5,200)\n",
    "  x_ticks = curve_ax.get_xticks().tolist()\n",
    "  x_ticks = np.round(np.linspace(0.6, 4.4, 11), 1).astype(str)\n",
    "  a_x = [\" \"]*len(x_ticks)\n",
    "  a_x[1] = x_ticks[1]\n",
    "  a_x[-3] = x_ticks[-2]\n",
    "  curve_ax.set_xticklabels(a_x, size=fontsize)\n",
    "  y_ticks = curve_ax.get_yticks().tolist()\n",
    "  y_ticks = np.round(np.linspace(-10, 10, 11), 1).astype(str)\n",
    "  a_y = [\" \"]*len(y_ticks)\n",
    "  a_y[1] = y_ticks[1]\n",
    "  a_y[-2] = y_ticks[-2]\n",
    "  curve_ax.set_yticklabels(a_y, size=fontsize)\n",
    "  curve_ax.set_zticklabels([])\n",
    "  curve_ax.zaxis.set_rotate_label(False)\n",
    "  curve_ax.set_zlabel(\"Normalized Activity\", rotation=90, size=fontsize)\n",
    "  curve_ax.scatter(x_mesh, y_mesh, activity, color=\"#A9A9A9\",s=0.05)\n",
    "  loc0, loc1, loc2 = contour_text_loc[0]\n",
    "  curve_ax.text(loc0, loc1, loc2, \"Iso-\\nresponse\", color='black', size=fontsize)\n",
    "  iso_line_offset = 165\n",
    "  curve_ax.plot(np.zeros_like(x)+iso_line_offset, y, activity[:, iso_line_offset], color=\"black\", lw=5)\n",
    "  v = Arrow3D([-200/3., -200/3.], [200/2., 200/2.+200/16.], \n",
    "              [0, 0.0], mutation_scale=10, \n",
    "              lw=2, arrowstyle=\"-|>\", color=\"r\", linestyle=\"dashed\")\n",
    "  curve_ax.add_artist(v)\n",
    "  curve_ax.text(-270/3., 300/3.0, 0.0, \"v\", color='red', size=fontsize)\n",
    "  phi_k = Arrow3D([-200/3., 0.], [200/2., 200/2.], \n",
    "              [0, 0.0], mutation_scale=10, \n",
    "              lw=2, arrowstyle=\"-|>\", color=\"r\", linestyle = \"dashed\")\n",
    "  curve_ax.add_artist(phi_k)\n",
    "  curve_ax.text(-175/3., 270/3.0, 0.0, r\"${\\phi}_{k}$\", color='red', size=fontsize)\n",
    "  loc0, loc1, loc2 = contour_text_loc[1]\n",
    "  curve_ax.text(loc0, loc1, loc2, \"Response\\nAttenuation\", color='black', size=fontsize)\n",
    "  lines = np.array([0.2, 0.203, 0.197]) - 0.1\n",
    "  for i in lines:\n",
    "      curve_ax.contour3D(x_mesh, y_mesh, activity, [i], colors=\"black\")\n",
    "  curve_ax.view_init(30, contour_angle)\n",
    "  \n",
    "  sub_num_y_plots = 4#len(hist_list) #2\n",
    "  sub_num_x_plots = 1#len(hist_list[0]) #2\n",
    "  hist_gs = gridspec.GridSpecFromSubplotSpec(sub_num_y_plots, sub_num_x_plots, gs0[:, 2:], hspace=0.40, wspace=0.15)\n",
    "  all_lists = zip(hist_list, label_list, color_list, bin_centers, title, xlabel)\n",
    "  orig_ax = fig.add_subplot(hist_gs[0])\n",
    "  axes = []\n",
    "  axis_index = 0\n",
    "  for sub_plt_x in range(0, sub_num_x_plots):\n",
    "    for sub_plt_y in range(0, sub_num_y_plots):\n",
    "      if (sub_plt_x, sub_plt_y) == (0,0):\n",
    "        axes.append(orig_ax)\n",
    "      else:\n",
    "        #axes.append(fig.add_subplot(hist_gs[sub_plt_y, sub_plt_x], sharey=orig_ax))\n",
    "        axes.append(fig.add_subplot(hist_gs[axis_index], sharey=orig_ax))\n",
    "      axis_index += 1\n",
    "  axis_index = 0\n",
    "  for axis_x, (sub_hist, sub_label, sub_color, sub_bins, sub_title, sub_xlabel) in enumerate(all_lists):\n",
    "    handles = []\n",
    "    labels = []\n",
    "    max_val = 0\n",
    "    for axis_y, (axis_hists, axis_labels, axis_colors) in enumerate(zip(sub_hist, sub_label, sub_color)):\n",
    "      axes[axis_index].set_xticks(sub_bins, minor=True)\n",
    "      axes[axis_index].set_xticks(sub_bins[::int(len(sub_bins)/5)], minor=False)\n",
    "      axes[axis_index].set_xlabel(sub_xlabel, fontsize=fontsize)\n",
    "      axes[axis_index].xaxis.set_major_formatter(FormatStrFormatter(\"%0.3f\"))\n",
    "      for hist, label, color in zip(axis_hists, axis_labels, axis_colors):\n",
    "        axes[axis_index].plot(sub_bins, hist, color=color, linestyle=\"-\", drawstyle=\"steps-mid\", label=label)\n",
    "        axes[axis_index].set_yscale('log')\n",
    "        if np.max(hist) > max_val:\n",
    "          max_val = np.max(hist)\n",
    "      axes[axis_index].axvline(0.0, color='k', linestyle='dashed', linewidth=1)\n",
    "      for tick in axes[axis_index].xaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(fontsize) \n",
    "      for tick in axes[axis_index].yaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(fontsize) \n",
    "      axes[axis_index].set_ylabel(\"Normalized\\nCount\", fontsize=fontsize)\n",
    "      ax_handles, ax_labels = axes[axis_index].get_legend_handles_labels()\n",
    "      handles += ax_handles\n",
    "      labels += ax_labels\n",
    "      if axis_x == 0 and axis_y == 1:#sub_num_y_plots-1:\n",
    "        legend = axes[axis_index].legend(handles=handles, labels=labels, fontsize=fontsize, loc=\"upper right\",\n",
    "          borderaxespad=0.5, borderpad=0., ncol=2)\n",
    "        legend.get_frame().set_linewidth(0.0)\n",
    "        for text, color in zip(legend.get_texts(), [color for sublist in sub_color for color in sublist]):\n",
    "          text.set_color(color)\n",
    "        for item in legend.legendHandles:\n",
    "          item.set_visible(False)\n",
    "      axis_index += 1\n",
    "  plt.show()\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_angle = 210\n",
    "\n",
    "full_hist_list = [iso_hist_list, attn_hist_list]\n",
    "full_label_list = [label_list,]*2\n",
    "full_color_list = [color_list,]*2\n",
    "full_bin_centers = [iso_plot_bin_centers, attn_plot_bin_centers]\n",
    "full_title = [iso_title, attn_title]\n",
    "full_xlabel = [iso_xlabel, attn_xlabel]\n",
    "\n",
    "iso_resp_loc = [0, 180, 0.42]\n",
    "resp_att_loc = [100, 240, 0.38]\n",
    "contour_text_loc = [iso_resp_loc, resp_att_loc]\n",
    "\n",
    "curvature_fig = nc.plot_curvature_histograms(contour_activity, contour_pts, contour_angle, contour_text_loc,\n",
    "  full_hist_list, full_label_list, full_color_list, full_bin_centers, full_title, full_xlabel,\n",
    "  figsize=(2*figsize[0], figsize[1]), dpi=dpi, fontsize=fontsize)\n",
    "\n",
    "for analyzer in analyzer_list:\n",
    "  for ext in [\".png\"]:#, \".eps\"]:\n",
    "    save_name = (analyzer.analysis_out_dir+\"/vis/curvatures_and_histograms\"\n",
    "      +\"_\"+analyzer.analysis_params.save_info+ext)\n",
    "    curvature_fig.savefig(save_name, transparent=False, bbox_inches=\"tight\", pad_inches=0.01, dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orientation Selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = [rica_768_vh_params(), sae_768_vh_params(), lca_768_vh_params()]\n",
    "params_list[-1].display_name = \"Sparse Coding\"\n",
    "for params in params_list:\n",
    "  params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+params.model_name)\n",
    "analyzer_list = [ap.get_analyzer(params.model_type) for params in params_list]\n",
    "for analyzer, params in zip(analyzer_list, params_list):\n",
    "  analyzer.setup(params)\n",
    "  analyzer.model.setup(analyzer.model_params)\n",
    "  analyzer.load_analysis(save_info=params.save_info)\n",
    "  analyzer.model_name = params.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contrast_orientation_tuning(bf_indices, contrasts, orientations, activations, figsize=(32,32)):\n",
    "  \"\"\"\n",
    "  Generate contrast orientation tuning curves. Every subplot will have curves for each contrast.\n",
    "  Inputs:\n",
    "    bf_indices: [list or array] of neuron indices to use\n",
    "      all indices should be less than activations.shape[0]\n",
    "    contrasts: [list or array] of contrasts to use\n",
    "    orientations: [list or array] of orientations to use\n",
    "  \"\"\"\n",
    "  orientations = np.asarray(orientations)*(180/np.pi) #convert to degrees for plotting\n",
    "  num_bfs = np.asarray(bf_indices).size\n",
    "  cmap = plt.get_cmap('Greys')\n",
    "  cNorm = matplotlib.colors.Normalize(vmin=0.0, vmax=1.0)\n",
    "  scalarMap = matplotlib.cm.ScalarMappable(norm=cNorm, cmap=cmap)\n",
    "  fig = plt.figure(figsize=figsize)\n",
    "  num_plots_y = np.int32(np.ceil(np.sqrt(num_bfs)))+1\n",
    "  num_plots_x = np.int32(np.ceil(np.sqrt(num_bfs)))\n",
    "  gs_widths = [1.0,]*num_plots_x\n",
    "  gs_heights = [1.0,]*num_plots_y\n",
    "  gs = gridspec.GridSpec(num_plots_y, num_plots_x, wspace=0.5, hspace=0.7,\n",
    "    width_ratios=gs_widths, height_ratios=gs_heights)\n",
    "  bf_idx = 0\n",
    "  for plot_id in np.ndindex((num_plots_y, num_plots_x)):\n",
    "    (y_id, x_id) = plot_id\n",
    "    if y_id == 0 and x_id == 0:\n",
    "      ax = fig.add_subplot(gs[plot_id])\n",
    "      #ax.set_ylabel(\"Activation\", fontsize=16)\n",
    "      #ax.set_xlabel(\"Orientation\", fontsize=16)\n",
    "      ax00 = ax\n",
    "    else:\n",
    "      ax = fig.add_subplot(gs[plot_id])#, sharey=ax00)\n",
    "    if bf_idx < num_bfs:\n",
    "      for co_idx, contrast in enumerate(contrasts):\n",
    "        co_idx = -1\n",
    "        contrast = 1.0#contrasts[co_idx]\n",
    "        activity = activations[bf_indices[bf_idx], co_idx, :]\n",
    "        color_val = scalarMap.to_rgba(contrast)\n",
    "        ax.plot(orientations, activity, linewidth=1, color=color_val)\n",
    "        ax.scatter(orientations, activity, s=4, c=[color_val])\n",
    "        ax.yaxis.set_major_formatter(FormatStrFormatter('%0.2g'))\n",
    "        ax.set_yticks([0, np.max(activity)])\n",
    "        ax.set_xticks([0, 90, 180])\n",
    "      bf_idx += 1\n",
    "    else:\n",
    "      ax = pf.clear_axis(ax, spines=\"none\")\n",
    "  plt.show()\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(weights, title=\"\", figsize=None, save_filename=None):\n",
    "  \"\"\"\n",
    "    weights: [np.ndarray] of shape [num_outputs, num_input_y, num_input_x]\n",
    "    The matrices are renormalized before plotting.\n",
    "  \"\"\"\n",
    "  weights = dp.norm_weights(weights)\n",
    "  vmin = np.min(weights)\n",
    "  vmax = np.max(weights)\n",
    "  num_plots = weights.shape[0]\n",
    "  num_plots_y = int(np.floor(np.sqrt(num_plots)))\n",
    "  num_plots_x = int(np.ceil(np.sqrt(num_plots)))\n",
    "  fig, sub_ax = plt.subplots(num_plots_y, num_plots_x, figsize=figsize)\n",
    "  filter_total = 0\n",
    "  for plot_id in  np.ndindex((num_plots_y, num_plots_x)):\n",
    "    if filter_total < num_plots:\n",
    "      sub_ax[plot_id].imshow(np.squeeze(weights[filter_total, ...]), vmin=vmin, vmax=vmax, cmap=\"Greys_r\")\n",
    "      filter_total += 1\n",
    "    pf.clear_axis(sub_ax[plot_id])\n",
    "    sub_ax[plot_id].set_aspect(\"equal\")\n",
    "  fig.suptitle(title, y=0.95, x=0.5, fontsize=20)\n",
    "  if save_filename is not None:\n",
    "      fig.savefig(save_filename)\n",
    "      plt.close(fig)\n",
    "      return None\n",
    "  plt.show()\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_curve(tuning_curve):\n",
    "  \"\"\"\n",
    "  Centers a curve about its preferred orientation\n",
    "  \"\"\"\n",
    "  return np.roll(tuning_curve, (len(tuning_curve) // 2) - np.argmax(tuning_curve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fwhm(centered_ot_curve, corresponding_angles_deg):\n",
    "  \"\"\"\n",
    "  Calculates the full width at half maximum of the tuning curve\n",
    "\n",
    "  Result is expressed in degrees to make it a little more intuitive. The curve\n",
    "  is often NOT symmetric about the maximum value so we don't do any fitting and\n",
    "  we return the FULL width\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  centered_ot_curve : ndarray\n",
    "      A 1d array of floats giving the value of the ot curve, at an orientation\n",
    "      relative to the *preferred orientation* which is given by the angles in\n",
    "      corresponding_angles_deg. This has the maximum orientation in the\n",
    "      center of the array which is nicer for visualization.\n",
    "  corresponding_angles_deg : ndarray\n",
    "      The orientations relative to preferred orientation that correspond to\n",
    "      the values in centered_ot_curve\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  half_max_left : float\n",
    "      The position of the intercept to the left of the max\n",
    "  half_max_right : float\n",
    "      The position of the intercept to the right of the max\n",
    "  half_max_value : float\n",
    "      Mainly for plotting purposes, the actual curve value that corresponds\n",
    "      to the left and right points\n",
    "  \"\"\"\n",
    "  max_idx = np.argmax(centered_ot_curve)\n",
    "  min_idx = np.argmin(centered_ot_curve)\n",
    "  max_val = centered_ot_curve[max_idx]\n",
    "  min_val = centered_ot_curve[min_idx]\n",
    "  midpoint = (max_val / 2) + (min_val / 2)\n",
    "  # find the left hand point\n",
    "  idx = max_idx\n",
    "  while centered_ot_curve[idx] > midpoint:\n",
    "    idx -= 1\n",
    "    if idx == -1:\n",
    "      # the width is *at least* 90 degrees\n",
    "      half_max_left = -90.\n",
    "      break\n",
    "  if idx > -1:\n",
    "    # we'll linearly interpolate between the two straddling points\n",
    "    # if (x2, y2) is the coordinate of the point below the half-max and\n",
    "    # (x1, y1) is the point above the half-max, then we can solve for x3, the\n",
    "    # x-position of the point that corresponds to the half-max on the line\n",
    "    # that connects (x1, y1) and (x2, y2)\n",
    "    half_max_left = (((midpoint - centered_ot_curve[idx])\n",
    "      * (corresponding_angles_deg[idx+1] - corresponding_angles_deg[idx])\n",
    "      / (centered_ot_curve[idx+1] - centered_ot_curve[idx]))\n",
    "      + corresponding_angles_deg[idx])\n",
    "  # find the right hand point\n",
    "  idx = max_idx\n",
    "  while centered_ot_curve[idx] > midpoint:\n",
    "    idx += 1\n",
    "    if idx == len(centered_ot_curve):\n",
    "      # the width is *at least* 90\n",
    "      half_max_right = 90.\n",
    "      break\n",
    "  if idx < len(centered_ot_curve):\n",
    "    # we'll linearly interpolate between the two straddling points again\n",
    "    half_max_right = (((midpoint - centered_ot_curve[idx-1])\n",
    "      * (corresponding_angles_deg[idx] - corresponding_angles_deg[idx-1])\n",
    "      / (centered_ot_curve[idx] - centered_ot_curve[idx-1]))\n",
    "      + corresponding_angles_deg[idx-1])\n",
    "  return half_max_left, half_max_right, midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_circ_var(centered_ot_curve, corresponding_angles_rad):\n",
    "  \"\"\"\n",
    "  From\n",
    "  DL Ringach, RM Shapley, MJ Hawken (2002) - Orientation Selectivity in Macaque V1:\n",
    "  Diversity and Laminar Dependence\n",
    "  \n",
    "  Computes the circular variance of a tuning curve and returns vals for plotting\n",
    "\n",
    "  This is a scale-invariant measure of how 'oriented' a curve is in some\n",
    "  global sense. It wraps reponses around the unit circle and then sums their\n",
    "  vectors, resulting in an average vector, the magnitude of which indicates\n",
    "  the strength of the tuning. Circular variance is an index of 'orientedness'\n",
    "  that falls in the interval [0.0, 1.0], with 0.0 indicating a delta function\n",
    "  and 1.0 indicating a completely flat tuning curve.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  centered_ot_curve : ndarray\n",
    "      A 1d array of floats giving the value of the ot curve, at an orientation\n",
    "      relative to the *preferred orientation* which is given by the angles in\n",
    "      corresponding_angles_rad. This has the maximum orientation in the\n",
    "      center of the array which is nicer for visualization.\n",
    "  corresponding_angles_rad : ndarray\n",
    "      The orientations relative to preferred orientation that correspond to\n",
    "      the values in centered_ot_curve\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  numerator_sum_components : ndarray\n",
    "      The complex values the are produced from r * np.exp(j*2*theta). These\n",
    "      are the elements that get summed up in the numerator\n",
    "  direction_vector : complex64 or complex128\n",
    "      This is the vector that points in the direction of *aggregate* tuning.\n",
    "      its magnitude is upper bounded by 1.0 which is the case when only one\n",
    "      orientation has a nonzero value. We can plot it to get an idea of how\n",
    "      tuned a curve is\n",
    "  circular_variance : float\n",
    "      This is 1 minus the magnitude of the direction vector. It represents and\n",
    "      index of 'global selectivity'\n",
    "  \"\"\"\n",
    "  # in the original definition, angles are [0, 2*np.pi] so the factor of 2\n",
    "  # in the exponential wraps the phase twice around the complex circle,\n",
    "  # placing responses that correspond to angles pi degrees apart\n",
    "  # onto the same place. We know there's a redudancy in our responses at pi\n",
    "  # offsets so our responses get wrapped around the unit circle once.\n",
    "  numerator_sum_components = (centered_ot_curve\n",
    "    * np.exp(1j * 2 * corresponding_angles_rad))\n",
    "  direction_vector = (np.sum(numerator_sum_components)\n",
    "    / np.sum(centered_ot_curve))\n",
    "  circular_variance = 1 - np.abs(direction_vector)\n",
    "  return (numerator_sum_components, direction_vector, circular_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_osi(centered_ot_curve):\n",
    "  \"\"\"\n",
    "  Compute the Orientation Selectivity Index.\n",
    "\n",
    "  This is the most coarse but popular measure of selectivity. It really\n",
    "  doesn't tell you much. It just measures the maximum response relative to\n",
    "  the minimum response.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  centered_ot_curve : ndarray\n",
    "      A 1d array of floats giving the value of the ot curve, at an orientation\n",
    "      relative to the *preferred orientation*\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  osi : float\n",
    "      This is (a_max - a_orth) / (a_max + a_orth) where a_max is the maximum\n",
    "      response across orientations when orientation responses are\n",
    "      *averages* over phase. a_orth is the orientation which is orthogonal to\n",
    "      the orientation which produces a_max.\n",
    "  \"\"\"\n",
    "  max_val = np.max(centered_ot_curve)\n",
    "  # Assume that orthogonal orientation is at either end of the curve modulo 1\n",
    "  # bin (if we had like an even number of orientation values)\n",
    "  orth_val = centered_ot_curve[0]\n",
    "  osi = (max_val - orth_val) / (max_val + orth_val)\n",
    "  return osi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_circular_variance(cv_data, max_bfs_per_fig=400, title=\"\", save_filename=None):\n",
    "  assert np.sqrt(max_bfs_per_fig) % 1 == 0, \"Pick a square number for max_bfs_per_fig\"\n",
    "  orientations = (np.pi * np.arange(len(cv_data))\n",
    "    / len(cv_data)) - (np.pi/2) # relative to preferred\n",
    "  num_bfs = len(cv_data)\n",
    "  num_bf_figs = int(np.ceil(num_bfs / max_bfs_per_fig))\n",
    "  # this determines how many ot curves are aranged in a square grid within\n",
    "  # any given figure\n",
    "  if num_bf_figs > 1:\n",
    "    bfs_per_fig = max_bfs_per_fig\n",
    "  else:\n",
    "    squares = [x**2 for x in range(1, int(np.sqrt(max_bfs_per_fig))+1)]\n",
    "    bfs_per_fig = squares[bisect.bisect_left(squares, num_bfs)]\n",
    "  plot_sidelength = int(np.sqrt(bfs_per_fig))\n",
    "  bf_idx = 0\n",
    "  bf_figs = []\n",
    "  for in_bf_fig_idx in range(num_bf_figs):\n",
    "    fig = plt.figure(figsize=(32, 32))\n",
    "    plt.suptitle(title + ', fig {} of {}'.format(\n",
    "      in_bf_fig_idx+1, num_bf_figs), fontsize=20)\n",
    "    subplot_grid = gridspec.GridSpec(plot_sidelength, plot_sidelength,\n",
    "      wspace=0.4, hspace=0.4)\n",
    "    fig_bf_idx = bf_idx % bfs_per_fig\n",
    "    while fig_bf_idx < bfs_per_fig and bf_idx < num_bfs:\n",
    "      #if bf_idx % 100 == 0:\n",
    "      #  print(\"plotted \", bf_idx, \" of \", num_bfs, \" circular variance plots\")\n",
    "      ## print(\"sum vector: \", np.real(cv_data[bf_idx][1]), np.imag(cv_data[bf_idx][1]))\n",
    "      ax = plt.Subplot(fig, subplot_grid[fig_bf_idx])\n",
    "      ax.plot(np.real(cv_data[bf_idx][0]), np.imag(cv_data[bf_idx][0]),\n",
    "              c='g', linewidth=0.5)\n",
    "      ax.scatter(np.real(cv_data[bf_idx][0]), np.imag(cv_data[bf_idx][0]),\n",
    "                 c='g', s=4)\n",
    "      ax.quiver(np.real(cv_data[bf_idx][1]), np.imag(cv_data[bf_idx][1]),\n",
    "                angles='xy', scale_units='xy', scale=1.0, color='b',\n",
    "                width=0.01)\n",
    "      # ax.quiver(0.5, 0.5, color='b')\n",
    "      ax.axvline(x=0.0, color='k', linestyle='--', alpha=0.6, linewidth=0.3)\n",
    "      ax.axhline(y=0.0, color='k', linestyle='--', alpha=0.6, linewidth=0.3)\n",
    "      ax.yaxis.set_major_formatter(FormatStrFormatter('%0.2g'))\n",
    "      xaxis_size = max(np.max(np.real(cv_data[bf_idx][0])), 1.0)\n",
    "      yaxis_size = max(np.max(np.imag(cv_data[bf_idx][0])), 1.0)\n",
    "      ax.set_yticks([-1. * yaxis_size, yaxis_size])\n",
    "      ax.set_xticks([-1. * xaxis_size, xaxis_size])\n",
    "      # put the circular variance index in the upper left\n",
    "      ax.text(0.02, 0.97, 'CV: {:.2f}'.format(cv_data[bf_idx][2]),\n",
    "              horizontalalignment='left', verticalalignment='top',\n",
    "              transform=ax.transAxes, color='b', fontsize=10)\n",
    "      fig.add_subplot(ax)\n",
    "      fig_bf_idx += 1\n",
    "      bf_idx += 1\n",
    "    if save_filename is not None:\n",
    "      filename_split = os.path.split(save_filename)\n",
    "      save_filename = filename_split[0]+str(in_bf_fig_idx).zfill(2)+\"_\"+filename_split[1]\n",
    "      fig.savefig(save_filename)\n",
    "      plt.close(fig)\n",
    "      bf_figs.append(None)\n",
    "    else:\n",
    "      bf_figs.append(fig)\n",
    "  if save_filename is None:\n",
    "    plt.show()\n",
    "  return bf_figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_circular_variance_histogram(variances_list, label_list, num_bins=50, y_max=None,\n",
    "  figsize=None, save_filename=None):\n",
    "  variance_min = np.min([np.min(var) for var in variances_list])#0.0\n",
    "  variance_max = np.max([np.max(var) for var in variances_list])#1.0\n",
    "  bins = np.linspace(variance_min, variance_max, num_bins)\n",
    "  bar_width = np.diff(bins).min()\n",
    "  fig, ax = plt.subplots(1, figsize=figsize)\n",
    "  hist_list = []\n",
    "  handles = []\n",
    "  for variances, label in zip(variances_list, label_list):\n",
    "    hist, bin_edges = np.histogram(variances.flatten(), bins)\n",
    "    #hist = hist / np.max(hist)\n",
    "    hist_list.append(hist)\n",
    "    bin_left, bin_right = bin_edges[:-1], bin_edges[1:]\n",
    "    bin_centers = bin_left + (bin_right - bin_left)/2\n",
    "    handles.append(ax.bar(bin_centers, hist, width=bar_width, log=True, align=\"center\", alpha=0.5, label=label))\n",
    "  ax.set_xticks(bin_left, minor=True)\n",
    "  ax.set_xticks(bin_left[::4], minor=False)\n",
    "  ax.xaxis.set_major_formatter(FormatStrFormatter(\"%0.0f\"))\n",
    "  ax.tick_params(\"both\", labelsize=16)\n",
    "  ax.set_xlim([variance_min, variance_max])\n",
    "  ax.set_xticks([variance_min, variance_max])\n",
    "  ax.set_xticklabels([\"More selective\", \"Less selective\"])\n",
    "  ticks = ax.xaxis.get_major_ticks()\n",
    "  ticks[0].label1.set_horizontalalignment(\"left\")\n",
    "  ticks[1].label1.set_horizontalalignment(\"right\")\n",
    "  if y_max is None:\n",
    "    # Round up to the nearest power of 10\n",
    "    y_max = 10**(np.ceil(np.log10(np.max([np.max(hist) for hist in hist_list]))))\n",
    "  ax.set_ylim([1, y_max])\n",
    "  ax.set_title(\"Circular Variance Histogram\", fontsize=18)\n",
    "  ax.set_xlabel(\"Selectivity\", fontsize=18)\n",
    "  ax.set_ylabel(\"Log Count\", fontsize=18)\n",
    "  legend = ax.legend(handles, label_list, fontsize=12, #ncol=len(label_list),\n",
    "    borderaxespad=0., bbox_to_anchor=[0.98, 0.98], fancybox=True, loc=\"upper right\")\n",
    "  if save_filename is not None:\n",
    "    fig.savefig(save_filename)\n",
    "    plt.close(fig)\n",
    "    return None\n",
    "  plt.show()\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circ_var_list = []\n",
    "for analyzer in analyzer_list:\n",
    "  analyzer.bf_indices = np.random.choice(analyzer.ot_grating_responses[\"neuron_indices\"], 12)\n",
    "  num_orientation_samples = len(analyzer.ot_grating_responses['orientations'])\n",
    "  corresponding_angles_deg = (180 * np.arange(num_orientation_samples) / num_orientation_samples) - 90\n",
    "  corresponding_angles_rad = (np.pi * np.arange(num_orientation_samples) / num_orientation_samples) - (np.pi/2)\n",
    "  analyzer.metrics_list = {\"fwhm\":[], \"circ_var\":[], \"osi\":[], \"skipped_indices\":[]}\n",
    "  contrast_idx = -1\n",
    "  for bf_idx in range(analyzer.bf_stats[\"num_outputs\"]):\n",
    "    ot_curve = center_curve(analyzer.ot_grating_responses[\"mean_responses\"][bf_idx, contrast_idx, :])\n",
    "    if np.max(ot_curve) - np.min(ot_curve) == 0:\n",
    "      analyzer.metrics_list[\"skipped_indices\"].append(bf_idx)\n",
    "    else:\n",
    "      fwhm = compute_fwhm(ot_curve, corresponding_angles_deg)\n",
    "      analyzer.metrics_list[\"fwhm\"].append(fwhm)\n",
    "      circ_var = compute_circ_var(ot_curve, corresponding_angles_rad)\n",
    "      analyzer.metrics_list[\"circ_var\"].append(circ_var)\n",
    "      osi = compute_osi(ot_curve)\n",
    "      analyzer.metrics_list[\"osi\"].append(osi)\n",
    "  circ_var_list.append(np.array([val[2] for val in analyzer.metrics_list[\"circ_var\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "color_list = [color_vals[\"md_green\"], color_vals[\"md_blue\"], color_vals[\"md_red\"]]\n",
    "label_list = [\"Linear Autoencoder\", \"Sparse Autoencoder\", \"Sparse Coding\"]\n",
    "num_bins = 30\n",
    "width_ratios = [0.5, 0.25, 0.25]\n",
    "fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "gs0 = gridspec.GridSpec(1, 3, width_ratios=width_ratios)\n",
    "axes = []\n",
    "\n",
    "height_ratios = [0.13, 0.25, 0.25, 0.25]\n",
    "gs_hist = gridspec.GridSpecFromSubplotSpec(4, 1, gs0[0], height_ratios=height_ratios)\n",
    "axes.append(fig.add_subplot(gs_hist[1:3, 0]))\n",
    "variance_min = 0.0\n",
    "variance_max = 1.0\n",
    "bins = np.linspace(variance_min, variance_max, num_bins)\n",
    "bar_width = np.diff(bins).min()\n",
    "hist_list = []\n",
    "for variances, label, color in zip(circ_var_list, label_list, color_list):\n",
    "  hist, bin_edges = np.histogram(variances.flatten(), bins)\n",
    "  hist_list.append(hist)\n",
    "  bin_left, bin_right = bin_edges[:-1], bin_edges[1:]\n",
    "  bin_centers = bin_left + (bin_right - bin_left)/2\n",
    "  axes[-1].plot(bin_centers, hist, linestyle=\"-\", drawstyle=\"steps-mid\", color=color, label=label)\n",
    "  #axes[-1].bar(bin_centers, hist, width=bar_width, log=False,\n",
    "  #  align=\"center\", alpha=0.5, label=label)\n",
    "axes[-1].set_xticks(bin_left, minor=True)\n",
    "axes[-1].set_xticks(bin_left[::4], minor=False)\n",
    "axes[-1].xaxis.set_major_formatter(FormatStrFormatter(\"%0.0f\"))\n",
    "axes[-1].tick_params(\"both\", labelsize=fontsize)\n",
    "axes[-1].set_xlim([variance_min, variance_max])\n",
    "axes[-1].set_xticks([variance_min, variance_max])\n",
    "axes[-1].set_xticklabels([\"More\\nselective\", \"Less\\nselective\"])\n",
    "ticks = axes[-1].xaxis.get_major_ticks()\n",
    "ticks[0].label1.set_horizontalalignment(\"left\")\n",
    "ticks[1].label1.set_horizontalalignment(\"right\")\n",
    "y_max = np.max([np.max(hist) for hist in hist_list])\n",
    "axes[-1].set_ylim([0, y_max+1])\n",
    "axes[-1].set_title(\"Circular Variance\", fontsize=fontsize)\n",
    "axes[-1].set_ylabel(\"Count\", fontsize=fontsize)\n",
    "handles, labels = axes[-1].get_legend_handles_labels()\n",
    "#legend = axes[-1].legend(handles, label_list, fontsize=12,\n",
    "#  borderaxespad=0., bbox_to_anchor=[0.98, 0.98], loc=\"upper right\")\n",
    "legend = axes[-1].legend(handles=handles, labels=labels, fontsize=fontsize,\n",
    "  borderaxespad=0., framealpha=0.0, loc=\"upper right\")\n",
    "legend.get_frame().set_linewidth(0.0)\n",
    "for text, color in zip(legend.get_texts(), color_list):\n",
    "  text.set_color(color)\n",
    "for item in legend.legendHandles:\n",
    "  item.set_visible(False)\n",
    "\n",
    "gs_weights = gridspec.GridSpecFromSubplotSpec(len(analyzer_list), 1, gs0[1], hspace=-0.6)\n",
    "for gs_idx, analyzer in enumerate(analyzer_list):\n",
    "  weights = np.stack(analyzer.bf_stats[\"basis_functions\"], axis=0)[analyzer.bf_indices, ...]\n",
    "  weights = dp.norm_weights(weights)\n",
    "  vmin = np.min(weights)\n",
    "  vmax = np.max(weights)\n",
    "  num_plots = weights.shape[0]\n",
    "  num_plots_y = int(np.ceil(np.sqrt(num_plots)))\n",
    "  num_plots_x = int(np.ceil(np.sqrt(num_plots)))\n",
    "  gs_weights_inner = gridspec.GridSpecFromSubplotSpec(num_plots_y, num_plots_x, gs_weights[gs_idx],\n",
    "    hspace=-0.85)\n",
    "  bf_idx = 0\n",
    "  for plot_id in  np.ndindex((num_plots_y, num_plots_x)):\n",
    "    if bf_idx < num_plots:\n",
    "      axes.append(fig.add_subplot(gs_weights_inner[plot_id]))\n",
    "      axes[-1].imshow(np.squeeze(weights[bf_idx, ...]), vmin=vmin, vmax=vmax, cmap=\"Greys_r\")\n",
    "      bf_idx += 1\n",
    "    pf.clear_axis(axes[-1])\n",
    "\n",
    "gs_tuning = gridspec.GridSpecFromSubplotSpec(len(analyzer_list), 1, gs0[2], hspace=-0.6)\n",
    "for analyzer_idx, analyzer in enumerate(analyzer_list):\n",
    "  contrasts = analyzer.ot_grating_responses[\"contrasts\"]\n",
    "  orientations = analyzer.ot_grating_responses[\"orientations\"]\n",
    "  activations = analyzer.ot_grating_responses[\"mean_responses\"]\n",
    "  activations = activations / np.max(activations[analyzer.bf_indices, -1, ...])\n",
    "  orientations = np.asarray(orientations)*(180/np.pi) #convert to degrees for plotting\n",
    "  orientations = orientations / np.max(orientations)\n",
    "  num_plots = len(analyzer.bf_indices)\n",
    "  num_plots_y = int(np.ceil(np.sqrt(num_plots)))\n",
    "  num_plots_x = int(np.ceil(np.sqrt(num_plots)))\n",
    "  gs_tuning_inner = gridspec.GridSpecFromSubplotSpec(num_plots_y, num_plots_x, gs_tuning[analyzer_idx],\n",
    "      hspace=-0.85)\n",
    "  bf_idx = 0\n",
    "  for plot_id in np.ndindex((num_plots_y, num_plots_x)):\n",
    "    if bf_idx < num_plots:\n",
    "      if bf_idx == 0:\n",
    "        axes.append(fig.add_subplot(gs_tuning_inner[plot_id]))\n",
    "        ax_orig_id = len(axes)-1\n",
    "      else:\n",
    "        axes.append(fig.add_subplot(gs_tuning_inner[plot_id], sharey=axes[ax_orig_id], sharex=axes[ax_orig_id]))\n",
    "      contrast_idx = -1\n",
    "      activity = activations[analyzer.bf_indices[bf_idx], contrast_idx, :]\n",
    "      #activity = activity / np.max(activity)\n",
    "      axes[-1].plot(orientations, activity, linewidth=0.5, color='k')\n",
    "      axes[-1].scatter(orientations, activity, s=0.1, c='k')\n",
    "      axes[-1].set_aspect('equal', adjustable='box')\n",
    "      axes[-1].set_yticks([])\n",
    "      axes[-1].set_xticks([])\n",
    "      bf_idx += 1\n",
    "    (y_id, x_id) = plot_id\n",
    "    if y_id == 0 and x_id == 0:\n",
    "      plt.text(x=0.1, y=1.4, s=analyzer.analysis_params.display_name, horizontalalignment='center',\n",
    "        verticalalignment='center', transform=axes[-1].transAxes, fontsize=fontsize)\n",
    "\n",
    "#gs_circvar = gridspec.GridSpecFromSubplotSpec(len(analyzer_list), 1, gs0[3])#, hspace=-0.5)\n",
    "#for analyzer_index, analyzer in enumerate(analyzer_list):\n",
    "#  cv_data = [val for index, val in enumerate(analyzer.metrics_list[\"circ_var\"]) if index in bf_indices]\n",
    "#  orientations = (np.pi * np.arange(len(cv_data)) / len(cv_data)) - (np.pi/2) # relative to preferred\n",
    "#  num_bfs = len(cv_data)\n",
    "#  num_plots_y = np.int32(np.ceil(np.sqrt(num_bfs)))+1\n",
    "#  num_plots_x = np.int32(np.ceil(np.sqrt(num_bfs)))\n",
    "#  gs_circvar_inner = gridspec.GridSpecFromSubplotSpec(num_plots_y, num_plots_x, gs_circvar[analyzer_index],\n",
    "#    wspace=0.4, hspace=0.4)\n",
    "#  bf_idx = 0\n",
    "#  for plot_id in np.ndindex((num_plots_y, num_plots_x)):\n",
    "#    (y_id, x_id) = plot_id\n",
    "#    if y_id == 0 and x_id == 0:\n",
    "#      axes.append(fig.add_subplot(gs_circvar_inner[plot_id]))\n",
    "#      ax00 = axes[-1]\n",
    "#    else:\n",
    "#      axes.append(fig.add_subplot(gs_circvar_inner[plot_id]))\n",
    "#    if bf_idx < num_bfs:\n",
    "#      axes[-1].plot(np.real(cv_data[bf_idx][0]), np.imag(cv_data[bf_idx][0]), c='g', linewidth=0.5)\n",
    "#      #axes[-1].scatter(np.real(cv_data[bf_idx][0]), np.imag(cv_data[bf_idx][0]), c='g', s=4)\n",
    "#      #axes[-1].quiver(np.real(cv_data[bf_idx][1]), np.imag(cv_data[bf_idx][1]),\n",
    "#      #          angles='xy', scale_units='xy', scale=1.0, color='b', width=0.01)\n",
    "#      #axes[-1].quiver(0.5, 0.5, color='b')\n",
    "#      axes[-1].yaxis.set_major_formatter(FormatStrFormatter('%0.2g'))\n",
    "#      xaxis_size = max(np.max(np.real(cv_data[bf_idx][0])), 1.0)\n",
    "#      yaxis_size = max(np.max(np.imag(cv_data[bf_idx][0])), 1.0)\n",
    "#      axes[-1].set_yticks([])#[-1. * yaxis_size, yaxis_size])\n",
    "#      axes[-1].set_xticks([])#[-1. * xaxis_size, xaxis_size])\n",
    "#      # put the circular variance index in the upper left\n",
    "#      #axes[-1].text(0.02, 0.97, '{:.2f}'.format(cv_data[bf_idx][2]),\n",
    "#      #        horizontalalignment='left', verticalalignment='top',\n",
    "#      #        transform=axes[-1].transAxes, color='b', fontsize=10)\n",
    "#      bf_idx += 1\n",
    "#    else:\n",
    "#      pf.clear_axis(axes[-1])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "for analyzer in analyzer_list:\n",
    "  for ext in [\".png\", \".eps\"]:\n",
    "    save_name = (analyzer.analysis_out_dir+\"/vis/circular_variance_combo\"\n",
    "      +\"_\"+analyzer.analysis_params.save_info+ext)\n",
    "    fig.savefig(save_name, transparent=False, bbox_inches=\"tight\", pad_inches=0.01, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rads = []\n",
    "max_rads = []\n",
    "for analyzer in analyzer_list:\n",
    "  analyzer.bf_spatial_freq_rads = [np.sqrt(x**2+y**2) for (y,x) in analyzer.bf_stats[\"fourier_centers\"]]\n",
    "  min_rads.append(np.min(analyzer.bf_spatial_freq_rads))\n",
    "  max_rads.append(np.max(analyzer.bf_spatial_freq_rads))\n",
    "  \n",
    "num_bins = 10\n",
    "min_rad = np.min(min_rads)\n",
    "max_rad = np.max(max_rads)\n",
    "bins = np.linspace(min_rad, max_rad, num_bins)\n",
    "fig, ax = plt.subplots(1, figsize=figsize, dpi=dpi)\n",
    "hist_max = []\n",
    "for analyzer in analyzer_list:\n",
    "  hist, bin_edges = np.histogram(analyzer.bf_spatial_freq_rads, bins, density=True)\n",
    "  bin_left, bin_right = bin_edges[:-1], bin_edges[1:]\n",
    "  bin_centers = bin_left + (bin_right - bin_left)/2\n",
    "  \n",
    "  label = analyzer.model.params.model_type.upper()# + \" \" + str(analyzer.model.get_num_latent())# + \" van Hateren\"\n",
    "  #label = re.sub(\"_\", \" \", analyzer.model_name)\n",
    "  ax.plot(bin_centers, hist, alpha=1.0, linestyle=\"-\", drawstyle=\"steps-mid\", label=label)\n",
    "  hist_max.append(np.max(hist))\n",
    "  \n",
    "ax.set_xticks(bin_left, minor=True)\n",
    "ax.set_xticks(bin_left[::4], minor=False)\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter(\"%0.0f\"))\n",
    "ax.tick_params(\"both\", labelsize=16)\n",
    "ax.set_xlim([min_rad, max_rad])\n",
    "ax.set_xticks([0, int(np.floor(max_rad/4)), int(2*np.floor(max_rad/4)),\n",
    "  int(3*np.floor(max_rad/4)), max_rad])\n",
    "ax.set_ylim([0, 0.6])\n",
    "ax.set_xlabel(\"Spatial Frequency\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"Density\", fontsize=fontsize)\n",
    "ax.set_title(\"Neuron Weight Spatial Frequency Histogram\", fontsize=fontsize)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legend = ax.legend(handles, labels, fontsize=fontsize, ncol=3,\n",
    "  borderaxespad=0., bbox_to_anchor=[0.01, 0.99], fancybox=True, loc=\"upper left\")\n",
    "for line in legend.get_lines():\n",
    "  line.set_linewidth(3)\n",
    "plt.show()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
