{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Computation Paper Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import tensorflow as tf\n",
    "from data.dataset import Dataset\n",
    "import data.data_selector as ds\n",
    "import utils.data_processing as dp\n",
    "import utils.plot_functions as pf\n",
    "import analysis.analysis_picker as ap"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def compute_iso_vectors(analyzer, min_angle, max_angle, num_neurons, use_bf_stats):\n",
    "  \"\"\"\n",
    "  Calculate all projection vectors for each target neuron\n",
    "  For each target neuron, build dataset of random orthogonal vectors & selected orthogonal vectors\n",
    "  \"\"\"\n",
    "  # Get list of candidate target vectors\n",
    "  if(use_bf_stats):\n",
    "    analyzer.neuron_angles, analyzer.plot_matrix = analyzer.get_neuron_angles(analyzer.bf_stats)\n",
    "  else:\n",
    "    optimal_stims_dict = {\"patch_edge_size\":int(np.sqrt(analyzer.model.params.data_shape)),\n",
    "      \"num_outputs\":len(analyzer.analysis_params.neuron_vis_targets),\n",
    "      \"basis_functions\":[]}\n",
    "    for target_id in range(len(analyzer.analysis_params.neuron_vis_targets)):\n",
    "      bf = analyzer.neuron_vis_output[\"optimal_stims\"][target_id][-1]\n",
    "      optimal_stims_dict[\"basis_functions\"].append(bf.reshape((optimal_stims_dict[\"patch_edge_size\"],)*2))\n",
    "    analyzer.neuron_angles, analyzer.plot_matrix = analyzer.get_neuron_angles(optimal_stims_dict)\n",
    "  orig_min_angle = min_angle\n",
    "  orig_max_angle = max_angle\n",
    "  vectors = []\n",
    "  while len(vectors) <= num_neurons:\n",
    "    vectors = np.argwhere(np.logical_and(analyzer.plot_matrix<max_angle, analyzer.plot_matrix>min_angle))\n",
    "    if min_angle > 5:\n",
    "      min_angle -= 1\n",
    "    if max_angle < 89:\n",
    "      max_angle += 1\n",
    "  if min_angle < orig_min_angle or max_angle > orig_max_angle:\n",
    "    print(\"compute_iso_vectors:WARNING:\"\n",
    "      +\"The provided angle range was too small, the new angle range is [%g, %g]\"%(min_angle, max_angle))\n",
    "\n",
    "  analyzer.target_neuron_ids = []\n",
    "  analyzer.comparison_neuron_ids = [] # list of lists [num_targets][num_comparisons_per_target]\n",
    "  analyzer.target_vectors = []\n",
    "  analyzer.rand_orth_vectors = []\n",
    "  analyzer.comparison_vectors = []\n",
    "  for vector_id in range(num_neurons):\n",
    "    target_neuron_id = vectors[vector_id, 0]\n",
    "    analyzer.target_neuron_ids.append(target_neuron_id)\n",
    "\n",
    "    # Reshape & rescale target vector\n",
    "    target_vector = analyzer.bf_stats[\"basis_functions\"][target_neuron_id]\n",
    "    target_vector = target_vector.reshape(analyzer.model_params.num_pixels)\n",
    "    target_vector = target_vector / np.linalg.norm(target_vector) \n",
    "    analyzer.target_vectors.append(target_vector)\n",
    "\n",
    "    # Build matrix of random orthogonal vectors\n",
    "    analyzer.rand_orth_vectors.append(dp.get_rand_orth_vectors(target_vector, analyzer.model.params.num_pixels-1))\n",
    "    \n",
    "    # Build matrix of comparison vectors\n",
    "    comparison_orth_vectors = target_vector.T[:,None] # matrix of alternate vectors\n",
    "    if(use_bf_stats):\n",
    "      sub_comparison_neuron_ids = [index for index in range(analyzer.bf_stats[\"num_outputs\"]) if index != target_neuron_id]\n",
    "    else:\n",
    "      sub_comparison_neuron_ids = [index for index in range(optimal_stims_dict[\"num_outputs\"]) if index != target_neuron_id]\n",
    "    comparison_vector_matrix = target_vector.T[:,None] # matrix of alternate vectors\n",
    "    for comparison_neuron_id in sub_comparison_neuron_ids:\n",
    "      if(use_bf_stats):\n",
    "        comparison_vector = analyzer.bf_stats[\"basis_functions\"][comparison_neuron_id]\n",
    "      else:\n",
    "        comparison_vector = optimal_stims_dict[\"basis_functions\"][comparison_neuron_id]\n",
    "      comparison_vector = comparison_vector.reshape(analyzer.model_params.num_pixels)\n",
    "      comparison_vector = np.squeeze((comparison_vector / np.linalg.norm(comparison_vector)).T)\n",
    "      comparison_vector_matrix = np.append(comparison_vector_matrix, comparison_vector[:,None], axis=1)\n",
    "    analyzer.comparison_neuron_ids.append(sub_comparison_neuron_ids)\n",
    "    analyzer.comparison_vectors.append(comparison_vector_matrix.T[1:,:])\n",
    "    \n",
    "    return analyzer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_contour_dataset(analyzer, num_comparison_vects, use_random_orth_vects, x_range, y_range, num_images):\n",
    "  \"\"\"\n",
    "  datapoints has shape [num_target_neurons][num_comparisons_per_target (or num_planes)][num_datapoints, datapoint_length]\n",
    "  \"\"\"\n",
    "  x_pts = np.linspace(x_range[0], x_range[1], int(np.sqrt(num_images)))\n",
    "  y_pts = np.linspace(y_range[0], y_range[1], int(np.sqrt(num_images)))\n",
    "  \n",
    "  X_mesh, Y_mesh = np.meshgrid(x_pts, y_pts)\n",
    "  proj_datapoints = np.stack([X_mesh.reshape(num_images), Y_mesh.reshape(num_images)], axis=1)\n",
    "  \n",
    "  out_dict = {\n",
    "    \"datapoints\": [],\n",
    "    \"proj_target_neuron\": [],\n",
    "    \"proj_comparison_neuron\": [],\n",
    "    \"proj_orth_vect\": [],\n",
    "    \"orth_vect\": [],\n",
    "    \"proj_datapoints\": proj_datapoints,\n",
    "    \"X_mesh\": X_mesh,\n",
    "    \"Y_mesh\": Y_mesh}\n",
    "\n",
    "  if use_random_orth_vects:\n",
    "    comparison_vectors = analyzer.rand_orth_vectors\n",
    "  else:\n",
    "    comparison_vectors = analyzer.comparison_vectors\n",
    "  for target_vect, all_comparison_vects in zip(analyzer.target_vectors, comparison_vectors):\n",
    "    proj_target_neuron_sub_list = []\n",
    "    proj_comparison_neuron_sub_list = []\n",
    "    proj_orth_vect_sub_list = []\n",
    "    orth_vect_sub_list = []\n",
    "    datapoints_sub_list = []\n",
    "    if num_comparison_vects is None or num_comparison_vects > all_comparison_vects.shape[0]-1:\n",
    "        num_comparison_vects = all_comparison_vects.shape[0]\n",
    "    for comparison_vect_idx in range(num_comparison_vects): # Each contour plane for the population study\n",
    "      comparison_vect = np.squeeze(all_comparison_vects[comparison_vect_idx, :])\n",
    "      proj_matrix, orth_vect = dp.bf_projections(target_vect, comparison_vect)\n",
    "      proj_target_neuron_sub_list.append(np.dot(proj_matrix, target_vect).T) #project\n",
    "      proj_comparison_neuron_sub_list.append(np.dot(proj_matrix, comparison_vect).T) #project\n",
    "      proj_orth_vect_sub_list.append(np.dot(proj_matrix, orth_vect).T) #project\n",
    "      orth_vect_sub_list.append(orth_vect)\n",
    "      datapoints = np.stack([np.dot(proj_matrix.T, proj_datapoints[data_id,:])\n",
    "        for data_id in range(num_images)], axis=0) #inject\n",
    "      datapoints = dp.reshape_data(datapoints, flatten=False)[0]\n",
    "      datapoints = {\"test\": Dataset(datapoints, lbls=None, ignore_lbls=None, rand_state=analyzer.rand_state)}\n",
    "      datapoints = analyzer.model.reshape_dataset(datapoints, analyzer.model_params)\n",
    "      datapoints_sub_list.append(datapoints)\n",
    "    out_dict[\"datapoints\"].append(datapoints_sub_list)\n",
    "    out_dict[\"proj_target_neuron\"].append(proj_target_neuron_sub_list)\n",
    "    out_dict[\"proj_comparison_neuron\"].append(proj_comparison_neuron_sub_list)\n",
    "    out_dict[\"proj_orth_vect\"].append(proj_orth_vect_sub_list)\n",
    "    out_dict[\"orth_vect\"].append(orth_vect_sub_list)\n",
    "  return out_dict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_normalized_activations(analyzer, contour_dataset):\n",
    "  \"\"\"\n",
    "  contour_dataset should have shape [num_target_neurons][num_comparisons_per_target][num_datapoints, datapoint_length]\n",
    "  output list is shape [num_target_neurons][num_comparisons_per_target][num_datapoints_x, num_datapoints_y]\n",
    "  \n",
    "  # TODO: Verify batch size is working for compute_activations\n",
    "  \"\"\"\n",
    "  activations_list = []\n",
    "  for target_index, neuron_index in enumerate(analyzer.target_neuron_ids):\n",
    "    activity_sub_list = []\n",
    "    for comparison_index, datapoints in enumerate(contour_dataset[target_index]):\n",
    "      num_images, data_size = datapoints[\"test\"].images.shape \n",
    "      batch_size = 1\n",
    "      for n in range(2, num_images):\n",
    "        if num_images%n == 0:\n",
    "          batch_size = num_images // n # second greatest factor\n",
    "          break\n",
    "      activations = analyzer.compute_activations(datapoints[\"test\"].images, batch_size)\n",
    "      activations = activations[:, neuron_index]\n",
    "      activity_max = np.amax(np.abs(activations))\n",
    "      activations = activations / (activity_max + 0.00001)\n",
    "      activations = activations.reshape(int(np.sqrt(num_images)), int(np.sqrt(num_images)))\n",
    "      activity_sub_list.append(activations)\n",
    "    activations_list.append(activity_sub_list)\n",
    "  return activations_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "min_angle = 15\n",
    "max_angle = 60\n",
    "num_neurons = 2 # How many neurons to plot\n",
    "use_bf_stats = True # If false, then use optimal stimulus\n",
    "num_comparison_vects = 10\n",
    "use_random_orth_vects = False\n",
    "x_range = [-2, 2]#[-0.1, 3.9]\n",
    "y_range = [-2, 2]\n",
    "num_images = int(10**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 16\n",
    "figsize = (10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lca_512_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_512_vh\"\n",
    "    self.display_name = \"Sparse Coding\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_768_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_768_vh\"\n",
    "    self.display_name = \"Sparse Coding\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_1024_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_1024_vh\"\n",
    "    self.display_name = \"Sparse Coding\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class sae_768_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"sae\"\n",
    "    self.model_name = \"sae_768_vh\"\n",
    "    self.display_name = \"Sparse Autoencoder\"\n",
    "    self.version = \"1.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class rica_768_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"rica\"\n",
    "    self.model_name = \"rica_768_vh\"\n",
    "    self.display_name = \"Linear Autoencoder\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_768_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_768_mnist\"\n",
    "    self.display_name = \"Sparse Coding\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class ae_768_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"ae\"\n",
    "    self.model_name = \"ae_768_mnist\"\n",
    "    self.display_name = \"ReLU Autoencoder\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class sae_768_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"sae\"\n",
    "    self.model_name = \"sae_768_mnist\"\n",
    "    self.display_name = \"Sparse Autoencoder\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class rica_768_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"rica\"\n",
    "    self.model_name = \"rica_768_mnist\"\n",
    "    self.display_name = \"Linear Autoencoder\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class ae_deep_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"ae\"\n",
    "    self.model_name = \"ae_deep_mnist\"\n",
    "    self.display_name = \"ReLU Autoencoder\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iso-contour activations comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params_list = [lca_768_vh_params(), sae_768_vh_params(), sae_768_vh_params(), rica_768_vh_params()]\n",
    "params_list = [rica_768_mnist_params(), ae_768_mnist_params(), sae_768_mnist_params(), lca_768_mnist_params()]\n",
    "for params in params_list:\n",
    "  params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+params.model_name)\n",
    "analyzer_list = [ap.get_analyzer(params.model_type) for params in params_list]\n",
    "for analyzer, params in zip(analyzer_list, params_list):\n",
    "  analyzer.setup(params)\n",
    "  analyzer.model.setup(analyzer.model_params)\n",
    "  analyzer.load_analysis(save_info=params.save_info)\n",
    "  analyzer.model_name = params.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analyzer in analyzer_list:\n",
    "  run_params = np.load(analyzer.analysis_out_dir+\"savefiles/iso_params_\"+analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()\n",
    "  min_angle = run_params[\"min_angle\"]\n",
    "  max_angle = run_params[\"max_angle\"]\n",
    "  num_neurons = run_params[\"num_neurons\"]\n",
    "  use_bf_stats = run_params[\"use_bf_stats\"]\n",
    "  num_comparison_vectors = run_params[\"num_comparison_vects\"]\n",
    "  use_random_orth_vects = run_params[\"use_random_orth_vects\"]\n",
    "  x_range = run_params[\"x_range\"]\n",
    "  y_range = run_params[\"y_range\"]\n",
    "  num_images = run_params[\"num_images\"]\n",
    "  analyzer.activations = np.load(analyzer.analysis_out_dir+\"savefiles/iso_normalized_activations_\"+analyzer.analysis_params.save_info+\".npz\")[\"data\"]\n",
    "  analyzer.contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_contour_dataset_\"+analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_list[0].activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_index = [0,0,0,0]#[1, 1, 1, 1]\n",
    "orth_index = [2, 2, 2, 2]\n",
    "num_plots_y = 2\n",
    "num_plots_x = 2\n",
    "num_levels = 10\n",
    "\n",
    "gs0 = gridspec.GridSpec(num_plots_y, num_plots_x, wspace=-0.1, hspace=0.5)\n",
    "fig = plt.figure(figsize=(figsize[0]*1.5, figsize[1]*1))\n",
    "cmap = plt.get_cmap(\"cividis\")#\"Greys_r\")#\"viridis\")\n",
    "vmin = np.min([analyzer.activations for analyzer in analyzer_list])\n",
    "vmax = np.max([analyzer.activations for analyzer in analyzer_list])\n",
    "levels = np.linspace(vmin, vmax, num_levels)\n",
    "  \n",
    "analyzer_index = 0\n",
    "for plot_id in  np.ndindex((num_plots_y, num_plots_x)):\n",
    "  (y_id, x_id) = plot_id\n",
    "  if type(neuron_index) == list:\n",
    "    analyzer_neuron_index = neuron_index[analyzer_index]\n",
    "  else:\n",
    "    analyzer_neuron_index = neuron_index\n",
    "  if type(orth_index) == list:\n",
    "    analyzer_orth_index = orth_index[analyzer_index]\n",
    "  else:\n",
    "    analyzer_orth_index = orth_index\n",
    "  analyzer = analyzer_list[analyzer_index]\n",
    "  gs1 = gridspec.GridSpecFromSubplotSpec(1, 2, gs0[plot_id], wspace=-0.15, hspace=0.1)\n",
    "  curve_ax = pf.clear_axis(fig.add_subplot(gs1[0]))\n",
    "  curve_ax.set_title(analyzer.analysis_params.display_name, fontsize=fontsize)\n",
    "\n",
    "  norm_activity = analyzer.activations[analyzer_neuron_index, analyzer_orth_index, ...]\n",
    "  contsf = curve_ax.contourf(analyzer.contour_dataset[\"X_mesh\"], analyzer.contour_dataset[\"Y_mesh\"], norm_activity,\n",
    "    levels=levels, vmin=vmin, vmax=vmax, alpha=1.0, antialiased=True, cmap=cmap)\n",
    "\n",
    "  proj_target = analyzer.contour_dataset[\"proj_target_neuron\"][analyzer_neuron_index][analyzer_orth_index]\n",
    "  curve_ax.arrow(0, 0, proj_target[0].item(), proj_target[1].item(),\n",
    "    width=0.00, head_width=0.15, head_length=0.15, fc='k', ec='k',\n",
    "    linestyle='-', linewidth=3)\n",
    "\n",
    "  proj_comparison = analyzer.contour_dataset[\"proj_comparison_neuron\"][analyzer_neuron_index][analyzer_orth_index]\n",
    "  curve_ax.arrow(0, 0, proj_comparison[0].item(), proj_comparison[1].item(),\n",
    "    width=0.00, head_width=0.15, head_length=0.15, fc='k', ec='k',\n",
    "    linestyle=\"dotted\", linewidth=2.0)\n",
    "\n",
    "  #for proj_alt in analyzer.contour_dataset[\"proj_comparison_neuron\"][analyzer_neuron_index]:\n",
    "  #  if not np.all(proj_alt == proj_comparison):\n",
    "  #    curve_ax.arrow(0, 0, proj_alt[0].item(), proj_alt[1].item(),\n",
    "  #      width=0.00, head_width=0.15, head_length=0.15, fc='w', ec='w',\n",
    "  #      linestyle=\"dashed\", linewidth=1.0, alpha=0.9)\n",
    "\n",
    "  proj_orth = analyzer.contour_dataset[\"proj_orth_vect\"][analyzer_neuron_index][analyzer_orth_index]\n",
    "  curve_ax.arrow(0, 0, proj_orth[0].item(), proj_orth[1].item(),\n",
    "    width=0.00, head_width=0.10, head_length=0.10, fc='k', ec='k',\n",
    "    linestyle=\"-\", linewidth=3)\n",
    "\n",
    "  curve_ax.plot(x_range, [0,0], color='k')\n",
    "  curve_ax.plot([0,0], y_range, color='k')\n",
    "  #curve_ax.set_xticks(x_range)\n",
    "  #curve_ax.set_yticks(y_range)\n",
    "  #curve_ax.tick_params(axis=\"both\", bottom=True, top=False, left=True, right=False)\n",
    "  #curve_ax.get_xaxis().set_visible(True)\n",
    "  #curve_ax.get_yaxis().set_visible(True)\n",
    "  #curve_ax.set_xticklabels(x_range, fontsize=fontsize)\n",
    "  #curve_ax.set_yticklabels(x_range, fontsize=fontsize)\n",
    "\n",
    "  #gs2 = gridspec.GridSpecFromSubplotSpec(2, 1, gs1[1], wspace=0.0, hspace=0.5)#-0.55)\n",
    "  #target_vect_ax = pf.clear_axis(fig.add_subplot(gs2[0]))\n",
    "  #target_vect_ax.imshow(analyzer.bf_stats[\"basis_functions\"][analyzer.target_neuron_ids[0]], cmap=\"Greys_r\")\n",
    "  #target_vect_ax.set_title(\"Primary\\nBasis Function\", color='r', fontsize=16)\n",
    "  #comparison_vect_ax = pf.clear_axis(fig.add_subplot(gs2[1]))\n",
    "  #comparison_vect_ax.imshow(analyzer.bf_stats[\"basis_functions\"][analyzer.comparison_neuron_ids[0][0]], cmap=\"Greys_r\")\n",
    "  #comparison_vect_ax.set_title(\"Comparison\\nBasis Function\", color='k', fontsize=16)\n",
    "\n",
    "  analyzer_index += 1\n",
    "\n",
    "\n",
    "plt.show()\n",
    "for analyzer in analyzer_list:\n",
    "  fig.savefig(analyzer.analysis_out_dir+\"/vis/iso_contour_comparison\"+analyzer.analysis_params.save_info+\".eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orientation Selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = [rica_768_vh_params(), sae_768_vh_params(), lca_768_vh_params()]\n",
    "for params in params_list:\n",
    "  params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+params.model_name)\n",
    "analyzer_list = [ap.get_analyzer(params.model_type) for params in params_list]\n",
    "for analyzer, params in zip(analyzer_list, params_list):\n",
    "  analyzer.setup(params)\n",
    "  analyzer.model.setup(analyzer.model_params)\n",
    "  analyzer.load_analysis(save_info=params.save_info)\n",
    "  analyzer.model_name = params.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rads = []\n",
    "max_rads = []\n",
    "for analyzer in analyzer_list:\n",
    "  analyzer.bf_spatial_freq_rads = [np.sqrt(x**2+y**2) for (y,x) in analyzer.bf_stats[\"fourier_centers\"]]\n",
    "  min_rads.append(np.min(analyzer.bf_spatial_freq_rads))\n",
    "  max_rads.append(np.max(analyzer.bf_spatial_freq_rads))\n",
    "  \n",
    "num_bins = 10\n",
    "min_rad = np.min(min_rads)\n",
    "max_rad = np.max(max_rads)\n",
    "bins = np.linspace(min_rad, max_rad, num_bins)\n",
    "fig, ax = plt.subplots(1, figsize=figsize)\n",
    "hist_max = []\n",
    "for analyzer in analyzer_list:\n",
    "  hist, bin_edges = np.histogram(analyzer.bf_spatial_freq_rads, bins, density=True)\n",
    "  bin_left, bin_right = bin_edges[:-1], bin_edges[1:]\n",
    "  bin_centers = bin_left + (bin_right - bin_left)/2\n",
    "  \n",
    "  label = analyzer.model.params.model_type.upper() + \" \" + str(analyzer.model.get_num_latent())# + \" van Hateren\"\n",
    "  #label = re.sub(\"_\", \" \", analyzer.model_name)\n",
    "  ax.plot(bin_centers, hist, alpha=1.0, linestyle=\"-\", drawstyle=\"steps-mid\", label=label)\n",
    "  hist_max.append(np.max(hist))\n",
    "  \n",
    "ax.set_xticks(bin_left, minor=True)\n",
    "ax.set_xticks(bin_left[::4], minor=False)\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter(\"%0.0f\"))\n",
    "ax.tick_params(\"both\", labelsize=16)\n",
    "ax.set_xlim([min_rad, max_rad])\n",
    "ax.set_xticks([0, int(np.floor(max_rad/4)), int(2*np.floor(max_rad/4)),\n",
    "  int(3*np.floor(max_rad/4)), max_rad])\n",
    "ax.set_ylim([0, 0.6])\n",
    "ax.set_xlabel(\"Spatial Frequency\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"Normalized Density\", fontsize=fontsize)\n",
    "ax.set_title(\"Neuron Weight Spatial Frequency Histogram\", fontsize=fontsize)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legend = ax.legend(handles, labels, fontsize=fontsize, ncol=3,\n",
    "  borderaxespad=0., bbox_to_anchor=[0.01, 0.99], fancybox=True, loc=\"upper left\")\n",
    "for line in legend.get_lines():\n",
    "  line.set_linewidth(3)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_heatmap_fig = pf.plot_weight_angle_heatmap(analyzer.plot_matrix, angle_min=0, angle_max=180,\n",
    "  title=\"Angles Between Neurons\", figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRAE Iso-Response Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = [ae_deep_mnist_params()]#,lca_768_vh_params()]#, rica_768_vh_params(), sae_768_vh_params()]\n",
    "for params in params_list:\n",
    "  params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+params.model_name)\n",
    "analyzer_list = [ap.get_analyzer(params.model_type) for params in params_list]\n",
    "for analyzer, params in zip(analyzer_list, params_list):\n",
    "  analyzer.setup(params)\n",
    "  analyzer.model.setup(analyzer.model_params)\n",
    "  analyzer.load_analysis(save_info=params.save_info)\n",
    "  analyzer.model_name = params.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_angle = 20\n",
    "max_angle = 55\n",
    "num_neurons = 2 # How many neurons to plot\n",
    "use_bf_stats = True # If false, then use optimal stimulus\n",
    "num_comparison_vects = 10\n",
    "use_random_orth_vects = False\n",
    "x_range = [-2, 2]\n",
    "y_range = [-2, 2]\n",
    "num_images = int(10**2)\n",
    "\n",
    "for analyzer in analyzer_list:\n",
    "  compute_iso_vectors(analyzer, min_angle, max_angle, num_neurons, use_bf_stats)\n",
    "  contour_dataset = get_contour_dataset(analyzer, num_comparison_vects,\n",
    "    use_random_orth_vects, x_range, y_range, num_images)\n",
    "  analyzer.activations = get_normalized_activations(analyzer, contour_dataset[\"datapoints\"])\n",
    "  contour_dataset.pop(\"datapoints\")\n",
    "  analyzer.contour_dataset = contour_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots_y = num_neurons + 1 # extra dimension for example image\n",
    "num_plots_x = num_neurons + 1 # extra dimension for example image\n",
    "\n",
    "gs0 = gridspec.GridSpec(num_plots_y, num_plots_x, wspace=0.1, hspace=0.1)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "cmap = plt.get_cmap('viridis')\n",
    "\n",
    "orth_vectors = []\n",
    "for neuron_loop_index in range(num_neurons): # rows\n",
    "  for orth_loop_index in range(num_neurons): # columns\n",
    "    norm_activity = analyzer.activations[neuron_loop_index][orth_loop_index]\n",
    "    proj_target = analyzer.contour_dataset[\"proj_target_neuron\"][neuron_loop_index][orth_loop_index]\n",
    "    proj_comparison = analyzer.contour_dataset[\"proj_comparison_neuron\"][neuron_loop_index][orth_loop_index]\n",
    "    proj_orth = analyzer.contour_dataset[\"proj_orth_vect\"][neuron_loop_index][orth_loop_index]\n",
    "    orth_vectors.append(analyzer.contour_dataset[\"orth_vect\"][neuron_loop_index][orth_loop_index])\n",
    "\n",
    "    curve_plot_y_idx = neuron_loop_index + 1\n",
    "    curve_plot_x_idx = orth_loop_index + 1\n",
    "    curve_ax = pf.clear_axis(fig.add_subplot(gs0[curve_plot_y_idx, curve_plot_x_idx]))\n",
    "\n",
    "    # NOTE: each subplot has a renormalized color scale\n",
    "    # TODO: Add scale bar like in the lca inference plots\n",
    "    vmin = np.min(norm_activity)\n",
    "    vmax = np.max(norm_activity)\n",
    "\n",
    "    levels = 5\n",
    "    contsf = curve_ax.contourf(analyzer.contour_dataset[\"X_mesh\"], analyzer.contour_dataset[\"Y_mesh\"], norm_activity,\n",
    "      levels=levels, vmin=vmin, vmax=vmax, alpha=1.0, antialiased=True, cmap=cmap)\n",
    "\n",
    "    curve_ax.arrow(0, 0, proj_target[0].item(), proj_target[1].item(),\n",
    "      width=0.05, head_width=0.15, head_length=0.15, fc='r', ec='r')\n",
    "    curve_ax.arrow(0, 0, proj_comparison[0].item(), proj_comparison[1].item(),\n",
    "      width=0.05, head_width=0.15, head_length=0.15, fc='w', ec='w')\n",
    "    curve_ax.arrow(0, 0, proj_orth[0].item(), proj_orth[1].item(),\n",
    "      width=0.05, head_width=0.15, head_length=0.15, fc='k', ec='k')\n",
    "\n",
    "    curve_ax.set_xlim(x_range)\n",
    "    curve_ax.set_ylim(y_range)\n",
    "    \n",
    "for plot_y_id in range(num_plots_y):\n",
    "  for plot_x_id in range(num_plots_x):\n",
    "    if plot_y_id > 0 and plot_x_id == 0:\n",
    "      bf_ax = pf.clear_axis(fig.add_subplot(gs0[plot_y_id, plot_x_id]))\n",
    "      bf_resh = analyzer.target_vectors[plot_y_id-1].reshape((int(np.sqrt(np.prod(analyzer.model.params.data_shape))),\n",
    "        int(np.sqrt(np.prod(analyzer.model.params.data_shape)))))\n",
    "      bf_ax.imshow(bf_resh, cmap=\"Greys_r\")\n",
    "      if plot_y_id == 1:\n",
    "        bf_ax.set_title(\"Target vectors\", color=\"r\", fontsize=16)\n",
    "    if plot_y_id == 0 and plot_x_id > 0:\n",
    "      orth_img = orth_vectors[plot_x_id-1].reshape(int(np.sqrt(np.prod(analyzer.model.params.data_shape))),\n",
    "        int(np.sqrt(np.prod(analyzer.model.params.data_shape))))\n",
    "      orth_ax = pf.clear_axis(fig.add_subplot(gs0[plot_y_id, plot_x_id]))\n",
    "      orth_ax.imshow(orth_img, cmap=\"Greys_r\")\n",
    "      if plot_x_id == 1:\n",
    "        orth_ax.set_title(\"Orthogonal vectors\", color=\"k\", fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "#fig.savefig(analyzer.analysis_out_dir+\"/vis/iso_contour_grid_04.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
