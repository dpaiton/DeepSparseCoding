{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Computation Paper Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import tensorflow as tf\n",
    "from data.dataset import Dataset\n",
    "import data.data_selector as ds\n",
    "import utils.data_processing as dp\n",
    "import utils.plot_functions as pf\n",
    "import analysis.analysis_picker as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 18\n",
    "figsize = (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lca_512_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_512_vh\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_768_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_768_vh\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_1024_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_1024_vh\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class sae_768_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"sae\"\n",
    "    self.model_name = \"sae_768_vh\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class rica_768_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"rica\"\n",
    "    self.model_name = \"rica_768_vh\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_768_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_768_mnist\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class sae_768_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"sae\"\n",
    "    self.model_name = \"sae_768_mnist\"\n",
    "    self.version = \"1.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = [lca_768_vh_params()]#, rica_768_vh_params(), sae_768_vh_params()]\n",
    "for params in params_list:\n",
    "  params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+params.model_name)\n",
    "  analyzer_list = [ap.get_analyzer(params.model_type) for params in params_list]\n",
    "for analyzer, params in zip(analyzer_list, params_list):\n",
    "  analyzer.setup(params)\n",
    "  analyzer.model.setup(analyzer.model_params)\n",
    "  analyzer.load_analysis(save_info=params.save_info)\n",
    "  analyzer.model_name = params.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iso-Response Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_angle = 20\n",
    "max_angle = 55\n",
    "\n",
    "num_neurons = 2 # How many neurons to plot\n",
    "\n",
    "for analyzer in analyzer_list:\n",
    "  analyzer.neuron_angles, analyzer.plot_matrix = analyzer.get_neuron_angles(analyzer.bf_stats)\n",
    "\n",
    "  vectors = np.argwhere(np.logical_and(analyzer.plot_matrix<max_angle, analyzer.plot_matrix>min_angle))\n",
    "  \n",
    "  analyzer.target_neuron_ids = []\n",
    "  analyzer.comparison_neuron_ids = []\n",
    "  analyzer.target_vectors = []\n",
    "  analyzer.comparison_vectors = []\n",
    "  analyzer.rand_orth_vectors = []\n",
    "  analyzer.comp_orth_vectors = []\n",
    "  for vector_id in range(num_neurons):\n",
    "    analyzer.target_neuron_ids.append(vectors[vector_id, 0])\n",
    "\n",
    "    target_vector = analyzer.bf_stats[\"basis_functions\"][analyzer.target_neuron_id]\n",
    "    target_vector = target_vector.reshape(analyzer.model_params.num_pixels)\n",
    "    target_vector_norm = np.linalg.norm(target_vector)\n",
    "    analyzer.target_vectors.append(target_vector / target_vector_norm)\n",
    "\n",
    "    analyzer.rand_orth_vectors.append(dp.get_rand_vectors(analyzer.target_vector, analyzer.model.num_pixels-1))\n",
    "    \n",
    "    sub_comparison_vectors = []\n",
    "    sub_comparison_orth_vectors = []\n",
    "    sub_comparison_neuron_ids = [index for index in range(analyzer.bf_stats[\"num_outputs\"]) if index != vector_id]\n",
    "    for comparison_neuron_id in sub_comparison_neuron_ids:\n",
    "      if(analyzer.analysis_params.model_type.lower() == \"lca\"): # TODO: any 1 layer model would be fine - check num layers or something\n",
    "        comparison_vector = analyzer.bf_stats[\"basis_functions\"][comparison_neuron_id]\n",
    "      else:\n",
    "        comparison_vector = analyzer.optimal_stims[comparison_neuron_id]\n",
    "      comparison_vector = comparison_vector.reshape(analyzer.model_params.num_pixels)\n",
    "      comparison_vector = comparison_vector / np.linalg.norm(comparison_vector)\n",
    "      sub_comparison_vectors.append(comparison_vector)\n",
    "      sub_comparison_orth_vectors.append(dp.get_alt_vectors(target_vector, comparison_vector)[0])\n",
    "    analyzer.comparison_neuron_ids.append(sub_comparison_neuron_ids)\n",
    "    analyzer.comparison_vectors.append(sub_comparison_vectors)\n",
    "    analyzer.comparison_orth_vectors.append(sub_comparison_orth_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_activity(analyzer, x_range, y_range, neuron_id_list, target_vect_list, comparison_vect_list, num_imgs):\n",
    "  # Construct point dataset\n",
    "  x_pts = np.linspace(x_range[0], x_range[1], int(np.sqrt(num_imgs)))\n",
    "  y_pts = np.linspace(y_range[0], y_range[1], int(np.sqrt(num_imgs)))\n",
    "  \n",
    "  X_mesh, Y_mesh = np.meshgrid(x_pts, y_pts)\n",
    "  proj_datapoints = np.stack([X_mesh.reshape(num_imgs), Y_mesh.reshape(num_imgs)], axis=1)\n",
    "\n",
    "  out_dict = {\n",
    "    \"norm_activity\": [],\n",
    "    \"proj_target_neuron\": [],\n",
    "    \"proj_comparison_neuron\": [],\n",
    "    \"proj_orth_vect\": [],\n",
    "    \"orth_vect\": [],\n",
    "    \"proj_datapoints\": proj_datapoints,\n",
    "    \"X_mesh\": X_mesh,\n",
    "    \"Y_mesh\": Y_mesh}\n",
    "\n",
    "  # TODO: This can be made to be much faster by compiling all of the stimulus into a single set and computing activations\n",
    "  for neuron_id, target_vect in zip(neuron_id_list, target_vect_list):\n",
    "    activity_sub_list = []\n",
    "    proj_target_neuron_sub_list = []\n",
    "    proj_comparison_neuron_sub_list = []\n",
    "    proj_orth_vect_sub_list = []\n",
    "    orth_vect_sub_list = []\n",
    "    for comparison_vect in comparison_vect_list:\n",
    "      proj_matrix, orth_vect = dp.bf_projections(target_vect, comparison_vect)\n",
    "      proj_target_neuron_sub_list.append(np.dot(proj_matrix, target_vect).T) #project\n",
    "      proj_comparison_neuron_sub_list.append(np.dot(proj_matrix, comparison_vect).T) #project\n",
    "      proj_orth_vect_sub_list.append(np.dot(proj_matrix, orth_vect).T) #project\n",
    "      orth_vect_sub_list.append(orth_vect)\n",
    "      datapoints = np.stack([np.dot(proj_matrix.T, proj_datapoints[data_id,:])\n",
    "        for data_id in range(num_imgs)], axis=0) #inject\n",
    "      datapoints = dp.reshape_data(datapoints, flatten=False)[0]\n",
    "      datapoints = {\"test\": Dataset(datapoints, lbls=None, ignore_lbls=None, rand_state=analyzer.rand_state)}\n",
    "      datapoints = analyzer.model.reshape_dataset(datapoints, analyzer.model_params)\n",
    "      activations = analyzer.compute_activations(datapoints[\"test\"].images)#, batch_size=int(np.sqrt(num_imgs)))\n",
    "      activations = activations[:, neuron_id]\n",
    "      activity_max = np.amax(np.abs(activations))\n",
    "      activations = activations / (activity_max + 0.00001)\n",
    "      activations = activations.reshape(int(np.sqrt(num_imgs)), int(np.sqrt(num_imgs)))\n",
    "      activity_sub_list.append(activations)\n",
    "    out_dict[\"norm_activity\"].append(activity_sub_list)\n",
    "    out_dict[\"proj_target_neuron\"].append(proj_target_neuron_sub_list)\n",
    "    out_dict[\"proj_comparison_neuron\"].append(proj_comparison_neuron_sub_list)\n",
    "    out_dict[\"proj_orth_vect\"].append(proj_orth_vect_sub_list)\n",
    "    out_dict[\"orth_vect\"].append(orth_vect_sub_list)\n",
    "  return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orientation Selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rads = []\n",
    "max_rads = []\n",
    "for analyzer in analyzer_list:\n",
    "  analyzer.bf_spatial_freq_rads = [np.sqrt(x**2+y**2) for (y,x) in analyzer.bf_stats[\"fourier_centers\"]]\n",
    "  min_rads.append(np.min(analyzer.bf_spatial_freq_rads))\n",
    "  max_rads.append(np.max(analyzer.bf_spatial_freq_rads))\n",
    "  \n",
    "num_bins = 10\n",
    "min_rad = np.min(min_rads)\n",
    "max_rad = np.max(max_rads)\n",
    "bins = np.linspace(min_rad, max_rad, num_bins)\n",
    "fig, ax = plt.subplots(1, figsize=figsize)\n",
    "hist_max = []\n",
    "for analyzer in analyzer_list:\n",
    "  hist, bin_edges = np.histogram(analyzer.bf_spatial_freq_rads, bins, density=True)\n",
    "  bin_left, bin_right = bin_edges[:-1], bin_edges[1:]\n",
    "  bin_centers = bin_left + (bin_right - bin_left)/2\n",
    "  \n",
    "  label = analyzer.model.params.model_type.upper() + \" \" + str(analyzer.model.get_num_latent())# + \" van Hateren\"\n",
    "  #label = re.sub(\"_\", \" \", analyzer.model_name)\n",
    "  ax.plot(bin_centers, hist, alpha=1.0, linestyle=\"-\", drawstyle=\"steps-mid\", label=label)\n",
    "  hist_max.append(np.max(hist))\n",
    "  \n",
    "ax.set_xticks(bin_left, minor=True)\n",
    "ax.set_xticks(bin_left[::4], minor=False)\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter(\"%0.0f\"))\n",
    "ax.tick_params(\"both\", labelsize=16)\n",
    "ax.set_xlim([min_rad, max_rad])\n",
    "ax.set_xticks([0, int(np.floor(max_rad/4)), int(2*np.floor(max_rad/4)),\n",
    "  int(3*np.floor(max_rad/4)), max_rad])\n",
    "ax.set_ylim([0, 0.6])\n",
    "ax.set_xlabel(\"Spatial Frequency\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"Normalized Density\", fontsize=fontsize)\n",
    "ax.set_title(\"Neuron Weight Spatial Frequency Histogram\", fontsize=fontsize)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legend = ax.legend(handles, labels, fontsize=fontsize, ncol=3,\n",
    "  borderaxespad=0., bbox_to_anchor=[0.01, 0.99], fancybox=True, loc=\"upper left\")\n",
    "for line in legend.get_lines():\n",
    "  line.set_linewidth(3)\n",
    "plt.show()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
