{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Computation Paper Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import tensorflow as tf\n",
    "from data.dataset import Dataset\n",
    "import data.data_selector as ds\n",
    "import analysis.analysis_picker as ap\n",
    "import utils.data_processing as dp\n",
    "import utils.plot_functions as pf\n",
    "import utils.neural_comp_funcs as nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (8, 8)\n",
    "fontsize = 16\n",
    "dpi = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lca_512_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_512_vh\"\n",
    "    self.display_name = \"Sparse Coding 512\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_768_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_768_vh\"\n",
    "    self.display_name = \"Sparse Coding 768\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_1024_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_1024_vh\"\n",
    "    self.display_name = \"Sparse Coding 1024\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_2560_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_2560_vh\"\n",
    "    self.display_name = \"Sparse Coding\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class sae_768_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"sae\"\n",
    "    self.model_name = \"sae_768_vh\"\n",
    "    self.display_name = \"Sparse Autoencoder\"\n",
    "    self.version = \"1.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class rica_768_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"rica\"\n",
    "    self.model_name = \"rica_768_vh\"\n",
    "    self.display_name = \"Linear Autoencoder\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class ae_768_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"ae\"\n",
    "    self.model_name = \"ae_768_vh\"\n",
    "    self.display_name = \"ReLU\"\n",
    "    self.version = \"1.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_768_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_768_mnist\"\n",
    "    self.display_name = \"Sparse Coding 768\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_1536_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_1536_mnist\"\n",
    "    self.display_name = \"Sparse Coding 1536\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class ae_768_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"ae\"\n",
    "    self.model_name = \"ae_768_mnist\"\n",
    "    self.display_name = \"Leaky ReLU\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class sae_768_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"sae\"\n",
    "    self.model_name = \"sae_768_mnist\"\n",
    "    self.display_name = \"Sparse Autoencoder\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class rica_768_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"rica\"\n",
    "    self.model_name = \"rica_768_mnist\"\n",
    "    self.display_name = \"Linear Autoencoder\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class ae_deep_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"ae\"\n",
    "    self.model_name = \"ae_deep_mnist\"\n",
    "    self.display_name = \"Leaky ReLU\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_vals = dict(zip([\"lt_green\", \"md_green\", \"dk_green\", \"lt_blue\", \"md_blue\", \"dk_blue\", \"lt_red\", \"md_red\", \"dk_red\"],\n",
    "  [\"#A9DFBF\", \"#196F3D\", \"#27AE60\", \"#AED6F1\", \"#3498DB\", \"#21618C\", \"#F5B7B1\", \"#E74C3C\", \"#943126\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iso-contour activations comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params_list = [lca_512_vh_params()]#, lca_768_vh_params(), lca_1024_vh_params()]\n",
    "params_list = [rica_768_vh_params(), ae_768_vh_params(), sae_768_vh_params(), lca_2560_vh_params()]\n",
    "#params_list = [rica_768_mnist_params(), ae_768_mnist_params(), sae_768_mnist_params(), lca_1536_mnist_params()]\n",
    "for params in params_list:\n",
    "  params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+params.model_name)\n",
    "analyzer_list = [ap.get_analyzer(params.model_type) for params in params_list]\n",
    "for analyzer, params in zip(analyzer_list, params_list):\n",
    "  analyzer.setup(params)\n",
    "  analyzer.model.setup(analyzer.model_params)\n",
    "  analyzer.load_analysis(save_info=params.save_info)\n",
    "  analyzer.model_name = params.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analyzer in analyzer_list:\n",
    "  run_params = np.load(analyzer.analysis_out_dir+\"savefiles/iso_params_\"\n",
    "    +analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()\n",
    "  min_angle = run_params[\"min_angle\"]\n",
    "  max_angle = run_params[\"max_angle\"]\n",
    "  num_neurons = run_params[\"num_neurons\"]\n",
    "  use_bf_stats = run_params[\"use_bf_stats\"]\n",
    "  num_comparison_vectors = run_params[\"num_comparison_vects\"]\n",
    "  x_range = run_params[\"x_range\"]\n",
    "  y_range = run_params[\"y_range\"]\n",
    "  num_images = run_params[\"num_images\"]\n",
    "\n",
    "  iso_vectors = np.load(analyzer.analysis_out_dir+\"savefiles/iso_vectors_\"\n",
    "    +analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()\n",
    "  analyzer.target_neuron_ids = iso_vectors[\"target_neuron_ids\"]\n",
    "  analyzer.comparison_neuron_ids = iso_vectors[\"comparison_neuron_ids\"]\n",
    "  analyzer.target_vectors = iso_vectors[\"target_vectors\"]\n",
    "  analyzer.rand_orth_vectors = iso_vectors[\"rand_orth_vectors\"]\n",
    "  analyzer.comparison_vectors = iso_vectors[\"comparison_vectors\"]\n",
    "\n",
    "  analyzer.comp_activations = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_activations_\"\n",
    "    +analyzer.analysis_params.save_info+\".npz\")[\"data\"]\n",
    "  analyzer.comp_contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_contour_dataset_\"\n",
    "    +analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()\n",
    "  analyzer.rand_activations = np.load(analyzer.analysis_out_dir+\"savefiles/iso_rand_activations_\"\n",
    "    +analyzer.analysis_params.save_info+\".npz\")[\"data\"]\n",
    "  analyzer.rand_contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_rand_contour_dataset_\"\n",
    "    +analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_goup_iso_contours(analyzer_list, neuron_indices, orth_indices, num_levels, x_range, y_range, figsize=None, dpi=100, fontsize=12):\n",
    "  num_models = len(analyzer_list)\n",
    "  num_plots_y = np.int32(np.ceil(np.sqrt(num_models)))\n",
    "  num_plots_x = np.int32(np.ceil(np.sqrt(num_models)))\n",
    "  gs0 = gridspec.GridSpec(num_plots_y, num_plots_x, wspace=-0.1)\n",
    "  vmin = np.min([np.min(analyzer.comp_activations) for analyzer in analyzer_list])\n",
    "  vmax = np.max([np.max(analyzer.comp_activations) for analyzer in analyzer_list])\n",
    "  levels = np.linspace(vmin, vmax, num_levels)\n",
    "  cmap = plt.get_cmap(\"cividis\")\n",
    "  cNorm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "  scalarMap = matplotlib.cm.ScalarMappable(norm=cNorm, cmap=cmap)\n",
    "  fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "  contour_handles = []\n",
    "  curve_axes = []\n",
    "  analyzer_index = 0\n",
    "  for plot_id in np.ndindex((num_plots_y, num_plots_x)):\n",
    "    (y_id, x_id) = plot_id\n",
    "    if type(neuron_indices) == list:\n",
    "      analyzer_neuron_index = neuron_indices[analyzer_index]\n",
    "    else:\n",
    "      analyzer_neuron_index = neuron_indices\n",
    "    if type(orth_indices) == list:\n",
    "      analyzer_orth_index = orth_indices[analyzer_index]\n",
    "    else:\n",
    "      analyzer_orth_index = orth_indices\n",
    "    analyzer = analyzer_list[analyzer_index]\n",
    "    inner_gs = gridspec.GridSpecFromSubplotSpec(1, 1, gs0[plot_id])\n",
    "    curve_axes.append(pf.clear_axis(fig.add_subplot(inner_gs[0])))\n",
    "    curve_axes[-1].set_title(analyzer.analysis_params.display_name, fontsize=fontsize)\n",
    "    # plot colored mesh points\n",
    "    norm_activity = analyzer.comp_activations[analyzer_neuron_index, analyzer_orth_index, ...]\n",
    "    x_mesh, y_mesh = np.meshgrid(analyzer.comp_contour_dataset[\"x_pts\"], analyzer.comp_contour_dataset[\"y_pts\"])\n",
    "    contsf = curve_axes[-1].contourf(x_mesh, y_mesh, norm_activity,\n",
    "      levels=levels, vmin=vmin, vmax=vmax, alpha=1.0, antialiased=True, cmap=cmap)\n",
    "    contour_handles.append(contsf)\n",
    "    # plot target neuron arrow & label\n",
    "    proj_target = analyzer.comp_contour_dataset[\"proj_target_neuron\"][analyzer_neuron_index][analyzer_orth_index]\n",
    "    target_vector_x = proj_target[0].item()\n",
    "    target_vector_y = proj_target[1].item()\n",
    "    curve_axes[-1].arrow(0, 0, target_vector_x, target_vector_y,\n",
    "      width=0.00, head_width=0.15, head_length=0.15, fc='k', ec='k',\n",
    "      linestyle='-', linewidth=3)\n",
    "    tenth_range_shift = ((max(x_range) - min(x_range))/10) # For shifting labels\n",
    "    text_handle = curve_axes[-1].text(target_vector_x+(tenth_range_shift*0.3), target_vector_y+(tenth_range_shift*0.7),\n",
    "      r\"$\\Phi_{k}$\", fontsize=fontsize, horizontalalignment='center', verticalalignment='center')\n",
    "    # plot comparison neuron arrow & label \n",
    "    proj_comparison = analyzer.comp_contour_dataset[\"proj_comparison_neuron\"][analyzer_neuron_index][analyzer_orth_index]\n",
    "    comparison_vector_x = proj_comparison[0].item()\n",
    "    comparison_vector_y = proj_comparison[1].item()\n",
    "    curve_axes[-1].arrow(0, 0, comparison_vector_x, comparison_vector_y,\n",
    "      width=0.00, head_width=0.15, head_length=0.15, fc='k', ec='k',\n",
    "      linestyle=\"-\", linewidth=3)\n",
    "    text_handle = curve_axes[-1].text(comparison_vector_x+(tenth_range_shift*0.3), comparison_vector_y+(tenth_range_shift*0.7),\n",
    "      r\"$\\Phi_{j}$\", fontsize=fontsize, horizontalalignment='center', verticalalignment='center')\n",
    "    # Plot all other comparison neurons TODO: add flag to optionally do this\n",
    "    #for proj_alt in analyzer.comp_contour_dataset[\"proj_comparison_neuron\"][analyzer_neuron_index]:\n",
    "    #  if not np.all(proj_alt == proj_comparison):\n",
    "    #    curve_axes[-1].arrow(0, 0, proj_alt[0].item(), proj_alt[1].item(),\n",
    "    #      width=0.00, head_width=0.15, head_length=0.15, fc='w', ec='w',\n",
    "    #      linestyle=\"dashed\", linewidth=1.0, alpha=0.9)\n",
    "    # Plot orthogonal vector Nu\n",
    "    proj_orth = analyzer.comp_contour_dataset[\"proj_orth_vect\"][analyzer_neuron_index][analyzer_orth_index]\n",
    "    orth_vector_x = proj_orth[0].item()\n",
    "    orth_vector_y = proj_orth[1].item()\n",
    "    curve_axes[-1].arrow(0, 0, orth_vector_x, orth_vector_y,\n",
    "      width=0.00, head_width=0.10, head_length=0.10, fc='k', ec='k',\n",
    "      linestyle=\"-\", linewidth=3)\n",
    "    text_handle = curve_axes[-1].text(orth_vector_x+(tenth_range_shift*0.3), orth_vector_y+(tenth_range_shift*0.7),\n",
    "      r\"$\\nu$\", fontsize=fontsize, horizontalalignment='center', verticalalignment='center')\n",
    "    # Plot axes\n",
    "    curve_axes[-1].set_aspect(\"equal\")\n",
    "    curve_axes[-1].plot(x_range, [0,0], color='k')\n",
    "    curve_axes[-1].plot([0,0], y_range, color='k')\n",
    "    # Include basis function image - note, need to change number of plots for inner_gs for this code\n",
    "    #gs2 = gridspec.GridSpecFromSubplotSpec(2, 1, inner_gs[1], wspace=0.0, hspace=0.5)#-0.55)\n",
    "    #target_vect_ax = pf.clear_axis(fig.add_subplot(gs2[0]))\n",
    "    #target_vect_ax.imshow(analyzer.bf_stats[\"basis_functions\"][analyzer.target_neuron_ids[0]], cmap=\"Greys_r\")\n",
    "    #target_vect_ax.set_title(\"Primary\\nBasis Function\", color='r', fontsize=16)\n",
    "    #comparison_vect_ax = pf.clear_axis(fig.add_subplot(gs2[1]))\n",
    "    #comparison_vect_ax.imshow(analyzer.bf_stats[\"basis_functions\"][analyzer.comparison_neuron_ids[0][0]], cmap=\"Greys_r\")\n",
    "    #comparison_vect_ax.set_title(\"Comparison\\nBasis Function\", color='k', fontsize=16)\n",
    "    analyzer_index += 1\n",
    "  # Add colorbar\n",
    "  scalarMap._A = []\n",
    "  cbar_ax = inset_axes(curve_axes[1],\n",
    "    width=\"5%\",\n",
    "    height=\"100%\",\n",
    "    loc='lower left',\n",
    "    bbox_to_anchor=(1.05, 0., 1, 1),\n",
    "    bbox_transform=curve_axes[1].transAxes,\n",
    "    borderpad=0,\n",
    "    )\n",
    "  cbar = fig.colorbar(scalarMap, cax=cbar_ax, ticks=[vmin, vmax])\n",
    "  cbar.ax.tick_params(labelleft=False, labelright=True, left=False, right=True, labelsize=fontsize)\n",
    "  cbar.ax.set_yticklabels([\"{:.0f}\".format(vmin), \"{:.0f}\".format(vmax)])\n",
    "  plt.show()\n",
    "  return fig, contour_handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "neuron_indices = [0, 0, 0, 0]\n",
    "orth_indices = [0, 0, 0, 1]\n",
    "num_plots_y = 2\n",
    "num_plots_x = 2\n",
    "num_levels = 10\n",
    "\n",
    "fig, contour_handles = plot_goup_iso_contours(analyzer_list, neuron_indices, orth_indices, num_levels, x_range, y_range,\n",
    "  figsize, dpi, fontsize)\n",
    "for analyzer, neuron_index, orth_index in zip(analyzer_list, neuron_indices, orth_indices):\n",
    "  for ext in [\".png\", \".eps\"]:\n",
    "    neuron_str = str(analyzer.target_neuron_ids[neuron_index])\n",
    "    orth_str = str(analyzer.comparison_neuron_ids[neuron_index][orth_index])\n",
    "    save_name = (analyzer.analysis_out_dir+\"/vis/iso_contour_comparison_bf0id\"+neuron_str+\"_bf1id\"+orth_str\n",
    "      +\"_\"+analyzer.analysis_params.save_info+ext)\n",
    "    fig.savefig(save_name, dpi=dpi, transparent=True, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = np.min([np.min(analyzer.comp_activations) for analyzer in analyzer_list])\n",
    "vmax = np.max([np.max(analyzer.comp_activations) for analyzer in analyzer_list])\n",
    "levels = np.linspace(vmin, vmax, num_levels)\n",
    "analyzer = analyzer_list[-1]\n",
    "(num_neurons, num_planes, num_points_y, num_points_x) = analyzer.comp_activations.shape\n",
    "curvatures = []\n",
    "for neuron_id in range(num_neurons):\n",
    "  sub_curvatures = []\n",
    "  for plane_id in range(num_planes):\n",
    "    activity = analyzer.comp_activations[neuron_id, plane_id, ...]\n",
    "    (y_vals, x_vals) = np.nonzero(np.logical_and(activity<levels[-2], activity>levels[-3]))\n",
    "    coeffs = np.polynomial.polynomial.polyfit(y_vals, activity[y_vals, x_vals], deg=2)\n",
    "    sub_curvatures.append(coeffs[-1])\n",
    "  curvatures.append(sub_curvatures)\n",
    "    \n",
    "num_bins = 50\n",
    "comp_curvatures = []\n",
    "for neuron_index in range(num_neurons):\n",
    "  comp_curvatures += curvatures[neuron_index]\n",
    "all_curvatures = comp_curvatures\n",
    "max_curvature = np.amax(all_curvatures)\n",
    "min_curvature = np.amin(all_curvatures)\n",
    "bin_width = (max_curvature - min_curvature) / (num_bins-1) # subtract 1 to leave room for the zero bin\n",
    "bin_centers = [0.0]\n",
    "while min(bin_centers) > min_curvature:\n",
    "  bin_centers.append(bin_centers[-1]-bin_width)\n",
    "bin_centers = bin_centers[::-1]\n",
    "while max(bin_centers) < max_curvature:\n",
    "  bin_centers.append(bin_centers[-1]+bin_width)\n",
    "bin_lefts = bin_centers - (bin_width / 2)\n",
    "bin_rights = bin_centers + (bin_width / 2)\n",
    "bins = bin_lefts+bin_rights[-1]\n",
    "\n",
    "comp_curvatures = [item for sub_list in curvatures for item in sub_list]\n",
    "comp_hist, bin_edges = np.histogram(comp_curvatures, bins, density=False)\n",
    "comp_hist = comp_hist / np.sum(comp_hist)\n",
    "\n",
    "hist_list = [[comp_hist], [comp_hist]]\n",
    "label_list = [[\"Basis vector, 2x\"],\n",
    "  [\"Basis vector, 2x\"]]\n",
    "color_list = [[color_vals[\"dk_red\"]],\n",
    "  [color_vals[\"dk_blue\"]]]\n",
    "\n",
    "plot_bin_lefts, plot_bin_rights = bin_edges[:-1], bin_edges[1:]\n",
    "plot_bin_centers = plot_bin_lefts + (plot_bin_rights - plot_bin_lefts)\n",
    "\n",
    "curvature_fig = plot_curvature_histograms(hist_list, label_list, color_list, plot_bin_centers,\n",
    "  figsize=(1.5*figsize[0], figsize[1]), dpi=dpi, fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curvature histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = [lca_512_vh_params(), lca_768_vh_params(), lca_1024_vh_params(), lca_2560_vh_params()]\n",
    "#params_list = [lca_768_mnist_params(), lca_1536_mnist_params()]\n",
    "\n",
    "for params in params_list:\n",
    "  params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+params.model_name)\n",
    "analyzer_list = [ap.get_analyzer(params.model_type) for params in params_list]\n",
    "for analyzer, params in zip(analyzer_list, params_list):\n",
    "  analyzer.setup(params)\n",
    "  analyzer.model.setup(analyzer.model_params)\n",
    "  analyzer.load_analysis(save_info=params.save_info)\n",
    "  analyzer.model_name = params.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analyzer in analyzer_list:\n",
    "  run_params = np.load(analyzer.analysis_out_dir+\"savefiles/iso_params_1d_\"+analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()\n",
    "  analyzer.comp_activations = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_activations_1d_\"+analyzer.analysis_params.save_info+\".npz\")[\"data\"]\n",
    "  analyzer.comp_contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_contour_dataset_1d_\"+analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()\n",
    "  analyzer.rand_activations = np.load(analyzer.analysis_out_dir+\"savefiles/iso_rand_activations_1d_\"+analyzer.analysis_params.save_info+\".npz\")[\"data\"]\n",
    "  analyzer.rand_contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_rand_contour_dataset_1d_\"+analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()\n",
    "  #iso_vectors = np.load(analyzer.analysis_out_dir+\"savefiles/iso_vectors_1d_\"+analyzer.analysis_params.save_info+\".npz\")[\"data\"].item()\n",
    "  \n",
    "  #analyzer.min_angle = run_params[\"min_angle\"]\n",
    "  #analyzer.max_angle = run_params[\"max_angle\"]\n",
    "  analyzer.num_target_neurons = run_params[\"num_neurons\"]\n",
    "  #analyzer.use_bf_stats = run_params[\"use_bf_stats\"]\n",
    "  analyzer.num_comparison_vectors = run_params[\"num_comparison_vects\"]\n",
    "  #analyzer.x_range = run_params[\"x_range\"]\n",
    "  #analyzer.y_range = run_params[\"y_range\"]\n",
    "  #analyzer.num_images = run_params[\"num_images\"]\n",
    "  #analyzer.target_neuron_ids = iso_vectors[\"target_neuron_ids\"]\n",
    "  #analyzer.comparison_neuron_ids = iso_vectors[\"comparison_neuron_ids\"]\n",
    "  #analyzer.target_vectors = iso_vectors[\"target_vectors\"]\n",
    "  #analyzer.rand_orth_vectors = iso_vectors[\"rand_orth_vectors\"]\n",
    "  #analyzer.comparison_vectors = iso_vectors[\"comparison_vectors\"]\n",
    "  \n",
    "  #analyzer.num_neurons = analyzer.bf_stats[\"num_outputs\"]\n",
    "  #analyzer.num_pixels = analyzer.bf_stats[\"num_inputs\"]\n",
    "  \n",
    "  comp_x_pts = analyzer.comp_contour_dataset[\"x_pts\"]\n",
    "  rand_x_pts = analyzer.rand_contour_dataset[\"x_pts\"]\n",
    "  assert(np.all(comp_x_pts == rand_x_pts)) # This makes sure we don't need to recompute proj_datapoints for each case\n",
    "  num_x_imgs = len(comp_x_pts)\n",
    "  x_target = comp_x_pts[num_x_imgs-1] # find a location to take a slice\n",
    "  proj_datapoints = analyzer.comp_contour_dataset[\"proj_datapoints\"]\n",
    "  slice_indices = np.where(proj_datapoints[:, 0] == x_target)[0]\n",
    "  analyzer.sliced_datapoints = proj_datapoints[slice_indices, :][:, :] # slice grid\n",
    "\n",
    "  analyzer.comp_curvatures = []\n",
    "  analyzer.comp_fits = []\n",
    "  analyzer.comp_sliced_activity = []\n",
    "  analyzer.comp_delta_activity = []\n",
    "  analyzer.rand_curvatures = []\n",
    "  analyzer.rand_fits = []\n",
    "  analyzer.rand_sliced_activity = []\n",
    "  analyzer.rand_delta_activity = []\n",
    "  for neuron_index in range(analyzer.num_target_neurons):\n",
    "    sub_comp_curvatures = []\n",
    "    sub_comp_fits = []\n",
    "    sub_comp_sliced_activity = []\n",
    "    sub_comp_delta_activity = []\n",
    "    sub_rand_curvatures = []\n",
    "    sub_rand_fits = []\n",
    "    sub_rand_sliced_activity = []\n",
    "    sub_rand_delta_activity = []\n",
    "    for orth_index in range(analyzer.num_comparison_vectors):\n",
    "      comp_activity = analyzer.comp_activations[neuron_index, orth_index, ...].reshape([-1])\n",
    "      sub_comp_sliced_activity.append(comp_activity[slice_indices][:])\n",
    "      coeff = np.polynomial.polynomial.polyfit(analyzer.sliced_datapoints[:, 1],\n",
    "        sub_comp_sliced_activity[-1], deg=2) # [c0, c1, c2], where p = c0 + c1x + c2x^2\n",
    "      sub_comp_curvatures.append(coeff[2])\n",
    "      sub_comp_fits.append(np.polynomial.polynomial.polyval(analyzer.sliced_datapoints[:, 1], coeff))\n",
    "      delta_activity = sub_comp_sliced_activity[-1][-1] - sub_comp_sliced_activity[-1][0]\n",
    "      sub_comp_delta_activity.append(delta_activity)\n",
    "      \n",
    "    num_rand_vectors = np.minimum(analyzer.bf_stats[\"num_inputs\"]-1, analyzer.num_comparison_vectors)\n",
    "    for orth_index in range(num_rand_vectors):\n",
    "      rand_activity = analyzer.rand_activations[neuron_index, orth_index, ...].reshape([-1])\n",
    "      sub_rand_sliced_activity.append(rand_activity[slice_indices][:])\n",
    "      coeff = np.polynomial.polynomial.polyfit(analyzer.sliced_datapoints[:, 1],\n",
    "        sub_rand_sliced_activity[-1], deg=2)\n",
    "      sub_rand_curvatures.append(coeff[2])\n",
    "      sub_rand_fits.append(np.polynomial.polynomial.polyval(analyzer.sliced_datapoints[:, 1], coeff))\n",
    "      delta_activity = sub_rand_sliced_activity[-1][-1] - sub_rand_sliced_activity[-1][0]\n",
    "      sub_rand_delta_activity.append(delta_activity)\n",
    "\n",
    "    analyzer.comp_curvatures.append(sub_comp_curvatures)\n",
    "    analyzer.comp_fits.append(sub_comp_fits)\n",
    "    analyzer.comp_sliced_activity.append(sub_comp_sliced_activity)\n",
    "    analyzer.comp_delta_activity.append(sub_comp_delta_activity)\n",
    "    analyzer.rand_curvatures.append(sub_rand_curvatures)\n",
    "    analyzer.rand_fits.append(sub_rand_fits)\n",
    "    analyzer.rand_sliced_activity.append(sub_rand_sliced_activity)\n",
    "    analyzer.rand_delta_activity.append(sub_rand_delta_activity)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "line_alpha = 0.08\n",
    "for analyzer in analyzer_list:\n",
    "  target_neuron_index = 0\n",
    "  fig = nc.plot_bf_curvature(analyzer, target_neuron_index, line_alpha=line_alpha,\n",
    "    figsize=(figsize[0], 2*figsize[1]), dpi=dpi, fontsize=fontsize)\n",
    "  neuron_str = str(analyzer.target_neuron_ids[target_neuron_index])\n",
    "  save_name = (analyzer.analysis_out_dir+\"/vis/bf_curvatures_bfid0\"+neuron_str\n",
    "    +\"_\"+analyzer.analysis_params.save_info+\".png\")\n",
    "  fig.savefig(save_name, transparent=True, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for analyzer in analyzer_list:\n",
    "  target_neuron_index = 0\n",
    "  fig = nc.plot_fit_curvature(analyzer, target_neuron_index=0, line_alpha=line_alpha,\n",
    "    figsize=(figsize[0], 2*figsize[1]), dpi=dpi, fontsize=fontsize)\n",
    "  neuron_str = str(analyzer.target_neuron_ids[target_neuron_index])\n",
    "  save_name = (analyzer.analysis_out_dir+\"/vis/bf_fit_curvatures_bfid0\"+neuron_str\n",
    "    +\"_\"+analyzer.analysis_params.save_info+\".png\")\n",
    "  fig.savefig(save_name, transparent=True, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curvature_histograms(hist_list, label_list, color_list, bin_centers, figsize=None, dpi=100, fontsize=12):\n",
    "  num_y_plots = len(hist_list)\n",
    "  fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "  gs0 = gridspec.GridSpec(num_y_plots, 1, hspace=0.05)\n",
    "  axes = []\n",
    "  axes.append(fig.add_subplot(gs0[0]))\n",
    "  for plt_idx in range(1, num_y_plots):\n",
    "    axes.append(fig.add_subplot(gs0[plt_idx], sharey=axes[0]))\n",
    "  handles = []\n",
    "  labels = []\n",
    "  max_val = 0\n",
    "  for axis_index, (axis_hists, axis_colors, axis_labels) in enumerate(zip(hist_list, color_list, label_list)):\n",
    "    for hist, color, label in zip(axis_hists, axis_colors, axis_labels):\n",
    "      axes[axis_index].plot(bin_centers, hist, color=color, linestyle=\"-\", drawstyle=\"steps-mid\", label=label)\n",
    "      axes[axis_index].set_yscale('log')\n",
    "      if np.max(hist) > max_val:\n",
    "        max_val = np.max(hist)\n",
    "    axes[axis_index].axvline(0.0, color='k', linestyle='dashed', linewidth=1)\n",
    "    for tick in axes[axis_index].xaxis.get_major_ticks():\n",
    "      tick.label.set_fontsize(fontsize) \n",
    "    for tick in axes[axis_index].yaxis.get_major_ticks():\n",
    "      tick.label.set_fontsize(fontsize) \n",
    "    axes[axis_index].set_ylabel(\"Normalized Count\", fontsize=fontsize)\n",
    "    ax_handles, ax_labels = axes[axis_index].get_legend_handles_labels()\n",
    "    handles += ax_handles\n",
    "    labels += ax_labels\n",
    "\n",
    "  #axes[0].set_ylim([0.0, max_val+1])\n",
    "  #for axis_index in range(num_y_plots):\n",
    "  #  ticks = range(0, int(max_val), int(max_val/4))\n",
    "  #  axes[axis_index].set_yticks(ticks, minor=False)\n",
    "\n",
    "  axes[0].set_xticks(bin_centers, minor=True)\n",
    "  axes[0].set_xticks([], minor=False)\n",
    "  axes[-1].set_xticks(bin_centers, minor=True)\n",
    "  axes[-1].set_xticks(bin_centers[::int(len(bin_centers)/5)], minor=False)\n",
    "  axes[-1].xaxis.set_major_formatter(FormatStrFormatter(\"%0.3f\"))\n",
    "\n",
    "  axes[0].set_title(\"Histogram of Response Attenuation\", fontsize=fontsize)\n",
    "  axes[-1].set_xlabel(\"Curvature of Response to Orthogonal Perturbations\", fontsize=fontsize)\n",
    "\n",
    "  legend = axes[1].legend(handles=handles, labels=labels, fontsize=fontsize,\n",
    "    borderaxespad=0., bbox_to_anchor=[0.08, 0.30], framealpha=0.0)\n",
    "  legend.get_frame().set_linewidth(0.0)\n",
    "  for text, color in zip(legend.get_texts(), [color for sublist in color_list for color in sublist]):\n",
    "    text.set_color(color)\n",
    "  for item in legend.legendHandles:\n",
    "    item.set_visible(False)\n",
    "  plt.show()\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 50\n",
    "\n",
    "comp_curvatures = []\n",
    "rand_curvatures = []\n",
    "for analyzer in analyzer_list:\n",
    "  for neuron_index in range(analyzer.num_target_neurons):\n",
    "    comp_curvatures += analyzer.comp_curvatures[neuron_index]\n",
    "    rand_curvatures += analyzer.rand_curvatures[neuron_index]\n",
    "    #comp_curvatures += analyzer.comp_delta_activity[neuron_index]\n",
    "    #rand_curvatures += analyzer.rand_delta_activity[neuron_index]\n",
    "all_curvatures = comp_curvatures + rand_curvatures\n",
    "max_curvature = np.amax(all_curvatures)\n",
    "min_curvature = np.amin(all_curvatures)\n",
    "bin_width = (max_curvature - min_curvature) / (num_bins-1) # subtract 1 to leave room for the zero bin\n",
    "bin_centers = [0.0]\n",
    "while min(bin_centers) > min_curvature:\n",
    "  bin_centers.append(bin_centers[-1]-bin_width)\n",
    "bin_centers = bin_centers[::-1]\n",
    "while max(bin_centers) < max_curvature:\n",
    "  bin_centers.append(bin_centers[-1]+bin_width)\n",
    "bin_lefts = bin_centers - (bin_width / 2)\n",
    "bin_rights = bin_centers + (bin_width / 2)\n",
    "bins = bin_lefts+bin_rights[-1]\n",
    "\n",
    "# MNIST\n",
    "#comp_hist_768, bin_edges = np.histogram(analyzer_list[0].comp_curvatures, bins, density=True)\n",
    "#rand_hist_768, _ = np.histogram(analyzer_list[0].rand_curvatures, bins, density=True)\n",
    "#comp_hist_1536, _ = np.histogram(analyzer_list[1].comp_curvatures, bins, density=True)\n",
    "#rand_hist_1536, _ = np.histogram(analyzer_list[1].rand_curvatures, bins, density=True)\n",
    "\n",
    "#hist_list = [[comp_hist_768, comp_hist_1536], [rand_hist_768, rand_hist_1536]]\n",
    "#label_list = [[\"Basis vector, complete\", \"Basis vector, overcomplete\"], [\"Random vector, complete\", \"Random vector, overcomplete\"]]\n",
    "##color_list = [[\"#E74C3C\", \"#943126\"], [\"#3498DB\", \"#21618C\"]]\n",
    "#color_list = [[color_vals[\"md_red\"], color_vals[\"dk_red\"]], [color_vals[\"md_blue\"], color_vals[\"dk_blue\"]]]\n",
    "\n",
    "# van Hateren\n",
    "comp_curvatures_512 = [item for sub_list in analyzer_list[0].comp_curvatures for item in sub_list]\n",
    "rand_curvatures_512 = [item for sub_list in analyzer_list[0].rand_curvatures for item in sub_list]\n",
    "comp_hist_512, bin_edges = np.histogram(comp_curvatures_512, bins, density=False)\n",
    "comp_hist_512 = comp_hist_512 / np.sum(comp_hist_512)\n",
    "rand_hist_512, _ = np.histogram(analyzer_list[0].rand_curvatures, bins)\n",
    "rand_hist_512 = rand_hist_512 / np.sum(rand_hist_512)\n",
    "\n",
    "comp_curvatures_768 = [item for sub_list in analyzer_list[1].comp_curvatures for item in sub_list]\n",
    "rand_curvatures_768 = [item for sub_list in analyzer_list[1].rand_curvatures for item in sub_list]\n",
    "comp_hist_768, _ = np.histogram(comp_curvatures_768, bins)\n",
    "comp_hist_768 = comp_hist_768 / np.sum(comp_hist_768)\n",
    "rand_hist_768, _ = np.histogram(rand_curvatures_768, bins)\n",
    "rand_hist_768 = rand_hist_768 / np.sum(rand_hist_768)\n",
    "\n",
    "comp_curvatures_1024 = [item for sub_list in analyzer_list[2].comp_curvatures for item in sub_list]\n",
    "rand_curvatures_1024 = [item for sub_list in analyzer_list[2].rand_curvatures for item in sub_list]\n",
    "comp_hist_1024, _ = np.histogram(comp_curvatures_1024, bins)\n",
    "comp_hist_1024 = comp_hist_1024 / np.sum(comp_hist_1024)\n",
    "rand_hist_1024, _ = np.histogram(rand_curvatures_1024, bins)\n",
    "rand_hist_1024 = rand_hist_1024 / np.sum(rand_hist_1024)\n",
    "\n",
    "comp_curvatures_2560 = [item for sub_list in analyzer_list[3].comp_curvatures for item in sub_list]\n",
    "rand_curvatures_2560 = [item for sub_list in analyzer_list[3].rand_curvatures for item in sub_list]\n",
    "comp_hist_2560, _ = np.histogram(comp_curvatures_2560, bins)\n",
    "comp_hist_2560 = comp_hist_2560 / np.sum(comp_hist_2560)\n",
    "rand_hist_2560, _ = np.histogram(rand_curvatures_2560, bins)\n",
    "rand_hist_2560 = rand_hist_2560 / np.sum(rand_hist_2560)\n",
    "\n",
    "hist_list = [[comp_hist_512, comp_hist_1024, comp_hist_2560], [rand_hist_512, rand_hist_1024, rand_hist_2560]]\n",
    "label_list = [[\"Basis vector, 2x\", \"Basis vector, 4x\", \"Basis vector, 10x\"],\n",
    "  [\"Random vector, 2x\", \"Random vector, 4x\", \"Random vector, 10x\"]]\n",
    "color_list = [[color_vals[\"lt_red\"], color_vals[\"md_red\"], color_vals[\"dk_red\"]],\n",
    "  [color_vals[\"lt_blue\"], color_vals[\"md_blue\"], color_vals[\"dk_blue\"]]]\n",
    "\n",
    "#hist_list = [[comp_hist_512, comp_hist_768, comp_hist_1024], [rand_hist_512, rand_hist_768, rand_hist_1024]]\n",
    "#label_list = [[\"Basis vector, 2x\", \"Basis vector, 3x\", \"Basis vector, 4x\"],\n",
    "#  [\"Random vector, 2x\", \"Random vector, 3x\", \"Random vector, 4x\"]]\n",
    "#color_list = [[color_vals[\"lt_red\"], color_vals[\"md_red\"], color_vals[\"dk_red\"]],\n",
    "#  [color_vals[\"lt_blue\"], color_vals[\"md_blue\"], color_vals[\"dk_blue\"]]]\n",
    "\n",
    "plot_bin_lefts, plot_bin_rights = bin_edges[:-1], bin_edges[1:]\n",
    "plot_bin_centers = plot_bin_lefts + (plot_bin_rights - plot_bin_lefts)\n",
    "\n",
    "curvature_fig = plot_curvature_histograms(hist_list, label_list, color_list, plot_bin_centers,\n",
    "  figsize=(1.5*figsize[0], figsize[1]), dpi=dpi, fontsize=fontsize)\n",
    "for analyzer in analyzer_list:\n",
    "  for ext in [\".png\", \".eps\"]:\n",
    "    save_name = (analyzer.analysis_out_dir+\"/vis/histogram_of_response_attenuation\"\n",
    "      +\"_\"+analyzer.analysis_params.save_info+ext)\n",
    "    curvature_fig.savefig(save_name, transparent=False, bbox_inches=\"tight\", pad_inches=0.01, dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orientation Selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = [rica_768_vh_params(), sae_768_vh_params(), lca_768_vh_params()]\n",
    "params_list[-1].display_name = \"Sparse Coding\"\n",
    "for params in params_list:\n",
    "  params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+params.model_name)\n",
    "analyzer_list = [ap.get_analyzer(params.model_type) for params in params_list]\n",
    "for analyzer, params in zip(analyzer_list, params_list):\n",
    "  analyzer.setup(params)\n",
    "  analyzer.model.setup(analyzer.model_params)\n",
    "  analyzer.load_analysis(save_info=params.save_info)\n",
    "  analyzer.model_name = params.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contrast_orientation_tuning(bf_indices, contrasts, orientations, activations, figsize=(32,32)):\n",
    "  \"\"\"\n",
    "  Generate contrast orientation tuning curves. Every subplot will have curves for each contrast.\n",
    "  Inputs:\n",
    "    bf_indices: [list or array] of neuron indices to use\n",
    "      all indices should be less than activations.shape[0]\n",
    "    contrasts: [list or array] of contrasts to use\n",
    "    orientations: [list or array] of orientations to use\n",
    "  \"\"\"\n",
    "  orientations = np.asarray(orientations)*(180/np.pi) #convert to degrees for plotting\n",
    "  num_bfs = np.asarray(bf_indices).size\n",
    "  cmap = plt.get_cmap('Greys')\n",
    "  cNorm = matplotlib.colors.Normalize(vmin=0.0, vmax=1.0)\n",
    "  scalarMap = matplotlib.cm.ScalarMappable(norm=cNorm, cmap=cmap)\n",
    "  fig = plt.figure(figsize=figsize)\n",
    "  num_plots_y = np.int32(np.ceil(np.sqrt(num_bfs)))+1\n",
    "  num_plots_x = np.int32(np.ceil(np.sqrt(num_bfs)))\n",
    "  gs_widths = [1.0,]*num_plots_x\n",
    "  gs_heights = [1.0,]*num_plots_y\n",
    "  gs = gridspec.GridSpec(num_plots_y, num_plots_x, wspace=0.5, hspace=0.7,\n",
    "    width_ratios=gs_widths, height_ratios=gs_heights)\n",
    "  bf_idx = 0\n",
    "  for plot_id in np.ndindex((num_plots_y, num_plots_x)):\n",
    "    (y_id, x_id) = plot_id\n",
    "    if y_id == 0 and x_id == 0:\n",
    "      ax = fig.add_subplot(gs[plot_id])\n",
    "      #ax.set_ylabel(\"Activation\", fontsize=16)\n",
    "      #ax.set_xlabel(\"Orientation\", fontsize=16)\n",
    "      ax00 = ax\n",
    "    else:\n",
    "      ax = fig.add_subplot(gs[plot_id])#, sharey=ax00)\n",
    "    if bf_idx < num_bfs:\n",
    "      for co_idx, contrast in enumerate(contrasts):\n",
    "        co_idx = -1\n",
    "        contrast = 1.0#contrasts[co_idx]\n",
    "        activity = activations[bf_indices[bf_idx], co_idx, :]\n",
    "        color_val = scalarMap.to_rgba(contrast)\n",
    "        ax.plot(orientations, activity, linewidth=1, color=color_val)\n",
    "        ax.scatter(orientations, activity, s=4, c=[color_val])\n",
    "        ax.yaxis.set_major_formatter(FormatStrFormatter('%0.2g'))\n",
    "        ax.set_yticks([0, np.max(activity)])\n",
    "        ax.set_xticks([0, 90, 180])\n",
    "      bf_idx += 1\n",
    "    else:\n",
    "      ax = pf.clear_axis(ax, spines=\"none\")\n",
    "  plt.show()\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(weights, title=\"\", figsize=None, save_filename=None):\n",
    "  \"\"\"\n",
    "    weights: [np.ndarray] of shape [num_outputs, num_input_y, num_input_x]\n",
    "    The matrices are renormalized before plotting.\n",
    "  \"\"\"\n",
    "  weights = dp.norm_weights(weights)\n",
    "  vmin = np.min(weights)\n",
    "  vmax = np.max(weights)\n",
    "  num_plots = weights.shape[0]\n",
    "  num_plots_y = int(np.floor(np.sqrt(num_plots)))\n",
    "  num_plots_x = int(np.ceil(np.sqrt(num_plots)))\n",
    "  fig, sub_ax = plt.subplots(num_plots_y, num_plots_x, figsize=figsize)\n",
    "  filter_total = 0\n",
    "  for plot_id in  np.ndindex((num_plots_y, num_plots_x)):\n",
    "    if filter_total < num_plots:\n",
    "      sub_ax[plot_id].imshow(np.squeeze(weights[filter_total, ...]), vmin=vmin, vmax=vmax, cmap=\"Greys_r\")\n",
    "      filter_total += 1\n",
    "    pf.clear_axis(sub_ax[plot_id])\n",
    "    sub_ax[plot_id].set_aspect(\"equal\")\n",
    "  fig.suptitle(title, y=0.95, x=0.5, fontsize=20)\n",
    "  if save_filename is not None:\n",
    "      fig.savefig(save_filename)\n",
    "      plt.close(fig)\n",
    "      return None\n",
    "  plt.show()\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_curve(tuning_curve):\n",
    "  \"\"\"\n",
    "  Centers a curve about its preferred orientation\n",
    "  \"\"\"\n",
    "  return np.roll(tuning_curve, (len(tuning_curve) // 2) - np.argmax(tuning_curve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fwhm(centered_ot_curve, corresponding_angles_deg):\n",
    "  \"\"\"\n",
    "  Calculates the full width at half maximum of the tuning curve\n",
    "\n",
    "  Result is expressed in degrees to make it a little more intuitive. The curve\n",
    "  is often NOT symmetric about the maximum value so we don't do any fitting and\n",
    "  we return the FULL width\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  centered_ot_curve : ndarray\n",
    "      A 1d array of floats giving the value of the ot curve, at an orientation\n",
    "      relative to the *preferred orientation* which is given by the angles in\n",
    "      corresponding_angles_deg. This has the maximum orientation in the\n",
    "      center of the array which is nicer for visualization.\n",
    "  corresponding_angles_deg : ndarray\n",
    "      The orientations relative to preferred orientation that correspond to\n",
    "      the values in centered_ot_curve\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  half_max_left : float\n",
    "      The position of the intercept to the left of the max\n",
    "  half_max_right : float\n",
    "      The position of the intercept to the right of the max\n",
    "  half_max_value : float\n",
    "      Mainly for plotting purposes, the actual curve value that corresponds\n",
    "      to the left and right points\n",
    "  \"\"\"\n",
    "  max_idx = np.argmax(centered_ot_curve)\n",
    "  min_idx = np.argmin(centered_ot_curve)\n",
    "  max_val = centered_ot_curve[max_idx]\n",
    "  min_val = centered_ot_curve[min_idx]\n",
    "  midpoint = (max_val / 2) + (min_val / 2)\n",
    "  # find the left hand point\n",
    "  idx = max_idx\n",
    "  while centered_ot_curve[idx] > midpoint:\n",
    "    idx -= 1\n",
    "    if idx == -1:\n",
    "      # the width is *at least* 90 degrees\n",
    "      half_max_left = -90.\n",
    "      break\n",
    "  if idx > -1:\n",
    "    # we'll linearly interpolate between the two straddling points\n",
    "    # if (x2, y2) is the coordinate of the point below the half-max and\n",
    "    # (x1, y1) is the point above the half-max, then we can solve for x3, the\n",
    "    # x-position of the point that corresponds to the half-max on the line\n",
    "    # that connects (x1, y1) and (x2, y2)\n",
    "    half_max_left = (((midpoint - centered_ot_curve[idx])\n",
    "      * (corresponding_angles_deg[idx+1] - corresponding_angles_deg[idx])\n",
    "      / (centered_ot_curve[idx+1] - centered_ot_curve[idx]))\n",
    "      + corresponding_angles_deg[idx])\n",
    "  # find the right hand point\n",
    "  idx = max_idx\n",
    "  while centered_ot_curve[idx] > midpoint:\n",
    "    idx += 1\n",
    "    if idx == len(centered_ot_curve):\n",
    "      # the width is *at least* 90\n",
    "      half_max_right = 90.\n",
    "      break\n",
    "  if idx < len(centered_ot_curve):\n",
    "    # we'll linearly interpolate between the two straddling points again\n",
    "    half_max_right = (((midpoint - centered_ot_curve[idx-1])\n",
    "      * (corresponding_angles_deg[idx] - corresponding_angles_deg[idx-1])\n",
    "      / (centered_ot_curve[idx] - centered_ot_curve[idx-1]))\n",
    "      + corresponding_angles_deg[idx-1])\n",
    "  return half_max_left, half_max_right, midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_circ_var(centered_ot_curve, corresponding_angles_rad):\n",
    "  \"\"\"\n",
    "  From\n",
    "  DL Ringach, RM Shapley, MJ Hawken (2002) - Orientation Selectivity in Macaque V1:\n",
    "  Diversity and Laminar Dependence\n",
    "  \n",
    "  Computes the circular variance of a tuning curve and returns vals for plotting\n",
    "\n",
    "  This is a scale-invariant measure of how 'oriented' a curve is in some\n",
    "  global sense. It wraps reponses around the unit circle and then sums their\n",
    "  vectors, resulting in an average vector, the magnitude of which indicates\n",
    "  the strength of the tuning. Circular variance is an index of 'orientedness'\n",
    "  that falls in the interval [0.0, 1.0], with 0.0 indicating a delta function\n",
    "  and 1.0 indicating a completely flat tuning curve.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  centered_ot_curve : ndarray\n",
    "      A 1d array of floats giving the value of the ot curve, at an orientation\n",
    "      relative to the *preferred orientation* which is given by the angles in\n",
    "      corresponding_angles_rad. This has the maximum orientation in the\n",
    "      center of the array which is nicer for visualization.\n",
    "  corresponding_angles_rad : ndarray\n",
    "      The orientations relative to preferred orientation that correspond to\n",
    "      the values in centered_ot_curve\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  numerator_sum_components : ndarray\n",
    "      The complex values the are produced from r * np.exp(j*2*theta). These\n",
    "      are the elements that get summed up in the numerator\n",
    "  direction_vector : complex64 or complex128\n",
    "      This is the vector that points in the direction of *aggregate* tuning.\n",
    "      its magnitude is upper bounded by 1.0 which is the case when only one\n",
    "      orientation has a nonzero value. We can plot it to get an idea of how\n",
    "      tuned a curve is\n",
    "  circular_variance : float\n",
    "      This is 1 minus the magnitude of the direction vector. It represents and\n",
    "      index of 'global selectivity'\n",
    "  \"\"\"\n",
    "  # in the original definition, angles are [0, 2*np.pi] so the factor of 2\n",
    "  # in the exponential wraps the phase twice around the complex circle,\n",
    "  # placing responses that correspond to angles pi degrees apart\n",
    "  # onto the same place. We know there's a redudancy in our responses at pi\n",
    "  # offsets so our responses get wrapped around the unit circle once.\n",
    "  numerator_sum_components = (centered_ot_curve\n",
    "    * np.exp(1j * 2 * corresponding_angles_rad))\n",
    "  direction_vector = (np.sum(numerator_sum_components)\n",
    "    / np.sum(centered_ot_curve))\n",
    "  circular_variance = 1 - np.abs(direction_vector)\n",
    "  return (numerator_sum_components, direction_vector, circular_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_osi(centered_ot_curve):\n",
    "  \"\"\"\n",
    "  Compute the Orientation Selectivity Index.\n",
    "\n",
    "  This is the most coarse but popular measure of selectivity. It really\n",
    "  doesn't tell you much. It just measures the maximum response relative to\n",
    "  the minimum response.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  centered_ot_curve : ndarray\n",
    "      A 1d array of floats giving the value of the ot curve, at an orientation\n",
    "      relative to the *preferred orientation*\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  osi : float\n",
    "      This is (a_max - a_orth) / (a_max + a_orth) where a_max is the maximum\n",
    "      response across orientations when orientation responses are\n",
    "      *averages* over phase. a_orth is the orientation which is orthogonal to\n",
    "      the orientation which produces a_max.\n",
    "  \"\"\"\n",
    "  max_val = np.max(centered_ot_curve)\n",
    "  # Assume that orthogonal orientation is at either end of the curve modulo 1\n",
    "  # bin (if we had like an even number of orientation values)\n",
    "  orth_val = centered_ot_curve[0]\n",
    "  osi = (max_val - orth_val) / (max_val + orth_val)\n",
    "  return osi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_circular_variance(cv_data, max_bfs_per_fig=400, title=\"\", save_filename=None):\n",
    "  assert np.sqrt(max_bfs_per_fig) % 1 == 0, \"Pick a square number for max_bfs_per_fig\"\n",
    "  orientations = (np.pi * np.arange(len(cv_data))\n",
    "    / len(cv_data)) - (np.pi/2) # relative to preferred\n",
    "  num_bfs = len(cv_data)\n",
    "  num_bf_figs = int(np.ceil(num_bfs / max_bfs_per_fig))\n",
    "  # this determines how many ot curves are aranged in a square grid within\n",
    "  # any given figure\n",
    "  if num_bf_figs > 1:\n",
    "    bfs_per_fig = max_bfs_per_fig\n",
    "  else:\n",
    "    squares = [x**2 for x in range(1, int(np.sqrt(max_bfs_per_fig))+1)]\n",
    "    bfs_per_fig = squares[bisect.bisect_left(squares, num_bfs)]\n",
    "  plot_sidelength = int(np.sqrt(bfs_per_fig))\n",
    "  bf_idx = 0\n",
    "  bf_figs = []\n",
    "  for in_bf_fig_idx in range(num_bf_figs):\n",
    "    fig = plt.figure(figsize=(32, 32))\n",
    "    plt.suptitle(title + ', fig {} of {}'.format(\n",
    "      in_bf_fig_idx+1, num_bf_figs), fontsize=20)\n",
    "    subplot_grid = gridspec.GridSpec(plot_sidelength, plot_sidelength,\n",
    "      wspace=0.4, hspace=0.4)\n",
    "    fig_bf_idx = bf_idx % bfs_per_fig\n",
    "    while fig_bf_idx < bfs_per_fig and bf_idx < num_bfs:\n",
    "      #if bf_idx % 100 == 0:\n",
    "      #  print(\"plotted \", bf_idx, \" of \", num_bfs, \" circular variance plots\")\n",
    "      ## print(\"sum vector: \", np.real(cv_data[bf_idx][1]), np.imag(cv_data[bf_idx][1]))\n",
    "      ax = plt.Subplot(fig, subplot_grid[fig_bf_idx])\n",
    "      ax.plot(np.real(cv_data[bf_idx][0]), np.imag(cv_data[bf_idx][0]),\n",
    "              c='g', linewidth=0.5)\n",
    "      ax.scatter(np.real(cv_data[bf_idx][0]), np.imag(cv_data[bf_idx][0]),\n",
    "                 c='g', s=4)\n",
    "      ax.quiver(np.real(cv_data[bf_idx][1]), np.imag(cv_data[bf_idx][1]),\n",
    "                angles='xy', scale_units='xy', scale=1.0, color='b',\n",
    "                width=0.01)\n",
    "      # ax.quiver(0.5, 0.5, color='b')\n",
    "      ax.axvline(x=0.0, color='k', linestyle='--', alpha=0.6, linewidth=0.3)\n",
    "      ax.axhline(y=0.0, color='k', linestyle='--', alpha=0.6, linewidth=0.3)\n",
    "      ax.yaxis.set_major_formatter(FormatStrFormatter('%0.2g'))\n",
    "      xaxis_size = max(np.max(np.real(cv_data[bf_idx][0])), 1.0)\n",
    "      yaxis_size = max(np.max(np.imag(cv_data[bf_idx][0])), 1.0)\n",
    "      ax.set_yticks([-1. * yaxis_size, yaxis_size])\n",
    "      ax.set_xticks([-1. * xaxis_size, xaxis_size])\n",
    "      # put the circular variance index in the upper left\n",
    "      ax.text(0.02, 0.97, 'CV: {:.2f}'.format(cv_data[bf_idx][2]),\n",
    "              horizontalalignment='left', verticalalignment='top',\n",
    "              transform=ax.transAxes, color='b', fontsize=10)\n",
    "      fig.add_subplot(ax)\n",
    "      fig_bf_idx += 1\n",
    "      bf_idx += 1\n",
    "    if save_filename is not None:\n",
    "      filename_split = os.path.split(save_filename)\n",
    "      save_filename = filename_split[0]+str(in_bf_fig_idx).zfill(2)+\"_\"+filename_split[1]\n",
    "      fig.savefig(save_filename)\n",
    "      plt.close(fig)\n",
    "      bf_figs.append(None)\n",
    "    else:\n",
    "      bf_figs.append(fig)\n",
    "  if save_filename is None:\n",
    "    plt.show()\n",
    "  return bf_figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_circular_variance_histogram(variances_list, label_list, num_bins=50, y_max=None,\n",
    "  figsize=None, save_filename=None):\n",
    "  variance_min = np.min([np.min(var) for var in variances_list])#0.0\n",
    "  variance_max = np.max([np.max(var) for var in variances_list])#1.0\n",
    "  bins = np.linspace(variance_min, variance_max, num_bins)\n",
    "  bar_width = np.diff(bins).min()\n",
    "  fig, ax = plt.subplots(1, figsize=figsize)\n",
    "  hist_list = []\n",
    "  handles = []\n",
    "  for variances, label in zip(variances_list, label_list):\n",
    "    hist, bin_edges = np.histogram(variances.flatten(), bins)\n",
    "    #hist = hist / np.max(hist)\n",
    "    hist_list.append(hist)\n",
    "    bin_left, bin_right = bin_edges[:-1], bin_edges[1:]\n",
    "    bin_centers = bin_left + (bin_right - bin_left)/2\n",
    "    handles.append(ax.bar(bin_centers, hist, width=bar_width, log=True, align=\"center\", alpha=0.5, label=label))\n",
    "  ax.set_xticks(bin_left, minor=True)\n",
    "  ax.set_xticks(bin_left[::4], minor=False)\n",
    "  ax.xaxis.set_major_formatter(FormatStrFormatter(\"%0.0f\"))\n",
    "  ax.tick_params(\"both\", labelsize=16)\n",
    "  ax.set_xlim([variance_min, variance_max])\n",
    "  ax.set_xticks([variance_min, variance_max])\n",
    "  ax.set_xticklabels([\"More selective\", \"Less selective\"])\n",
    "  ticks = ax.xaxis.get_major_ticks()\n",
    "  ticks[0].label1.set_horizontalalignment(\"left\")\n",
    "  ticks[1].label1.set_horizontalalignment(\"right\")\n",
    "  if y_max is None:\n",
    "    # Round up to the nearest power of 10\n",
    "    y_max = 10**(np.ceil(np.log10(np.max([np.max(hist) for hist in hist_list]))))\n",
    "  ax.set_ylim([1, y_max])\n",
    "  ax.set_title(\"Circular Variance Histogram\", fontsize=18)\n",
    "  ax.set_xlabel(\"Selectivity\", fontsize=18)\n",
    "  ax.set_ylabel(\"Log Count\", fontsize=18)\n",
    "  legend = ax.legend(handles, label_list, fontsize=12, #ncol=len(label_list),\n",
    "    borderaxespad=0., bbox_to_anchor=[0.98, 0.98], fancybox=True, loc=\"upper right\")\n",
    "  if save_filename is not None:\n",
    "    fig.savefig(save_filename)\n",
    "    plt.close(fig)\n",
    "    return None\n",
    "  plt.show()\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circ_var_list = []\n",
    "for analyzer in analyzer_list:\n",
    "  analyzer.bf_indices = np.random.choice(analyzer.ot_grating_responses[\"neuron_indices\"], 12)\n",
    "  num_orientation_samples = len(analyzer.ot_grating_responses['orientations'])\n",
    "  corresponding_angles_deg = (180 * np.arange(num_orientation_samples) / num_orientation_samples) - 90\n",
    "  corresponding_angles_rad = (np.pi * np.arange(num_orientation_samples) / num_orientation_samples) - (np.pi/2)\n",
    "  analyzer.metrics_list = {\"fwhm\":[], \"circ_var\":[], \"osi\":[], \"skipped_indices\":[]}\n",
    "  contrast_idx = -1\n",
    "  for bf_idx in range(analyzer.bf_stats[\"num_outputs\"]):\n",
    "    ot_curve = center_curve(analyzer.ot_grating_responses[\"mean_responses\"][bf_idx, contrast_idx, :])\n",
    "    if np.max(ot_curve) - np.min(ot_curve) == 0:\n",
    "      analyzer.metrics_list[\"skipped_indices\"].append(bf_idx)\n",
    "    else:\n",
    "      fwhm = compute_fwhm(ot_curve, corresponding_angles_deg)\n",
    "      analyzer.metrics_list[\"fwhm\"].append(fwhm)\n",
    "      circ_var = compute_circ_var(ot_curve, corresponding_angles_rad)\n",
    "      analyzer.metrics_list[\"circ_var\"].append(circ_var)\n",
    "      osi = compute_osi(ot_curve)\n",
    "      analyzer.metrics_list[\"osi\"].append(osi)\n",
    "  circ_var_list.append(np.array([val[2] for val in analyzer.metrics_list[\"circ_var\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "color_list = [color_vals[\"md_green\"], color_vals[\"md_blue\"], color_vals[\"md_red\"]]\n",
    "label_list = [\"Linear Autoencoder\", \"Sparse Autoencoder\", \"Sparse Coding\"]\n",
    "num_bins = 30\n",
    "width_ratios = [0.5, 0.25, 0.25]\n",
    "fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "gs0 = gridspec.GridSpec(1, 3, width_ratios=width_ratios)\n",
    "axes = []\n",
    "\n",
    "height_ratios = [0.13, 0.25, 0.25, 0.25]\n",
    "gs_hist = gridspec.GridSpecFromSubplotSpec(4, 1, gs0[0], height_ratios=height_ratios)\n",
    "axes.append(fig.add_subplot(gs_hist[1:3, 0]))\n",
    "variance_min = 0.0\n",
    "variance_max = 1.0\n",
    "bins = np.linspace(variance_min, variance_max, num_bins)\n",
    "bar_width = np.diff(bins).min()\n",
    "hist_list = []\n",
    "for variances, label, color in zip(circ_var_list, label_list, color_list):\n",
    "  hist, bin_edges = np.histogram(variances.flatten(), bins)\n",
    "  hist_list.append(hist)\n",
    "  bin_left, bin_right = bin_edges[:-1], bin_edges[1:]\n",
    "  bin_centers = bin_left + (bin_right - bin_left)/2\n",
    "  axes[-1].plot(bin_centers, hist, linestyle=\"-\", drawstyle=\"steps-mid\", color=color, label=label)\n",
    "  #axes[-1].bar(bin_centers, hist, width=bar_width, log=False,\n",
    "  #  align=\"center\", alpha=0.5, label=label)\n",
    "axes[-1].set_xticks(bin_left, minor=True)\n",
    "axes[-1].set_xticks(bin_left[::4], minor=False)\n",
    "axes[-1].xaxis.set_major_formatter(FormatStrFormatter(\"%0.0f\"))\n",
    "axes[-1].tick_params(\"both\", labelsize=fontsize)\n",
    "axes[-1].set_xlim([variance_min, variance_max])\n",
    "axes[-1].set_xticks([variance_min, variance_max])\n",
    "axes[-1].set_xticklabels([\"More\\nselective\", \"Less\\nselective\"])\n",
    "ticks = axes[-1].xaxis.get_major_ticks()\n",
    "ticks[0].label1.set_horizontalalignment(\"left\")\n",
    "ticks[1].label1.set_horizontalalignment(\"right\")\n",
    "y_max = np.max([np.max(hist) for hist in hist_list])\n",
    "axes[-1].set_ylim([0, y_max+1])\n",
    "axes[-1].set_title(\"Circular Variance\", fontsize=fontsize)\n",
    "axes[-1].set_ylabel(\"Count\", fontsize=fontsize)\n",
    "handles, labels = axes[-1].get_legend_handles_labels()\n",
    "#legend = axes[-1].legend(handles, label_list, fontsize=12,\n",
    "#  borderaxespad=0., bbox_to_anchor=[0.98, 0.98], loc=\"upper right\")\n",
    "legend = axes[-1].legend(handles=handles, labels=labels, fontsize=fontsize,\n",
    "  borderaxespad=0., framealpha=0.0, loc=\"upper right\")\n",
    "legend.get_frame().set_linewidth(0.0)\n",
    "for text, color in zip(legend.get_texts(), color_list):\n",
    "  text.set_color(color)\n",
    "for item in legend.legendHandles:\n",
    "  item.set_visible(False)\n",
    "\n",
    "gs_weights = gridspec.GridSpecFromSubplotSpec(len(analyzer_list), 1, gs0[1], hspace=-0.6)\n",
    "for gs_idx, analyzer in enumerate(analyzer_list):\n",
    "  weights = np.stack(analyzer.bf_stats[\"basis_functions\"], axis=0)[analyzer.bf_indices, ...]\n",
    "  weights = dp.norm_weights(weights)\n",
    "  vmin = np.min(weights)\n",
    "  vmax = np.max(weights)\n",
    "  num_plots = weights.shape[0]\n",
    "  num_plots_y = int(np.ceil(np.sqrt(num_plots)))\n",
    "  num_plots_x = int(np.ceil(np.sqrt(num_plots)))\n",
    "  gs_weights_inner = gridspec.GridSpecFromSubplotSpec(num_plots_y, num_plots_x, gs_weights[gs_idx],\n",
    "    hspace=-0.85)\n",
    "  bf_idx = 0\n",
    "  for plot_id in  np.ndindex((num_plots_y, num_plots_x)):\n",
    "    if bf_idx < num_plots:\n",
    "      axes.append(fig.add_subplot(gs_weights_inner[plot_id]))\n",
    "      axes[-1].imshow(np.squeeze(weights[bf_idx, ...]), vmin=vmin, vmax=vmax, cmap=\"Greys_r\")\n",
    "      bf_idx += 1\n",
    "    pf.clear_axis(axes[-1])\n",
    "\n",
    "gs_tuning = gridspec.GridSpecFromSubplotSpec(len(analyzer_list), 1, gs0[2], hspace=-0.6)\n",
    "for analyzer_idx, analyzer in enumerate(analyzer_list):\n",
    "  contrasts = analyzer.ot_grating_responses[\"contrasts\"]\n",
    "  orientations = analyzer.ot_grating_responses[\"orientations\"]\n",
    "  activations = analyzer.ot_grating_responses[\"mean_responses\"]\n",
    "  activations = activations / np.max(activations[analyzer.bf_indices, -1, ...])\n",
    "  orientations = np.asarray(orientations)*(180/np.pi) #convert to degrees for plotting\n",
    "  orientations = orientations / np.max(orientations)\n",
    "  num_plots = len(analyzer.bf_indices)\n",
    "  num_plots_y = int(np.ceil(np.sqrt(num_plots)))\n",
    "  num_plots_x = int(np.ceil(np.sqrt(num_plots)))\n",
    "  gs_tuning_inner = gridspec.GridSpecFromSubplotSpec(num_plots_y, num_plots_x, gs_tuning[analyzer_idx],\n",
    "      hspace=-0.85)\n",
    "  bf_idx = 0\n",
    "  for plot_id in np.ndindex((num_plots_y, num_plots_x)):\n",
    "    if bf_idx < num_plots:\n",
    "      if bf_idx == 0:\n",
    "        axes.append(fig.add_subplot(gs_tuning_inner[plot_id]))\n",
    "        ax_orig_id = len(axes)-1\n",
    "      else:\n",
    "        axes.append(fig.add_subplot(gs_tuning_inner[plot_id], sharey=axes[ax_orig_id], sharex=axes[ax_orig_id]))\n",
    "      contrast_idx = -1\n",
    "      activity = activations[analyzer.bf_indices[bf_idx], contrast_idx, :]\n",
    "      #activity = activity / np.max(activity)\n",
    "      axes[-1].plot(orientations, activity, linewidth=0.5, color='k')\n",
    "      axes[-1].scatter(orientations, activity, s=0.1, c='k')\n",
    "      axes[-1].set_aspect('equal', adjustable='box')\n",
    "      axes[-1].set_yticks([])\n",
    "      axes[-1].set_xticks([])\n",
    "      bf_idx += 1\n",
    "    (y_id, x_id) = plot_id\n",
    "    if y_id == 0 and x_id == 0:\n",
    "      plt.text(x=0.1, y=1.4, s=analyzer.analysis_params.display_name, horizontalalignment='center',\n",
    "        verticalalignment='center', transform=axes[-1].transAxes, fontsize=fontsize)\n",
    "\n",
    "#gs_circvar = gridspec.GridSpecFromSubplotSpec(len(analyzer_list), 1, gs0[3])#, hspace=-0.5)\n",
    "#for analyzer_index, analyzer in enumerate(analyzer_list):\n",
    "#  cv_data = [val for index, val in enumerate(analyzer.metrics_list[\"circ_var\"]) if index in bf_indices]\n",
    "#  orientations = (np.pi * np.arange(len(cv_data)) / len(cv_data)) - (np.pi/2) # relative to preferred\n",
    "#  num_bfs = len(cv_data)\n",
    "#  num_plots_y = np.int32(np.ceil(np.sqrt(num_bfs)))+1\n",
    "#  num_plots_x = np.int32(np.ceil(np.sqrt(num_bfs)))\n",
    "#  gs_circvar_inner = gridspec.GridSpecFromSubplotSpec(num_plots_y, num_plots_x, gs_circvar[analyzer_index],\n",
    "#    wspace=0.4, hspace=0.4)\n",
    "#  bf_idx = 0\n",
    "#  for plot_id in np.ndindex((num_plots_y, num_plots_x)):\n",
    "#    (y_id, x_id) = plot_id\n",
    "#    if y_id == 0 and x_id == 0:\n",
    "#      axes.append(fig.add_subplot(gs_circvar_inner[plot_id]))\n",
    "#      ax00 = axes[-1]\n",
    "#    else:\n",
    "#      axes.append(fig.add_subplot(gs_circvar_inner[plot_id]))\n",
    "#    if bf_idx < num_bfs:\n",
    "#      axes[-1].plot(np.real(cv_data[bf_idx][0]), np.imag(cv_data[bf_idx][0]), c='g', linewidth=0.5)\n",
    "#      #axes[-1].scatter(np.real(cv_data[bf_idx][0]), np.imag(cv_data[bf_idx][0]), c='g', s=4)\n",
    "#      #axes[-1].quiver(np.real(cv_data[bf_idx][1]), np.imag(cv_data[bf_idx][1]),\n",
    "#      #          angles='xy', scale_units='xy', scale=1.0, color='b', width=0.01)\n",
    "#      #axes[-1].quiver(0.5, 0.5, color='b')\n",
    "#      axes[-1].yaxis.set_major_formatter(FormatStrFormatter('%0.2g'))\n",
    "#      xaxis_size = max(np.max(np.real(cv_data[bf_idx][0])), 1.0)\n",
    "#      yaxis_size = max(np.max(np.imag(cv_data[bf_idx][0])), 1.0)\n",
    "#      axes[-1].set_yticks([])#[-1. * yaxis_size, yaxis_size])\n",
    "#      axes[-1].set_xticks([])#[-1. * xaxis_size, xaxis_size])\n",
    "#      # put the circular variance index in the upper left\n",
    "#      #axes[-1].text(0.02, 0.97, '{:.2f}'.format(cv_data[bf_idx][2]),\n",
    "#      #        horizontalalignment='left', verticalalignment='top',\n",
    "#      #        transform=axes[-1].transAxes, color='b', fontsize=10)\n",
    "#      bf_idx += 1\n",
    "#    else:\n",
    "#      pf.clear_axis(axes[-1])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "for analyzer in analyzer_list:\n",
    "  for ext in [\".png\", \".eps\"]:\n",
    "    save_name = (analyzer.analysis_out_dir+\"/vis/circular_variance_combo\"\n",
    "      +\"_\"+analyzer.analysis_params.save_info+ext)\n",
    "    fig.savefig(save_name, transparent=False, bbox_inches=\"tight\", pad_inches=0.01, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rads = []\n",
    "max_rads = []\n",
    "for analyzer in analyzer_list:\n",
    "  analyzer.bf_spatial_freq_rads = [np.sqrt(x**2+y**2) for (y,x) in analyzer.bf_stats[\"fourier_centers\"]]\n",
    "  min_rads.append(np.min(analyzer.bf_spatial_freq_rads))\n",
    "  max_rads.append(np.max(analyzer.bf_spatial_freq_rads))\n",
    "  \n",
    "num_bins = 10\n",
    "min_rad = np.min(min_rads)\n",
    "max_rad = np.max(max_rads)\n",
    "bins = np.linspace(min_rad, max_rad, num_bins)\n",
    "fig, ax = plt.subplots(1, figsize=figsize, dpi=dpi)\n",
    "hist_max = []\n",
    "for analyzer in analyzer_list:\n",
    "  hist, bin_edges = np.histogram(analyzer.bf_spatial_freq_rads, bins, density=True)\n",
    "  bin_left, bin_right = bin_edges[:-1], bin_edges[1:]\n",
    "  bin_centers = bin_left + (bin_right - bin_left)/2\n",
    "  \n",
    "  label = analyzer.model.params.model_type.upper()# + \" \" + str(analyzer.model.get_num_latent())# + \" van Hateren\"\n",
    "  #label = re.sub(\"_\", \" \", analyzer.model_name)\n",
    "  ax.plot(bin_centers, hist, alpha=1.0, linestyle=\"-\", drawstyle=\"steps-mid\", label=label)\n",
    "  hist_max.append(np.max(hist))\n",
    "  \n",
    "ax.set_xticks(bin_left, minor=True)\n",
    "ax.set_xticks(bin_left[::4], minor=False)\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter(\"%0.0f\"))\n",
    "ax.tick_params(\"both\", labelsize=16)\n",
    "ax.set_xlim([min_rad, max_rad])\n",
    "ax.set_xticks([0, int(np.floor(max_rad/4)), int(2*np.floor(max_rad/4)),\n",
    "  int(3*np.floor(max_rad/4)), max_rad])\n",
    "ax.set_ylim([0, 0.6])\n",
    "ax.set_xlabel(\"Spatial Frequency\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"Density\", fontsize=fontsize)\n",
    "ax.set_title(\"Neuron Weight Spatial Frequency Histogram\", fontsize=fontsize)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legend = ax.legend(handles, labels, fontsize=fontsize, ncol=3,\n",
    "  borderaxespad=0., bbox_to_anchor=[0.01, 0.99], fancybox=True, loc=\"upper left\")\n",
    "for line in legend.get_lines():\n",
    "  line.set_linewidth(3)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRAE Iso-Response Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = [ae_deep_mnist_params()]#,lca_768_vh_params()]#, rica_768_vh_params(), sae_768_vh_params()]\n",
    "for params in params_list:\n",
    "  params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+params.model_name)\n",
    "analyzer_list = [ap.get_analyzer(params.model_type) for params in params_list]\n",
    "for analyzer, params in zip(analyzer_list, params_list):\n",
    "  analyzer.setup(params)\n",
    "  analyzer.model.setup(analyzer.model_params)\n",
    "  analyzer.load_analysis(save_info=params.save_info)\n",
    "  analyzer.model_name = params.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_angle = 20\n",
    "max_angle = 55\n",
    "num_neurons = 2 # How many neurons to plot\n",
    "use_bf_stats = True # If false, then use optimal stimulus\n",
    "num_comparison_vects = 10\n",
    "use_random_orth_vects = False\n",
    "x_range = [-2, 2]\n",
    "y_range = [-2, 2]\n",
    "num_images = int(10**2)\n",
    "\n",
    "for analyzer in analyzer_list:\n",
    "  compute_iso_vectors(analyzer, min_angle, max_angle, num_neurons, use_bf_stats)\n",
    "  contour_dataset = get_contour_dataset(analyzer, num_comparison_vects,\n",
    "    use_random_orth_vects, x_range, y_range, num_images)\n",
    "  analyzer.activations = get_normalized_activations(analyzer, contour_dataset[\"datapoints\"])\n",
    "  contour_dataset.pop(\"datapoints\")\n",
    "  analyzer.contour_dataset = contour_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots_y = num_neurons + 1 # extra dimension for example image\n",
    "num_plots_x = num_neurons + 1 # extra dimension for example image\n",
    "\n",
    "gs0 = gridspec.GridSpec(num_plots_y, num_plots_x, wspace=0.1, hspace=0.1)\n",
    "fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "cmap = plt.get_cmap('viridis')\n",
    "\n",
    "orth_vectors = []\n",
    "for neuron_loop_index in range(num_neurons): # rows\n",
    "  for orth_loop_index in range(num_neurons): # columns\n",
    "    norm_activity = analyzer.activations[neuron_loop_index][orth_loop_index]\n",
    "    proj_target = analyzer.contour_dataset[\"proj_target_neuron\"][neuron_loop_index][orth_loop_index]\n",
    "    proj_comparison = analyzer.contour_dataset[\"proj_comparison_neuron\"][neuron_loop_index][orth_loop_index]\n",
    "    proj_orth = analyzer.contour_dataset[\"proj_orth_vect\"][neuron_loop_index][orth_loop_index]\n",
    "    orth_vectors.append(analyzer.contour_dataset[\"orth_vect\"][neuron_loop_index][orth_loop_index])\n",
    "\n",
    "    curve_plot_y_idx = neuron_loop_index + 1\n",
    "    curve_plot_x_idx = orth_loop_index + 1\n",
    "    curve_ax = pf.clear_axis(fig.add_subplot(gs0[curve_plot_y_idx, curve_plot_x_idx]))\n",
    "\n",
    "    # NOTE: each subplot has a renormalized color scale\n",
    "    # TODO: Add scale bar like in the lca inference plots\n",
    "    vmin = np.min(norm_activity)\n",
    "    vmax = np.max(norm_activity)\n",
    "\n",
    "    levels = 5\n",
    "    x_mesh, y_mesh = np.meshgrid(analyzer.contour_dataset[\"x_pts\"], analyzer.contour_dataset[\"y_pts\"])\n",
    "    contsf = curve_ax.contourf(x_mesh, y_mesh, norm_activity,\n",
    "      levels=levels, vmin=vmin, vmax=vmax, alpha=1.0, antialiased=True, cmap=cmap)\n",
    "\n",
    "    curve_ax.arrow(0, 0, proj_target[0].item(), proj_target[1].item(),\n",
    "      width=0.05, head_width=0.15, head_length=0.15, fc='r', ec='r')\n",
    "    curve_ax.arrow(0, 0, proj_comparison[0].item(), proj_comparison[1].item(),\n",
    "      width=0.05, head_width=0.15, head_length=0.15, fc='w', ec='w')\n",
    "    curve_ax.arrow(0, 0, proj_orth[0].item(), proj_orth[1].item(),\n",
    "      width=0.05, head_width=0.15, head_length=0.15, fc='k', ec='k')\n",
    "\n",
    "    curve_ax.set_xlim(x_range)\n",
    "    curve_ax.set_ylim(y_range)\n",
    "    \n",
    "for plot_y_id in range(num_plots_y):\n",
    "  for plot_x_id in range(num_plots_x):\n",
    "    if plot_y_id > 0 and plot_x_id == 0:\n",
    "      bf_ax = pf.clear_axis(fig.add_subplot(gs0[plot_y_id, plot_x_id]))\n",
    "      bf_resh = analyzer.target_vectors[plot_y_id-1].reshape((int(np.sqrt(np.prod(analyzer.model.params.data_shape))),\n",
    "        int(np.sqrt(np.prod(analyzer.model.params.data_shape)))))\n",
    "      bf_ax.imshow(bf_resh, cmap=\"Greys_r\")\n",
    "      if plot_y_id == 1:\n",
    "        bf_ax.set_title(\"Target vectors\", color=\"r\", fontsize=16)\n",
    "    if plot_y_id == 0 and plot_x_id > 0:\n",
    "      orth_img = orth_vectors[plot_x_id-1].reshape(int(np.sqrt(np.prod(analyzer.model.params.data_shape))),\n",
    "        int(np.sqrt(np.prod(analyzer.model.params.data_shape))))\n",
    "      orth_ax = pf.clear_axis(fig.add_subplot(gs0[plot_y_id, plot_x_id]))\n",
    "      orth_ax.imshow(orth_img, cmap=\"Greys_r\")\n",
    "      if plot_x_id == 1:\n",
    "        orth_ax.set_title(\"Orthogonal vectors\", color=\"k\", fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "#fig.savefig(analyzer.analysis_out_dir+\"/vis/iso_contour_grid_04.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for analyzer in analyzer_list:\n",
    "  circ_var_figs = plot_circular_variance(analyzer.metrics_list[\"circ_var\"],\n",
    "    max_bfs_per_fig=144, title=\"Circular Variance\")\n",
    "  for fig_idx, circ_fig in enumerate(circ_var_figs):\n",
    "    circ_fig.savefig(analyzer.analysis_out_dir+\"/vis/circular_variance_\"+str(fig_idx)+\".png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
