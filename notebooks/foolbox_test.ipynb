{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "while 'DeepSparseCoding' in ROOT_DIR:\n",
    "    ROOT_DIR = os.path.dirname(ROOT_DIR)\n",
    "if ROOT_DIR not in sys.path: sys.path.append(ROOT_DIR)\n",
    "    \n",
    "import pickle\n",
    "import numpy as np\n",
    "import proplot as plot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "from DeepSparseCoding.utils.file_utils import Logger\n",
    "import DeepSparseCoding.utils.loaders as loaders\n",
    "import DeepSparseCoding.utils.run_utils as run_utils\n",
    "import DeepSparseCoding.utils.dataset_utils as dataset_utils\n",
    "import DeepSparseCoding.utils.run_utils as ru\n",
    "import DeepSparseCoding.utils.plot_functions as pf\n",
    "import DeepSparseCoding.utils.data_processing as dp\n",
    "\n",
    "import eagerpy as ep\n",
    "from foolbox import PyTorchModel, accuracy, samples\n",
    "import foolbox.attacks as fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_mnist_fb() -> PyTorchModel:\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 32, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Dropout2d(0.25),\n",
    "        nn.Flatten(),  # type: ignore\n",
    "        nn.Linear(9216, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(0.5),\n",
    "        nn.Linear(128, 10),\n",
    "    )\n",
    "    #path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"mnist_cnn.pth\")\n",
    "    path = os.path.join(*[ROOT_DIR, 'DeepSparseCoding', 'mnist_cnn.pth'])\n",
    "    model.load_state_dict(torch.load(path))  # type: ignore\n",
    "    model.eval()\n",
    "    preprocessing = dict(mean=0.1307, std=0.3081)\n",
    "    fmodel = PyTorchModel(model, bounds=(0, 1), preprocessing=preprocessing)\n",
    "    return fmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mnist_dsc(log_file, cp_file):\n",
    "    logger = Logger(log_file, overwrite=False)\n",
    "    log_text = logger.load_file()\n",
    "    params = logger.read_params(log_text)[-1]\n",
    "    params.cp_latest_filename = cp_file\n",
    "    params.standardize_data = False\n",
    "    params.rescale_data_to_one = True\n",
    "    train_loader, val_loader, test_loader, data_params = dataset_utils.load_dataset(params)\n",
    "    for key, value in data_params.items():\n",
    "        setattr(params, key, value)\n",
    "    model = loaders.load_model(params.model_type)\n",
    "    model.setup(params, logger)\n",
    "    model.params.analysis_out_dir = os.path.join(\n",
    "        *[model.params.model_out_dir, 'analysis', model.params.version])\n",
    "    model.params.analysis_save_dir = os.path.join(model.params.analysis_out_dir, 'savefiles')\n",
    "    if not os.path.exists(model.params.analysis_save_dir):\n",
    "        os.makedirs(model.params.analysis_save_dir)\n",
    "    model.to(params.device)\n",
    "    model.load_checkpoint()\n",
    "    fmodel = PyTorchModel(model.eval(), bounds=(0, 1))\n",
    "    return fmodel, test_loader, model.params.batch_size, model.params.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_files = [\n",
    "    os.path.join(*[ROOT_DIR, 'Torch_projects', 'mlp_768_mnist', 'logfiles', 'mlp_768_mnist_v0.log']),\n",
    "    os.path.join(*[ROOT_DIR, 'Torch_projects', 'lca_768_mlp_mnist', 'logfiles', 'lca_768_mlp_mnist_v0.log'])\n",
    "    ]\n",
    "\n",
    "cp_latest_filenames = [\n",
    "    os.path.join(*[ROOT_DIR,'Torch_projects', 'mlp_768_mnist', 'checkpoints', 'mlp_768_mnist_latest_checkpoint_v0.pt']),\n",
    "    os.path.join(*[ROOT_DIR, 'Torch_projects', 'lca_768_mlp_mnist', 'checkpoints', 'lca_768_mlp_mnist_latest_checkpoint_v0.pt'])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_results = True\n",
    "load_results = True\n",
    "\n",
    "\n",
    "attack_params = {\n",
    "    'LinfPGD': {\n",
    "        #'rel_stepsize':1e-3,\n",
    "        #'steps':int(1/(2*1e-3)) # max perturbation it can reach is 0.5\n",
    "        #'rel_stepsize':1/50,\n",
    "        'abs_stepsize':1/500,\n",
    "        'steps':400,\n",
    "    },\n",
    "    'L2PGD': {\n",
    "        #'rel_stepsize':0.01,\n",
    "        'abs_stepsize':0.01,\n",
    "        'steps':600,\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "linf_epsilons = [ # allowed perturbation size\n",
    "    0.0,\n",
    "    0.03,\n",
    "    0.06,\n",
    "    0.09,\n",
    "    0.1,\n",
    "    0.13,\n",
    "    0.16,\n",
    "    0.19,\n",
    "    0.2,\n",
    "    0.23,\n",
    "    0.26,\n",
    "    0.3\n",
    "]\n",
    "\n",
    "l2_epsilons = [10 * eps for eps in linf_epsilons]\n",
    "\n",
    "attacks = [\n",
    "    (fa.LinfPGD(**attack_params['LinfPGD']), linf_epsilons),\n",
    "    (fa.L2PGD(**attack_params['L2PGD']), l2_epsilons),\n",
    "    #fa.FGSM(),\n",
    "    #fa.LinfBasicIterativeAttack(),\n",
    "    #fa.LinfAdditiveUniformNoiseAttack(),\n",
    "    #fa.LinfDeepFoolAttack(),\n",
    "]\n",
    "\n",
    "if load_results:\n",
    "    attack_results = np.loadz('adv_attack_results.npz')['data']\n",
    "else:\n",
    "    fmodel_cnn = create_mnist_fb()\n",
    "    fmodel_mlp, test_loader, batch_size, device = create_mnist_dsc(log_files[0], cp_latest_filenames[0])\n",
    "    fmodel_lca = create_mnist_dsc(log_files[1], cp_latest_filenames[1])[0]\n",
    "\n",
    "    model_types = ['CNN', 'MLP', 'LCA+SLP']\n",
    "    fmodel_list = [fmodel_cnn, fmodel_mlp, fmodel_lca]\n",
    "\n",
    "    num_batches =  5#len(test_loader.dataset) // batch_size\n",
    "\n",
    "    attack_results = []\n",
    "    for model_index, (fmodel, model_type) in enumerate(zip(fmodel_list, model_types)):\n",
    "        attack_success = np.zeros(\n",
    "                (len(attacks), len(linf_epsilons), num_batches, batch_size), dtype=np.bool)\n",
    "        for batch_index, (data, target) in enumerate(test_loader):\n",
    "            if batch_index < num_batches:\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                images, labels = ep.astensors(*(data, target))\n",
    "                if model_type == 'CNN':\n",
    "                    images = images.squeeze().expand_dims(axis=1)\n",
    "                else:\n",
    "                    images = images.reshape((batch_size, 784))\n",
    "                print('\\n', '~' * 79)\n",
    "                print(f'Model type: {model_type} [{model_index+1} out of {len(model_types)}]')\n",
    "                print(f'Batch {batch_index+1} out of {num_batches}')\n",
    "                print(f'accuracy {accuracy(fmodel, images, labels)}')\n",
    "                for attack_index, (attack, epsilons) in enumerate(attacks):\n",
    "                    advs, inputs, success = attack(fmodel, images, labels, epsilons=epsilons)\n",
    "                    assert success.shape == (len(epsilons), len(images))\n",
    "                    success_ = success.numpy()\n",
    "                    assert success_.dtype == np.bool\n",
    "                    attack_success[attack_index, :, batch_index, :] = success_\n",
    "                    print('\\n', attack)\n",
    "                    print('  ', 1.0 - success_.mean(axis=-1).round(2))\n",
    "                robust_accuracy = 1.0 - attack_success[:, :, batch_index, :].max(axis=0).mean(axis=-1)\n",
    "                #print('\\n', '-' * 79, '\\n')\n",
    "                #print('worst case (best attack per-sample)')\n",
    "                #print('  ', robust_accuracy.round(2))\n",
    "                #print('-' * 79)\n",
    "        attack_success = attack_success.reshape(\n",
    "            (len(attacks), len(epsilons), num_batches * batch_size))\n",
    "        attack_types = [str(type(attack)).split('.')[-1][:-2] for attack, _ in attacks]\n",
    "        epsilons = [epsilons for attack, epsilons in attacks]\n",
    "        out_dict = {\n",
    "            'adversarial_analysis':attack_success,\n",
    "            'attack_types':attack_types,\n",
    "            'epsilons':epsilons,\n",
    "            'attack_params':attack_params}\n",
    "        attack_results.append(out_dict)\n",
    "        if save_results:\n",
    "            np.savez('adv_attack_results.npz', data=attack_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_abs = False\n",
    "\n",
    "if(plot_abs):\n",
    "    abs_filename = os.path.join(\n",
    "        *[ROOT_DIR, 'analysis-by-synthesis', 'figures', 'Linf_accuracy_distortion_curves.pickle'])\n",
    "    with open(abs_filename, 'rb') as file:\n",
    "        abs_linf_pgd_accuracies = pickle.load(file)\n",
    "\n",
    "fig, axes = plot.subplots(ncols=len(attacks), nrows=1)#, share=0)\n",
    "handles = []\n",
    "for model_idx, (results_dict, model_type) in enumerate(zip(attack_results, model_types)):\n",
    "    for attack_idx in range(len(attacks)):\n",
    "        score = results_dict['adversarial_analysis'][attack_idx, ...]\n",
    "        attack_accuracy = 1.0 - score.mean(axis=-1)\n",
    "        y_vals = 100*attack_accuracy\n",
    "        x_vals = results_dict['epsilons'][attack_idx]\n",
    "        handle = axes[attack_idx].plot(x_vals, y_vals, label=model_type)\n",
    "        if(attack_idx == 0):\n",
    "            handles.extend(handle)\n",
    "        if(plot_abs):\n",
    "            if(model_idx == 0 and attack_idx == 0):\n",
    "                for abs_model_type, abs_model_accuracy in abs_linf_pgd_accuracies.items():\n",
    "                    if(abs_model_type not in ['Binary CNN', 'Nearest Neighbor', 'Binary ABS', 'CNN']):\n",
    "                        handle = axes[attack_idx].plot(\n",
    "                            abs_model_accuracy['x'], abs_model_accuracy['y'], label=abs_model_type)\n",
    "                        handles.extend(handle)\n",
    "        axes[attack_idx].format(title=results_dict['attack_types'][attack_idx])\n",
    "        axes[attack_idx].format(\n",
    "            xlabel='Maximum perturbation size',\n",
    "            xlim=[0.0, np.max(x_vals)],\n",
    "            ylim=[0, 100])\n",
    "axes.format(ylabel='Model accuracy')\n",
    "fig.legend(handles, ncols=1, frame=False, label='Model type', loc='right')\n",
    "plot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
