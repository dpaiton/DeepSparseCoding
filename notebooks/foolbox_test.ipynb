{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "while 'DeepSparseCoding' in ROOT_DIR:\n",
    "    ROOT_DIR = os.path.dirname(ROOT_DIR)\n",
    "if ROOT_DIR not in sys.path: sys.path.append(ROOT_DIR)\n",
    "    \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import proplot as plot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "from DeepSparseCoding.utils.file_utils import Logger\n",
    "import DeepSparseCoding.utils.loaders as loaders\n",
    "import DeepSparseCoding.utils.run_utils as run_utils\n",
    "import DeepSparseCoding.utils.dataset_utils as dataset_utils\n",
    "import DeepSparseCoding.utils.run_utils as ru\n",
    "import DeepSparseCoding.utils.plot_functions as pf\n",
    "import DeepSparseCoding.utils.data_processing as dp\n",
    "\n",
    "import eagerpy as ep\n",
    "from foolbox import PyTorchModel, accuracy, samples\n",
    "import foolbox.attacks as fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_mnist_fb() -> PyTorchModel:\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 32, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Dropout2d(0.25),\n",
    "        nn.Flatten(),  # type: ignore\n",
    "        nn.Linear(9216, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(0.5),\n",
    "        nn.Linear(128, 10),\n",
    "        #nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "    #path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"mnist_cnn.pth\")\n",
    "    path = os.path.join(*[ROOT_DIR, 'DeepSparseCoding', 'mnist_cnn.pth'])\n",
    "    model.load_state_dict(torch.load(path))  # type: ignore\n",
    "    model.eval()\n",
    "    #preprocessing = dict(mean=0.1307, std=0.3081)\n",
    "    fmodel = PyTorchModel(model, bounds=(0, 1))#, preprocessing=preprocessing)\n",
    "    return fmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mnist_dsc(log_file, cp_file):\n",
    "    logger = Logger(log_file, overwrite=False)\n",
    "    log_text = logger.load_file()\n",
    "    params = logger.read_params(log_text)[-1]\n",
    "    params.cp_latest_filename = cp_file\n",
    "    params.standardize_data = False\n",
    "    params.rescale_data_to_one = True\n",
    "    train_loader, val_loader, test_loader, data_params = dataset_utils.load_dataset(params)\n",
    "    for key, value in data_params.items():\n",
    "        setattr(params, key, value)\n",
    "    model = loaders.load_model(params.model_type)\n",
    "    model.setup(params, logger)\n",
    "    model.params.analysis_out_dir = os.path.join(\n",
    "        *[model.params.model_out_dir, 'analysis', model.params.version])\n",
    "    model.params.analysis_save_dir = os.path.join(model.params.analysis_out_dir, 'savefiles')\n",
    "    if not os.path.exists(model.params.analysis_save_dir):\n",
    "        os.makedirs(model.params.analysis_save_dir)\n",
    "    model.to(params.device)\n",
    "    model.load_checkpoint()\n",
    "    fmodel = PyTorchModel(model.eval(), bounds=(0, 1))\n",
    "    return fmodel, model, test_loader, model.params.batch_size, model.params.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_files = [\n",
    "    os.path.join(*[ROOT_DIR, 'Torch_projects', 'mlp_768_mnist', 'logfiles', 'mlp_768_mnist_v0.log']),\n",
    "    os.path.join(*[ROOT_DIR, 'Torch_projects', 'lca_768_mlp_mnist', 'logfiles', 'lca_768_mlp_mnist_v0.log'])\n",
    "    ]\n",
    "\n",
    "cp_latest_filenames = [\n",
    "    os.path.join(*[ROOT_DIR,'Torch_projects', 'mlp_768_mnist', 'checkpoints', 'mlp_768_mnist_latest_checkpoint_v0.pt']),\n",
    "    os.path.join(*[ROOT_DIR, 'Torch_projects', 'lca_768_mlp_mnist', 'checkpoints', 'lca_768_mlp_mnist_latest_checkpoint_v0.pt'])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel_mlp, dmodel_mlp, test_loader, batch_size, device = create_mnist_dsc(log_files[0], cp_latest_filenames[0])\n",
    "fmodel_mlp.type = 'MLP'\n",
    "fmodel_cnn = create_mnist_fb()\n",
    "fmodel_cnn.type = 'CNN'\n",
    "fmodel_lca, dmodel_lca = create_mnist_dsc(log_files[1], cp_latest_filenames[1])[:2]\n",
    "fmodel_lca.type = 'LCA'\n",
    "model_list = {fmodel_mlp.type:fmodel_mlp, fmodel_cnn.type:fmodel_cnn, fmodel_lca.type:fmodel_lca}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(weights, title=\"\", figsize=None):\n",
    "    \"\"\"\n",
    "        weights: [np.ndarray] of shape [num_outputs, num_input_y, num_input_x]\n",
    "        The matrices are renormalized before plotting.\n",
    "    \"\"\"\n",
    "    #weights = dp.norm_weights(weights)\n",
    "    vmin = np.min(weights)\n",
    "    vmax = np.max(weights)\n",
    "    num_plots = weights.shape[0]\n",
    "    num_plots_y = int(np.ceil(np.sqrt(num_plots))+1)\n",
    "    num_plots_x = int(np.floor(np.sqrt(num_plots)))\n",
    "    fig, sub_ax = plt.subplots(num_plots_y, num_plots_x, figsize=figsize)\n",
    "    filter_total = 0\n",
    "    for plot_id in  np.ndindex((num_plots_y, num_plots_x)):\n",
    "        if filter_total < num_plots:\n",
    "            sub_ax[plot_id].imshow(np.squeeze(weights[filter_total, ...]), vmin=vmin, vmax=vmax, cmap=\"Greys_r\")\n",
    "            filter_total += 1\n",
    "        clear_axis(sub_ax[plot_id])\n",
    "        sub_ax[plot_id].set_aspect(\"equal\")\n",
    "    fig.suptitle(title, y=0.95, x=0.5, fontsize=20)\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(data, pad_values=1):\n",
    "    \"\"\"\n",
    "    Pad data with ones for visualization\n",
    "    Outputs:\n",
    "        padded version of input\n",
    "    Inputs:\n",
    "        data: np.ndarray\n",
    "        pad_values: [int] specifying what value will be used for padding\n",
    "    \"\"\"\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = (((0, n ** 2 - data.shape[0]),\n",
    "        (1, 1), (1, 1)) # add some space between filters\n",
    "        + ((0, 0),) * (data.ndim - 3)) # don't pad last dimension (if there is one)\n",
    "    padded_data = np.pad(data, padding, mode=\"constant\",\n",
    "        constant_values=pad_values)\n",
    "    # tile the filters into an image\n",
    "    padded_data = padded_data.reshape((\n",
    "        (n, n) + padded_data.shape[1:])).transpose((\n",
    "        (0, 2, 1, 3) + tuple(range(4, padded_data.ndim + 1))))\n",
    "    padded_data = padded_data.reshape((n * padded_data.shape[1],\n",
    "        n * padded_data.shape[3]) + padded_data.shape[4:])\n",
    "    return padded_data\n",
    "\n",
    "def normalize_data_with_max(data):\n",
    "    \"\"\"\n",
    "    Normalize data by dividing by abs(max(data))\n",
    "    Inputs:\n",
    "        data: [np.ndarray] data to be normalized\n",
    "    Outputs:\n",
    "        norm_data: [np.ndarray] normalized data\n",
    "        data_max: [float] max that was divided out\n",
    "    \"\"\"\n",
    "    data_max = np.max(np.abs(data))\n",
    "    if data_max > 0:\n",
    "        norm_data = data / data_max\n",
    "    else:\n",
    "        norm_data = data\n",
    "    return norm_data, data_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = dmodel_mlp.fc0_w.cpu().detach().numpy()\n",
    "num_neurons, num_pixels = weights.shape\n",
    "weights = weights.reshape((num_neurons, int(np.sqrt(num_pixels)), int(np.sqrt(num_pixels)), 1))\n",
    "weights = normalize_data_with_max(weights)[0]\n",
    "weights = pad_data(weights)\n",
    "fig, ax = plot.subplots(figsize=(10, 10))\n",
    "ax = pf.clear_axis(ax)\n",
    "axis_image = ax.imshow(np.squeeze(weights), cmap='greys_r', interpolation=\"nearest\")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = dmodel_lca.lca.w.cpu().detach().numpy().T\n",
    "num_plots, num_pixels = weights.shape\n",
    "weights = weights.reshape((num_plots, int(np.sqrt(num_pixels)), int(np.sqrt(num_pixels)), 1))\n",
    "weights = normalize_data_with_max(weights)[0]\n",
    "weights = pad_data(weights)\n",
    "fig, ax = plot.subplots(figsize=(10, 10))\n",
    "ax = pf.clear_axis(ax)\n",
    "axis_image = ax.imshow(np.squeeze(weights), cmap='greys_r', interpolation=\"nearest\")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results = True\n",
    "load_results = True\n",
    "\n",
    "num_batches =  10#len(test_loader.dataset) // batch_size\n",
    "\n",
    "attack_params = {\n",
    "    'LinfPGD': {\n",
    "        'random_start':False,\n",
    "        'abs_stepsize':0.002,\n",
    "        'steps':500\n",
    "    },\n",
    "    'L2PGD': {\n",
    "        'random_start':False,\n",
    "        'abs_stepsize':0.45,\n",
    "        'steps':2000\n",
    "    }\n",
    "}\n",
    "\n",
    "linf_epsilons = [ # allowed perturbation size\n",
    "    0.0,\n",
    "    0.03,\n",
    "    0.06,\n",
    "    0.09,\n",
    "    0.1,\n",
    "    0.13,\n",
    "    0.16,\n",
    "    0.19,\n",
    "    0.2,\n",
    "    0.23,\n",
    "    0.26,\n",
    "    0.29,\n",
    "    0.3,\n",
    "    0.33,\n",
    "    0.36,\n",
    "    0.39,\n",
    "    0.4\n",
    "]\n",
    "\n",
    "l2_epsilons = [10 * eps for eps in linf_epsilons]\n",
    "\n",
    "attacks = [\n",
    "    (fa.LinfPGD(**attack_params['LinfPGD']), linf_epsilons),\n",
    "    (fa.L2PGD(**attack_params['L2PGD']), l2_epsilons),\n",
    "    #fa.FGSM(),\n",
    "    #fa.LinfBasicIterativeAttack(),\n",
    "    #fa.LinfAdditiveUniformNoiseAttack(),\n",
    "    #fa.LinfDeepFoolAttack(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if load_results:\n",
    "    attack_results = []\n",
    "    for model_index, (model_type, fmodel) in enumerate(model_list.items()):\n",
    "        attack_results.append(np.load(f'adv_attack_results_{model_type}.npz', allow_pickle=True)['data'].item())\n",
    "else:\n",
    "    attack_results = []\n",
    "    for model_index, (model_type, fmodel) in enumerate(model_list.items()):\n",
    "        attack_success = np.zeros(\n",
    "                (len(attacks), len(linf_epsilons), num_batches, batch_size), dtype=np.bool)\n",
    "        for batch_index, (data, target) in enumerate(test_loader):\n",
    "            if batch_index < num_batches:\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                images, labels = ep.astensors(*(data, target))\n",
    "                if model_type == 'CNN':\n",
    "                    images = images.squeeze().expand_dims(axis=1)\n",
    "                else:\n",
    "                    images = images.reshape((batch_size, 784))\n",
    "                print('\\n', '~' * 79)\n",
    "                print(f'Model type: {model_type} [{model_index+1} out of {len(model_list)}]')\n",
    "                print(f'Batch {batch_index+1} out of {num_batches}')\n",
    "                print(f'accuracy {accuracy(fmodel, images, labels)}')\n",
    "                for attack_index, (attack, epsilons) in enumerate(attacks):\n",
    "                    advs, _, success = attack(fmodel, images, labels, epsilons=epsilons)\n",
    "                    assert success.shape == (len(epsilons), len(images))\n",
    "                    success_ = success.numpy()\n",
    "                    assert success_.dtype == np.bool\n",
    "                    attack_success[attack_index, :, batch_index, :] = success_\n",
    "                    print('\\n', attack)\n",
    "                    print('  ', 1.0 - success_.mean(axis=-1).round(2))\n",
    "                robust_accuracy = 1.0 - attack_success[:, :, batch_index, :].max(axis=0).mean(axis=-1)\n",
    "                #print('\\n', '-' * 79, '\\n')\n",
    "                #print('worst case (best attack per-sample)')\n",
    "                #print('  ', robust_accuracy.round(2))\n",
    "                #print('-' * 79)\n",
    "        attack_success = attack_success.reshape(\n",
    "            (len(attacks), len(epsilons), num_batches * batch_size))\n",
    "        attack_types = []\n",
    "        epsilon_list = []\n",
    "        for attack, epsilons in attacks:\n",
    "            attack_types.append(str(type(attack)).split('.')[-1][:-2])\n",
    "            epsilon_list.append(epsilons)\n",
    "        out_dict = {\n",
    "            'num_batches':num_batches,\n",
    "            'batch_size':batch_size,\n",
    "            'adversarial_analysis':attack_success,\n",
    "            'attack_types':attack_types,\n",
    "            'epsilons':epsilon_list,\n",
    "            'attack_params':attack_params}\n",
    "        attack_results.append(out_dict)\n",
    "        if save_results:\n",
    "            np.savez(f'adv_attack_results_{model_type}.npz', data=out_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_abs = False\n",
    "\n",
    "if(plot_abs):\n",
    "    abs_filename = os.path.join(\n",
    "        *[ROOT_DIR, 'analysis-by-synthesis', 'figures', 'Linf_accuracy_distortion_curves.pickle'])\n",
    "    with open(abs_filename, 'rb') as file:\n",
    "        abs_linf_pgd_accuracies = pickle.load(file)\n",
    "\n",
    "fig, axes = plot.subplots(ncols=len(attacks), nrows=1)#, share=0)\n",
    "handles = []\n",
    "for model_idx, (results_dict, model_type) in enumerate(zip(attack_results, model_list.keys())):\n",
    "    for attack_idx in range(len(attacks)):\n",
    "        score = results_dict['adversarial_analysis'][attack_idx, ...]\n",
    "        attack_accuracy = 1.0 - score.mean(axis=-1)\n",
    "        y_vals = 100*attack_accuracy\n",
    "        x_vals = results_dict['epsilons'][attack_idx]\n",
    "        handle = axes[attack_idx].plot(x_vals, y_vals, label=model_type)\n",
    "        if(attack_idx == 0):\n",
    "            handles.extend(handle)\n",
    "        if(plot_abs):\n",
    "            if(model_idx == 0 and attack_idx == 0):\n",
    "                for abs_model_type, abs_model_accuracy in abs_linf_pgd_accuracies.items():\n",
    "                    if(abs_model_type not in ['Binary CNN', 'Nearest Neighbor', 'Binary ABS', 'CNN']):\n",
    "                        handle = axes[attack_idx].plot(\n",
    "                            abs_model_accuracy['x'], abs_model_accuracy['y'], label=abs_model_type)\n",
    "                        handles.extend(handle)\n",
    "        axes[attack_idx].format(title=results_dict['attack_types'][attack_idx])\n",
    "        axes[attack_idx].format(\n",
    "            xlabel='Maximum perturbation size',\n",
    "            xlim=[0.0, np.max(x_vals)],\n",
    "            ylim=[0, 100])\n",
    "axes.format(ylabel='Model accuracy')\n",
    "fig.legend(handles, ncols=1, frame=False, label='Model type', loc='right')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Any, Optional, Callable, Tuple\n",
    "\n",
    "from foolbox.types import Bounds\n",
    "from foolbox.attacks.base import T\n",
    "from foolbox.models.base import Model\n",
    "from foolbox.criteria import Misclassification\n",
    "from foolbox.attacks.base import raise_if_kwargs\n",
    "from foolbox.attacks.base import get_criterion\n",
    "from foolbox.attacks.projected_gradient_descent import LinfProjectedGradientDescentAttack\n",
    "\n",
    "class LinfProjectedGradientDescentAttackWithStopping(LinfProjectedGradientDescentAttack):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        rel_stepsize: float = 0.025,\n",
    "        abs_stepsize: Optional[float] = None,\n",
    "        steps: int = 50,\n",
    "        random_start: bool = True,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            rel_stepsize=rel_stepsize,\n",
    "            abs_stepsize=abs_stepsize,\n",
    "            steps=steps,\n",
    "            random_start=random_start,\n",
    "        )\n",
    "        \n",
    "    def normalize(\n",
    "        self, gradients: ep.Tensor, *, x: ep.Tensor, bounds: Bounds\n",
    "    ) -> ep.Tensor:\n",
    "        return gradients.sign()\n",
    "        \n",
    "    def run(\n",
    "        self,\n",
    "        model: Model,\n",
    "        inputs: T,\n",
    "        criterion: Union[Misclassification, T],\n",
    "        *,\n",
    "        epsilon: float,\n",
    "        **kwargs: Any,\n",
    "    ) -> T:\n",
    "        raise_if_kwargs(kwargs)\n",
    "        x0, restore_type = ep.astensor_(inputs)\n",
    "        criterion_ = get_criterion(criterion)\n",
    "        del inputs, criterion, kwargs\n",
    "\n",
    "        if not isinstance(criterion_, Misclassification):\n",
    "            raise ValueError(\"unsupported criterion\")\n",
    "\n",
    "        labels = criterion_.labels\n",
    "        loss_fn = self.get_loss_fn(model, labels)\n",
    "\n",
    "        if self.abs_stepsize is None:\n",
    "            stepsize = self.rel_stepsize * epsilon\n",
    "        else:\n",
    "            stepsize = self.abs_stepsize\n",
    "\n",
    "        x = x0\n",
    "\n",
    "        #np.savez('tmp_x0.npz', data=x0.numpy())\n",
    "        if self.random_start:\n",
    "            x = self.get_random_start(x0, epsilon)\n",
    "            x = ep.clip(x, *model.bounds)\n",
    "        else:\n",
    "            x = x0\n",
    "\n",
    "        confidence_threshold = 0.9\n",
    "        store_x = np.zeros_like(x)\n",
    "        store_time_step = -1*np.ones(x.shape[0], dtype=np.int32)\n",
    "        store_confidence = np.zeros(x.shape[0], dtype=np.float32)\n",
    "        all_kept_indices = []\n",
    "        time_step = 0\n",
    "        num_failed = 0\n",
    "        while len(set(all_kept_indices)) < x.shape[0]:\n",
    "            loss, gradients = self.value_and_grad(loss_fn, x)\n",
    "            gradients = self.normalize(gradients=gradients, x=x, bounds=model.bounds)\n",
    "            x = x + stepsize * gradients\n",
    "            x = self.project(x, x0, epsilon)\n",
    "            x = ep.clip(x, *model.bounds)\n",
    "            \n",
    "            # for targeted attacks\n",
    "            #adversarial_outputs = ep.softmax(model(x))\n",
    "            #adversarial_confidence = ep.take_along_axis(adversarial_outputs, labels[:,None], axis=1).numpy()\n",
    "            \n",
    "            # for untargeted attacks\n",
    "            adversarial_outputs = ep.softmax(model(x)).numpy().copy()\n",
    "            adversarial_outputs[np.arange(x.shape[0]), labels.numpy()] = 0 # zero confidence at true label\n",
    "            adversarial_confidence = ep.max(adversarial_outputs, axis=1) # highest non-true label confidence\n",
    "            \n",
    "            all_above_thresh = np.nonzero(np.squeeze(adversarial_confidence>confidence_threshold))[0]\n",
    "            keep_indices = np.array([], dtype=np.int32)\n",
    "            for adv_index in all_above_thresh:\n",
    "                if adv_index not in set(all_kept_indices):\n",
    "                    keep_indices = np.append(keep_indices, adv_index)\n",
    "            all_kept_indices.extend(keep_indices)\n",
    "            store_x[keep_indices, ...] = x.numpy()[keep_indices, ...]\n",
    "            store_time_step[keep_indices] = time_step\n",
    "            store_confidence[keep_indices] = adversarial_confidence[keep_indices]\n",
    "            time_step += 1\n",
    "            if time_step == self.steps-1:\n",
    "                num_failed = x.shape[0] - len(set(all_kept_indices))\n",
    "                print(f'Max steps = {self.steps} reached for model {model.type}, {num_failed} images did not achieve adversarial confidence threshold of {confidence_threshold}')\n",
    "                #import IPython; IPython.embed(); raise SystemExit\n",
    "                break\n",
    "        remaining_indices = np.array([val for val in np.arange(x.shape[0], dtype=np.int32) if val not in all_kept_indices])\n",
    "        if len(remaining_indices) > 0:\n",
    "            store_confidence[remaining_indices] = adversarial_confidence[remaining_indices]\n",
    "            store_x[remaining_indices, ...] = x[remaining_indices, ...]\n",
    "        reduc_dim = tuple(range(1, len(x.shape)))\n",
    "        msd = np.mean(np.square(store_x - x0.numpy()), axis=reduc_dim)\n",
    "        output_dict = {\n",
    "            'adversarial_images':store_x,\n",
    "            'adversarial_time_step':store_time_step,\n",
    "            'adversarial_confidence':store_confidence,\n",
    "            'confidence_threshold':confidence_threshold,\n",
    "            'failed_indices':remaining_indices,\n",
    "            'mean_squared_distances':msd,\n",
    "            'epsilon':epsilon,\n",
    "            'image_bounds':model.bounds,\n",
    "            'max_steps':self.steps,\n",
    "            'num_failed':num_failed\n",
    "        }\n",
    "        np.savez(f'confidence_attack_{model.type}.npz', data=output_dict)\n",
    "        return restore_type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attack_params = {\n",
    "    'LinfPGD': {\n",
    "        'random_start':False,\n",
    "        'abs_stepsize':0.005,\n",
    "        'steps':500 # maximum number of steps\n",
    "    }\n",
    "}\n",
    "epsilons = [1.0]\n",
    "\n",
    "attack = LinfProjectedGradientDescentAttackWithStopping(**attack_params['LinfPGD'])\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "test_images = test_images.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "test_images, test_labels = ep.astensors(*(test_images, test_labels))\n",
    "\n",
    "test_images = test_images.squeeze().expand_dims(axis=1)\n",
    "cnn_advs, _, cnn_success = attack(fmodel_cnn, test_images, test_labels, epsilons=epsilons)\n",
    "\n",
    "test_images = test_images.squeeze().reshape((batch_size, 784))\n",
    "mlp_advs, _, mlp_success = attack(fmodel_mlp, test_images, test_labels, epsilons=epsilons)\n",
    "lca_advs, _, lca_success = attack(fmodel_lca, test_images, test_labels, epsilons=epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_results = {}\n",
    "for model_type in model_list.keys():\n",
    "    adv_results[model_type] = np.load(f'confidence_attack_{model_type}.npz', allow_pickle=True)['data'].item()\n",
    "    \n",
    "lca_success_indices = np.argwhere(adv_results['LCA']['adversarial_time_step']>=0).squeeze()\n",
    "mlp_success_indices = np.argwhere(adv_results['MLP']['adversarial_time_step']>=0).squeeze()\n",
    "all_success_indices = np.union1d(lca_success_indices, mlp_success_indices)\n",
    "adv_results_list = [adv_results['LCA']['mean_squared_distances'][all_success_indices],\n",
    "    adv_results['MLP']['mean_squared_distances'][all_success_indices]]\n",
    "all_results = np.stack(adv_results_list, axis=-1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['LCA 2L;768N', 'MLP 2L;768N']\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    all_results,\n",
    "    columns=pd.Index(names, name='Model')\n",
    ")\n",
    "fig, axs = plot.subplots(ncols=1, axwidth=2.5)\n",
    "axs.format(grid=False, suptitle='Mean Squared Distances')\n",
    "ax = axs[0]\n",
    "obj1 = ax.boxplot(\n",
    "    data, lw=0.7, marker='.', fillcolor='gray5',\n",
    "    medianlw=1, mediancolor='k', meancolor='k', meanlw=1\n",
    ")\n",
    "ax.format(title='L inf', titleloc='uc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
