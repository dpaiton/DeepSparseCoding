{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Union, Any, Optional, Callable, Tuple\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "if ROOT_DIR not in sys.path: sys.path.append(ROOT_DIR)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import proplot as plot\n",
    "import eagerpy as ep\n",
    "import torch\n",
    "from scipy.stats import pearsonr as linear_correlation\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from DeepSparseCoding.tf1x.utils.logger import Logger as tfLogger\n",
    "import DeepSparseCoding.tf1x.analysis.analysis_picker as ap\n",
    "from DeepSparseCoding.tf1x.data.dataset import Dataset\n",
    "import DeepSparseCoding.tf1x.utils.data_processing as tfdp\n",
    "\n",
    "from DeepSparseCoding.utils.file_utils import Logger\n",
    "import DeepSparseCoding.utils.dataset_utils as dataset_utils\n",
    "import DeepSparseCoding.utils.loaders as loaders\n",
    "import DeepSparseCoding.utils.plot_functions as pf\n",
    "\n",
    "import foolbox\n",
    "from foolbox import PyTorchModel\n",
    "from foolbox.attacks.projected_gradient_descent import LinfProjectedGradientDescentAttack\n",
    "from foolbox.types import Bounds\n",
    "from foolbox.models.base import Model\n",
    "from foolbox.attacks.base import T\n",
    "from foolbox.criteria import Misclassification\n",
    "from foolbox.attacks.base import raise_if_kwargs\n",
    "from foolbox.attacks.base import get_criterion\n",
    "\n",
    "rand_state = np.random.RandomState(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PyTorch Foolbox models & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithTemperature(nn.Module):\n",
    "    \"\"\"\n",
    "    A thin decorator, which wraps a model with temperature scaling\n",
    "    model (nn.Module):\n",
    "        A classification neural network\n",
    "        NB: Output of the neural network should be the classification logits,\n",
    "            NOT the softmax (or log softmax)!\n",
    "    \"\"\"\n",
    "    def __init__(self, model, init_temp):\n",
    "        super(ModelWithTemperature, self).__init__()\n",
    "        self.model = model\n",
    "        self.params = model.params\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * init_temp)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        logits = self.model.forward(input_tensor)\n",
    "        return self.temperature_scale(logits)\n",
    "\n",
    "    def temperature_scale(self, logits):\n",
    "        \"\"\"\n",
    "        Perform temperature scaling on logits\n",
    "        \"\"\"\n",
    "        # Expand temperature to match the size of logits\n",
    "        temperature = self.temperature.unsqueeze(1).expand(logits.size(0), logits.size(1))\n",
    "        return logits / temperature\n",
    "\n",
    "def bin_conf_acc_prop(softmaxes, labels, bin_boundaries):\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    confidences, predictions = softmaxes.max(axis=1)\n",
    "    #accuracies = predictions == labels\n",
    "    accuracies = predictions.eq(labels)\n",
    "    bin_confidence = []\n",
    "    bin_accuracy = []\n",
    "    bin_prop = []\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        # Calculated |confidence - accuracy| in each bin\n",
    "        in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "        bin_prop.append(in_bin.float().mean())\n",
    "        if bin_prop[-1].item() > 0:\n",
    "            bin_accuracy.append(accuracies[in_bin].float().mean())\n",
    "            bin_confidence.append(confidences[in_bin].mean())\n",
    "    return bin_confidence, bin_accuracy, bin_prop\n",
    "\n",
    "class _ECELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculates the Expected Calibration Error of a model.\n",
    "    (This isn't necessary for temperature scaling, just a cool metric).\n",
    "    The input to this loss is the logits of a model, NOT the softmax scores.\n",
    "    This divides the confidence outputs into equally-sized interval bins.\n",
    "    In each bin, we compute the confidence gap:\n",
    "    bin_gap = | avg_confidence_in_bin - accuracy_in_bin |\n",
    "    We then return a weighted average of the gaps, based on the number\n",
    "    of samples in each bin\n",
    "    See: Naeini, Mahdi Pakdaman, Gregory F. Cooper, and Milos Hauskrecht.\n",
    "    \"Obtaining Well Calibrated Probabilities Using Bayesian Binning.\" AAAI.\n",
    "    2015.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_bins=15):\n",
    "        \"\"\"\n",
    "        n_bins (int): number of confidence interval bins\n",
    "        \"\"\"\n",
    "        super(_ECELoss, self).__init__()\n",
    "        self.bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        softmaxes = F.softmax(logits, dim=1)\n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        accuracies = predictions.eq(labels)\n",
    "        ece = torch.zeros(1, device=logits.device)\n",
    "        bin_stats = bin_conf_acc_prop(softmaxes, labels, self.bin_boundaries)\n",
    "        for avg_confidence_in_bin, accuracy_in_bin, prop_in_bin in zip(*bin_stats):\n",
    "            if prop_in_bin.item() > 0:\n",
    "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "        return ece\n",
    "\n",
    "def set_temperature(model, valid_loader, device):\n",
    "        \"\"\"\n",
    "        Tune the tempearature of the model (using the validation set).\n",
    "        We're going to set it to optimize NLL.\n",
    "        valid_loader (DataLoader): validation set loader\n",
    "        \"\"\"\n",
    "        nll_criterion = nn.CrossEntropyLoss().to(device)\n",
    "        ece_criterion = _ECELoss().to(device)\n",
    "        # First: collect all the logits and labels for the validation set\n",
    "        logits_list = []\n",
    "        labels_list = []\n",
    "        with torch.no_grad():\n",
    "            for input_tensor, label_tensor in valid_loader:\n",
    "                input_tensor = input_tensor.reshape((input_tensor.shape[0], 784))\n",
    "                input_tensor = input_tensor.to(device)\n",
    "                logits = model(input_tensor)\n",
    "                logits_list.append(logits)\n",
    "                labels_list.append(label_tensor)\n",
    "            logits = torch.cat(logits_list).to(device)\n",
    "            labels = torch.cat(labels_list).to(device)\n",
    "        # Calculate NLL and ECE before temperature scaling\n",
    "        before_temperature_nll = nll_criterion(logits, labels).item()\n",
    "        before_temperature_ece = ece_criterion(logits, labels).item()\n",
    "        print('Before temperature - NLL: %.3f, ECE: %.3f' % (before_temperature_nll, before_temperature_ece))\n",
    "        # Next: optimize the temperature w.r.t. NLL\n",
    "        optimizer = optim.LBFGS([model.temperature], lr=0.01, max_iter=50)\n",
    "        def eval():\n",
    "            loss = nll_criterion(model.temperature_scale(logits), labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimizer.step(eval)\n",
    "        # Calculate NLL and ECE after temperature scaling\n",
    "        after_temperature_nll = nll_criterion(model.temperature_scale(logits), labels).item()\n",
    "        after_temperature_ece = ece_criterion(model.temperature_scale(logits), labels).item()\n",
    "        print('Optimal temperature: %.3f' % model.temperature.item())\n",
    "        print('After temperature - NLL: %.3f, ECE: %.3f' % (after_temperature_nll, after_temperature_ece))\n",
    "        return model\n",
    "\n",
    "def create_mnist_dsc(log_file, cp_file, calibrate=False, init_temp=1.0):\n",
    "    logger = Logger(log_file, overwrite=False)\n",
    "    log_text = logger.load_file()\n",
    "    params = logger.read_params(log_text)[-1]\n",
    "    params.cp_latest_filename = cp_file\n",
    "    params.standardize_data = False\n",
    "    params.rescale_data_to_one = True\n",
    "    params.shuffle_data = False\n",
    "    params.batch_size = 50\n",
    "    train_loader, val_loader, test_loader, data_params = dataset_utils.load_dataset(params)\n",
    "    for key, value in data_params.items():\n",
    "        setattr(params, key, value)\n",
    "    model = loaders.load_model(params.model_type)\n",
    "    model.setup(params, logger)\n",
    "    model.params.analysis_out_dir = os.path.join(\n",
    "        *[model.params.model_out_dir, 'analysis', model.params.version])\n",
    "    model.params.analysis_save_dir = os.path.join(model.params.analysis_out_dir, 'savefiles')\n",
    "    if not os.path.exists(model.params.analysis_save_dir):\n",
    "        os.makedirs(model.params.analysis_save_dir)\n",
    "    model.load_checkpoint()\n",
    "    if calibrate:\n",
    "        model = ModelWithTemperature(model, init_temp)\n",
    "        model.to(params.device)\n",
    "        #model = set_temperature(model, test_loader, params.device)\n",
    "    else:\n",
    "        model.to(params.device)\n",
    "    fmodel = PyTorchModel(model.eval(), bounds=(0, 1))\n",
    "    fmodel.params = params\n",
    "    return fmodel, model, test_loader, model.params.batch_size, model.params.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_full_test_set = True\n",
    "calibrate = True\n",
    "fb_mlp_temp = 1.69\n",
    "fb_lca_temp = 1.50\n",
    "\n",
    "log_files = [\n",
    "    os.path.join(*[ROOT_DIR, 'Torch_projects', 'mlp_768_mnist', 'logfiles', 'mlp_768_mnist_v0.log']),\n",
    "    os.path.join(*[ROOT_DIR, 'Torch_projects', 'lca_768_mlp_mnist', 'logfiles', 'lca_768_mlp_mnist_v0.log'])\n",
    "]\n",
    "\n",
    "cp_latest_filenames = [\n",
    "    os.path.join(*[ROOT_DIR,'Torch_projects', 'mlp_768_mnist', 'checkpoints', 'mlp_768_mnist_latest_checkpoint_v0.pt']),\n",
    "    os.path.join(*[ROOT_DIR, 'Torch_projects', 'lca_768_mlp_mnist', 'checkpoints', 'lca_768_mlp_mnist_latest_checkpoint_v0.pt'])\n",
    "]\n",
    "\n",
    "fmodel_mlp, dsc_model_mlp, test_loader, batch_size, device = create_mnist_dsc(log_files[0], cp_latest_filenames[0], calibrate=calibrate, init_temp=fb_mlp_temp)\n",
    "if calibrate:\n",
    "    fmodel_mlp.model_type = 'MLP_calibrated'\n",
    "else:\n",
    "    fmodel_mlp.model_type = 'MLP'\n",
    "print(fmodel_mlp.model_type)\n",
    "\n",
    "fmodel_lca, dsc_model_lca = create_mnist_dsc(log_files[1], cp_latest_filenames[1], calibrate=calibrate, init_temp=fb_lca_temp)[:2]\n",
    "if calibrate:\n",
    "    fmodel_lca.model_type = 'LCA_calibrated'\n",
    "else:\n",
    "    fmodel_lca.model_type = 'LCA'\n",
    "print(fmodel_lca.model_type)\n",
    "\n",
    "fmodels = [fmodel_mlp, fmodel_lca]\n",
    "\n",
    "fb_image_batch, fb_label_batch = next(iter(test_loader))\n",
    "fb_image_batch = fb_image_batch.reshape((batch_size, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DeepSparseCoding analyzer & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class params(object):\n",
    "  def __init__(self):\n",
    "    self.device = \"/gpu:0\"\n",
    "    self.analysis_dataset = \"test\"\n",
    "    self.save_info = \"analysis_temp_\" + self.analysis_dataset\n",
    "    self.overwrite_analysis_log = False\n",
    "    self.do_class_adversaries = True\n",
    "    self.do_run_analysis = False\n",
    "    self.do_evals = False\n",
    "    self.do_basis_analysis = False\n",
    "    self.do_inference = False\n",
    "    self.do_atas = False \n",
    "    self.do_recon_adversaries = False\n",
    "    self.do_neuron_visualization = False\n",
    "    self.do_full_recon = False\n",
    "    self.do_orientation_analysis = False \n",
    "    self.do_group_recons = False\n",
    "    \n",
    "    #Adversarial params\n",
    "    self.adversarial_attack_method = \"kurakin_targeted\"\n",
    "    self.adversarial_step_size = 0.005 # learning rate for optimizer\n",
    "    self.adversarial_num_steps = 500 # Number of iterations adversarial attacks\n",
    "    self.confidence_threshold = 0.9\n",
    "    self.adversarial_max_change = 1.0 # maximum size of adversarial perturation (epsilon)\n",
    "    self.carlini_change_variable = False # whether to use the change of variable trick from carlini et al\n",
    "    self.adv_optimizer = \"sgd\" # attack optimizer\n",
    "    self.adversarial_target_method = \"random\" # Not used if attack_method is untargeted#TODO support specified\n",
    "    self.adversarial_clip = True # whether or not to clip the final perturbed image\n",
    "    self.adversarial_clip_range = [0.0, 1.0] # Maximum range of image values\n",
    "    #self.carlini_recon_mult = 0.1#list(np.arange(.5, 1, .1))\n",
    "    self.adversarial_save_int = 1 # Interval at which to save adv examples to the npz file\n",
    "    self.eval_batch_size = 50 # batch size for computing adv examples\n",
    "    self.adversarial_input_id = None # Which adv images to use; None to use all\n",
    "    self.adversarial_target_labels = None # Parameter for \"specified\" target_method. Only for class attacks. Needs to be a list or numpy array of size [adv_batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_params = params()\n",
    "analysis_params.projects_dir = os.path.expanduser(\"~\")+\"/Work/Projects/\"\n",
    "\n",
    "#model_names = ['mlp_cosyne_mnist', 'slp_lca_768_latent_75_steps_mnist']\n",
    "#model_names = ['mlp_768_mnist', 'slp_lca_768_latent_mnist']\n",
    "model_names = ['mlp_1568_mnist', 'slp_lca_1568_latent_mnist']\n",
    "model_types = ['MLP', 'LCA']\n",
    "analyzers = []\n",
    "for model_type, model_name in zip(model_types, model_names):\n",
    "    analysis_params.model_name = model_name\n",
    "    analysis_params.version = '0.0'\n",
    "    analysis_params.model_dir = analysis_params.projects_dir+analysis_params.model_name\n",
    "    model_log_file = (analysis_params.model_dir+\"/logfiles/\"+analysis_params.model_name\n",
    "      +\"_v\"+analysis_params.version+\".log\")\n",
    "    model_logger = tfLogger(model_log_file, overwrite=False)\n",
    "    model_log_text = model_logger.load_file()\n",
    "    model_params = model_logger.read_params(model_log_text)[-1]\n",
    "    analysis_params.model_type = model_params.model_type\n",
    "    analyzer = ap.get_analyzer(analysis_params.model_type)\n",
    "    analysis_params.save_info = \"analysis_tmp_test_\" + analysis_params.analysis_dataset\n",
    "    analysis_params.save_info += (\n",
    "        \"_linf_\"+str(analysis_params.adversarial_max_change)\n",
    "        +\"_ss_\"+str(analysis_params.adversarial_step_size)\n",
    "        +\"_ns_\"+str(analysis_params.adversarial_num_steps)\n",
    "        +\"_pgd_targeted\"\n",
    "    )\n",
    "    analyzer.setup(analysis_params)\n",
    "    analyzer.model_type = model_type\n",
    "    analyzer.confidence_threshold = analysis_params.confidence_threshold\n",
    "    analyzers.append(analyzer)\n",
    "\n",
    "mnist_data = test_loader.dataset.data.numpy().astype(np.float32)\n",
    "mnist_data /= 255\n",
    "dsc_data = {\n",
    "    'test':Dataset(\n",
    "        np.expand_dims(mnist_data, axis=-1),\n",
    "        tfdp.dense_to_one_hot(test_loader.dataset.targets.numpy(), 10),\n",
    "        None,\n",
    "        rand_state\n",
    "    )\n",
    "}\n",
    "dsc_data = analyzers[0].model.reshape_dataset(dsc_data, analyzer.model_params)\n",
    "for analyzer in analyzers:\n",
    "    analyzer.model_params.data_shape = list(dsc_data[\"test\"].shape[1:])\n",
    "    analyzer.setup_model(analyzer.model_params)\n",
    "dsc_image_batch, dsc_label_batch, _ = dsc_data['test'].next_batch(batch_size, shuffle_data=False)\n",
    "dsc_data['test'].reset_counters()\n",
    "\n",
    "dsc_all_images = dsc_data['test'].images\n",
    "dsc_all_images = dsc_all_images.reshape((dsc_all_images.shape[0], 784))\n",
    "dsc_all_labels = dsc_data['test'].labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare DeepSparseCoding & Foolbox data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_idx = np.random.randint(batch_size)\n",
    "fig, axs = plot.subplots(ncols=3)\n",
    "im = axs[0].imshow(fb_image_batch.numpy()[img_idx,...].reshape(28, 28), cmap='greys_r')\n",
    "axs[0].format(title=f'PyTorch loader digit class {fb_label_batch[img_idx]}')\n",
    "axs[0].colorbar(im)\n",
    "im = axs[1].imshow(dsc_image_batch[img_idx,...].reshape(28, 28), cmap='greys_r')\n",
    "axs[1].format(title=f'DSC dataset digit class {tfdp.one_hot_to_dense(dsc_label_batch)[img_idx]}')\n",
    "axs[1].colorbar(im)\n",
    "diff_img = np.abs(dsc_image_batch[img_idx,...].reshape(28, 28) - fb_image_batch.numpy()[img_idx,...].reshape(28, 28))\n",
    "im = axs[2].imshow(diff_img, cmap='greys_r')\n",
    "axs[2].format(title=f'Difference image')\n",
    "axs[2].colorbar(im)\n",
    "pf.clear_axes(axs)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare DeepSparseCoding & Foolbox confidence and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for fmodel in fmodels:\n",
    "    fmodel.softmaxes = []\n",
    "    fmodel.logits = []\n",
    "    for input_tensor, label_tensor in test_loader:\n",
    "        input_tensor = input_tensor.reshape((input_tensor.shape[0], 784))\n",
    "        input_tensor = input_tensor.to(fmodel.params.device)\n",
    "        label_tensor = label_tensor.to(fmodel.params.device)\n",
    "        input_tensor, label_tensor = ep.astensors(input_tensor, label_tensor)\n",
    "        fmodel.logits.append(fmodel(input_tensor))\n",
    "        fmodel.softmaxes.append(torch.nn.functional.softmax(fmodel.logits[-1], dim=-1))\n",
    "    fmodel.softmaxes = ep.stack(fmodel.softmaxes, axis=0)\n",
    "    fmodel.num_batches, fmodel.batch_size, fmodel.num_classes = fmodel.softmaxes.shape\n",
    "    fmodel.softmaxes = fmodel.softmaxes.reshape((fmodel.num_batches*fmodel.batch_size, fmodel.num_classes)).numpy()\n",
    "    fmodel.logits = ep.stack(fmodel.logits, axis=0)\n",
    "    fmodel.logits = fmodel.logits.reshape((fmodel.num_batches*fmodel.batch_size, fmodel.num_classes)).numpy()\n",
    "\n",
    "for analyzer in analyzers:\n",
    "    print(analyzer.analysis_params.model_name)\n",
    "    analyzer.logits = np.squeeze(analyzer.compute_activations(dsc_all_images, batch_size=50, activation_operation=analyzer.model.get_logits))\n",
    "    dsc_data['test'].reset_counters()\n",
    "    print('bleh')\n",
    "    analyzer.softmaxes = np.squeeze(analyzer.compute_activations(dsc_all_images, batch_size=50, activation_operation=analyzer.model.get_label_est))\n",
    "    dsc_data['test'].reset_counters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_bins = 50\n",
    "bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "\n",
    "fig, axs = plot.subplots(ncols=4)\n",
    "for ax, model, codebase in zip(axs, fmodels+analyzers, ['FB', 'FB', 'DSC', 'DSC']):\n",
    "\n",
    "#fig, axs = plot.subplots(ncols=2)\n",
    "#for ax, model, codebase in zip(axs, analyzers, ['DSC', 'DSC']):\n",
    "\n",
    "#fig, axs = plot.subplots(ncols=2)\n",
    "#for ax, model, codebase in zip(axs, fmodels, ['FB', 'FB']):\n",
    "\n",
    "    confidence, accuracy, props = bin_conf_acc_prop(\n",
    "        torch.from_numpy(model.softmaxes).to(device),\n",
    "        test_loader.dataset.targets.to(device),\n",
    "        bin_boundaries\n",
    "    )\n",
    "    confidence = [conf.cpu().numpy() for conf in confidence]\n",
    "    accuracy = [acc.cpu().numpy() for acc in accuracy]\n",
    "    props = [prop.cpu().numpy() for prop in props]\n",
    "    ece = 0\n",
    "    for avg_confidence_in_bin, accuracy_in_bin, prop_in_bin in zip(confidence, accuracy, props):\n",
    "        ece += np.abs(avg_confidence_in_bin.item() - accuracy_in_bin.item()) * prop_in_bin.item()\n",
    "    ece *= 100\n",
    "    ax.scatter(confidence, accuracy, s=[prop*500 for prop in props if prop > 0], color='k')\n",
    "    ax.plot([0,1], [0,1], 'k--', linewidth=0.1)\n",
    "    ax.format(title=f'{codebase}_{model.model_type}\\nECE = {ece.round(3)}%')\n",
    "axs.format(\n",
    "    suptitle='Reliability of classifier confidence on test set',\n",
    "    xlabel='Confidence',\n",
    "    ylabel='Accuracy',\n",
    "    xlim=[0, 1],\n",
    "    ylim=[0, 1]\n",
    ")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_logit_forward = [fmodel.logits for fmodel in fmodels]\n",
    "fb_logit_forward = np.stack(fb_logit_forward, axis=0)\n",
    "fb_softmax_forward = [fmodel.softmaxes for fmodel in fmodels]\n",
    "fb_softmax_forward = np.stack(fb_softmax_forward, axis=0)\n",
    "\n",
    "dsc_logit_forward = [analyzer.logits for analyzer in analyzers]\n",
    "dsc_logit_forward = np.stack(dsc_logit_forward, axis=0)\n",
    "dsc_softmax_forward = [analyzer.softmaxes for analyzer in analyzers]\n",
    "dsc_softmax_forward = np.stack(dsc_softmax_forward, axis=0)\n",
    "\n",
    "all_softmax_results = np.concatenate((fb_softmax_forward.reshape(2, -1), dsc_softmax_forward.reshape(2, -1)), axis=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plot.subplots(ncols=2, nrows=2)\n",
    "axs[0,0].bar(np.arange(10), np.squeeze(fb_logit_forward[0, img_idx, :]))\n",
    "axs[0,0].format(title=f'FB_{fmodels[0].model_type}')\n",
    "axs[0,1].bar(np.arange(10), np.squeeze(fb_logit_forward[1, img_idx, :]))\n",
    "axs[0,1].format(title=f'FB_{fmodels[1].model_type}')\n",
    "axs[1,0].bar(np.arange(10), np.squeeze(dsc_logit_forward[0, img_idx, :]))\n",
    "axs[1,0].format(title=f'DSC_{analyzers[0].model_type}')\n",
    "axs[1,1].bar(np.arange(10), np.squeeze(dsc_logit_forward[1, img_idx, :]))\n",
    "axs[1,1].format(title=f'DSC_{analyzers[1].model_type}')\n",
    "axs.format(suptitle='Logit outputs for a single image', xtickminor=False, xticks=1)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plot.subplots(ncols=2, nrows=2)\n",
    "axs[0,0].bar(np.squeeze(fb_softmax_forward[0, img_idx, :]))\n",
    "axs[0,0].format(title=f'FB_{fmodels[0].model_type}')\n",
    "axs[0,1].bar(np.squeeze(fb_softmax_forward[1, img_idx, :]))\n",
    "axs[0,1].format(title=f'FB_{fmodels[1].model_type}')\n",
    "axs[1,0].bar(np.squeeze(dsc_softmax_forward[0, img_idx, :]))\n",
    "axs[1,0].format(title=f'DSC_{analyzers[0].model_type}')\n",
    "axs[1,1].bar(np.squeeze(dsc_softmax_forward[1, img_idx, :]))\n",
    "axs[1,1].format(title=f'DSC_{analyzers[1].model_type}')\n",
    "axs.format(suptitle='Softmax confidence for a single image', xtickminor=False, xticks=1, ylim=[0, 1])\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['FB_MLP', 'FB_LCA', 'DSC_MLP', 'DSC_LCA']\n",
    "data = pd.DataFrame(\n",
    "    all_softmax_results,\n",
    "    columns=pd.Index(names, name='Model')\n",
    ")\n",
    "fig, ax = plot.subplots(ncols=1, axwidth=2.5, share=0)\n",
    "ax.format(\n",
    "    grid=False,\n",
    "    suptitle='Softmax confidence for the test set' \n",
    ")\n",
    "obj1 = ax.boxplot(\n",
    "    data, linewidth=0.7, marker='.', fillcolor='gray5',\n",
    "    medianlw=1, mediancolor='k', meancolor='k', meanlw=1\n",
    ")\n",
    "ax.format(yscale='log', yformatter='sci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 100\n",
    "fig, axs = plot.subplots(ncols=2, nrows=2)\n",
    "for ax, model, atk_type in zip(axs, fmodels+analyzers, ['FB', 'FB', 'DSC', 'DSC']):\n",
    "    max_confidence = np.max(model.softmaxes, axis=1) # max is across categories, per image\n",
    "    conf_lim = [0, 1]\n",
    "    bins = np.linspace(conf_lim[0], conf_lim[1], num_bins)\n",
    "    count, bin_edges = np.histogram(max_confidence, bins)\n",
    "    bin_left, bin_right = bin_edges[:-1], bin_edges[1:]\n",
    "    bin_centers = bin_left + (bin_right - bin_left)/2\n",
    "    ax.bar(bin_centers, count, color='k')\n",
    "    mean_confidence = np.mean(max_confidence)\n",
    "    mean_idx = np.abs(bin_edges - mean_confidence).argmin()\n",
    "    mean_conf_bin = bin_edges[mean_idx].round(4)\n",
    "    ax.axvline(mean_conf_bin, lw=1, ls='--', color='r')\n",
    "    ax.format(\n",
    "        title=f'{atk_type}_{model.model_type}\\nMean confidence = {mean_conf_bin}',\n",
    "        yscale='log',\n",
    "        xlim=conf_lim\n",
    "    )\n",
    "axs.format(\n",
    "    suptitle='Softmax confidence on the clean test set correct label',\n",
    "    ylabel='Count',\n",
    "    xlabel='Confidence'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run DeepSparseCoding adversarial attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adv_indices(softmax_conf, all_kept_indices, confidence_threshold, num_images, labels):\n",
    "    softmax_conf[np.arange(num_images, dtype=np.int32), labels] = 0 # zero confidence at true label\n",
    "    confidence_indices = np.max(softmax_conf, axis=-1) # highest non-true label confidence\n",
    "    adversarial_labels = np.argmax(softmax_conf, axis=-1) # index of highest non-true label\n",
    "    all_above_thresh = np.nonzero(np.squeeze(confidence_indices>confidence_threshold))[0]\n",
    "    keep_indices = np.array([], dtype=np.int32)\n",
    "    for adv_index in all_above_thresh:\n",
    "        if adv_index not in set(all_kept_indices):\n",
    "            keep_indices = np.append(keep_indices, adv_index)\n",
    "    return keep_indices, confidence_indices, adversarial_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if run_full_test_set:\n",
    "#    data = dsc_all_images\n",
    "#    labels = dsc_all_labels\n",
    "#else:\n",
    "data = dsc_image_batch\n",
    "labels = dsc_label_batch\n",
    "\n",
    "\n",
    "for analyzer in analyzers:\n",
    "    analyzer.class_adversary_analysis(\n",
    "        data,\n",
    "        labels,\n",
    "        batch_size=analyzer.analysis_params.eval_batch_size,\n",
    "        input_id=analyzer.analysis_params.adversarial_input_id,\n",
    "        target_method = analyzer.analysis_params.adversarial_target_method,\n",
    "        target_labels = analyzer.analysis_params.adversarial_target_labels,\n",
    "        save_info=analyzer.analysis_params.save_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tfdp.one_hot_to_dense(labels.astype(np.int32))\n",
    "for analyzer in analyzers:\n",
    "    store_data = np.zeros_like(data)\n",
    "    store_time_step = -1*np.ones(data.shape[0], dtype=np.int32)\n",
    "    store_labels = np.zeros(data.shape[0], dtype=np.int32)\n",
    "    store_confidence = np.zeros(data.shape[0], dtype=np.float32)\n",
    "    store_mses = np.zeros(data.shape[0], dtype=np.float32)\n",
    "    all_kept_indices = []\n",
    "    for adv_step in range(1, analyzer.analysis_params.adversarial_num_steps+1): # first one is original\n",
    "        keep_indices, confidence_indices, adversarial_labels = get_adv_indices(\n",
    "            analyzer.adversarial_outputs[0, adv_step, ...],\n",
    "            all_kept_indices,\n",
    "            analyzer.confidence_threshold,\n",
    "            data.shape[0],\n",
    "            labels)\n",
    "        if keep_indices.size > 0:\n",
    "            all_kept_indices.extend(keep_indices)\n",
    "            store_data[keep_indices, ...] = analyzer.adversarial_images[0, adv_step, keep_indices, ...]\n",
    "            store_time_step[keep_indices] = adv_step\n",
    "            store_confidence[keep_indices] = confidence_indices[keep_indices]\n",
    "            store_mses[keep_indices] = analyzer.adversarial_input_adv_mses[0, adv_step, keep_indices]\n",
    "            store_labels[keep_indices] = adversarial_labels[keep_indices]\n",
    "    batch_indices = np.arange(data.shape[0], dtype=np.int32)[:,None]\n",
    "    failed_indices = np.array([val for val in batch_indices if val not in all_kept_indices])\n",
    "    if len(failed_indices) > 0:\n",
    "        store_confidence[failed_indices] = confidence_indices[failed_indices]\n",
    "        store_labels[failed_indices] = adversarial_labels[failed_indices]\n",
    "        store_data[failed_indices, ...] = data[failed_indices, ...]\n",
    "        store_mses[failed_indices] = analyzer.adversarial_input_adv_mses[0, -1, failed_indices]\n",
    "    analyzer.adversarial_images = [store_data]\n",
    "    analyzer.adversarial_time_step = [store_time_step]\n",
    "    analyzer.adversarial_confidence = [store_confidence]\n",
    "    analyzer.failed_indices = [failed_indices]\n",
    "    analyzer.success_indices = [list(set(all_kept_indices))]\n",
    "    analyzer.adversarial_labels = [store_labels]\n",
    "    analyzer.mean_squared_distances = [store_mses]\n",
    "    analyzer.num_failed = [data.shape[0] - len(set(all_kept_indices))]\n",
    "    print(f'model {analyzer.model_type} had {analyzer.num_failed} failed indices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Foolbox adversarial attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinfProjectedGradientDescentAttackWithStopping(LinfProjectedGradientDescentAttack):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        rel_stepsize: float = 0.025,\n",
    "        abs_stepsize: Optional[float] = None,\n",
    "        steps: int = 50,\n",
    "        random_start: bool = True,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            rel_stepsize=rel_stepsize,\n",
    "            abs_stepsize=abs_stepsize,\n",
    "            steps=steps,\n",
    "            random_start=random_start,\n",
    "        )\n",
    "        \n",
    "    #def project(self, x: ep.Tensor, x0: ep.Tensor, epsilon: float) -> ep.Tensor:\n",
    "    #    return x0 + ep.clip(x - x0, -epsilon, epsilon)\n",
    "    \n",
    "    def normalize(\n",
    "        self, gradients: ep.Tensor, *, x: ep.Tensor, bounds: Bounds\n",
    "    ) -> ep.Tensor:\n",
    "        return gradients.sign()\n",
    "        \n",
    "    def run(\n",
    "        self,\n",
    "        model: Model,\n",
    "        inputs: T,\n",
    "        criterion: Union[Misclassification, T],\n",
    "        *,\n",
    "        epsilon: float,\n",
    "        **kwargs: Any,\n",
    "    ) -> T:\n",
    "        raise_if_kwargs(kwargs)\n",
    "        x0, restore_type = ep.astensor_(inputs)\n",
    "        criterion_ = get_criterion(criterion)\n",
    "        del inputs, criterion, kwargs\n",
    "\n",
    "        if not isinstance(criterion_, Misclassification):\n",
    "            raise ValueError(\"unsupported criterion\")\n",
    "\n",
    "        labels = criterion_.labels\n",
    "        loss_fn = self.get_loss_fn(model, labels)\n",
    "\n",
    "        if self.abs_stepsize is None:\n",
    "            stepsize = self.rel_stepsize * epsilon\n",
    "        else:\n",
    "            stepsize = self.abs_stepsize\n",
    "\n",
    "        orig_x = x0.numpy().copy()\n",
    "        x = x0\n",
    "\n",
    "        if self.random_start:\n",
    "            x = self.get_random_start(x0, epsilon)\n",
    "            x = ep.clip(x, *model.bounds)\n",
    "        else:\n",
    "            x = x0\n",
    "        store_x = np.zeros_like(x, dtype=np.float32)\n",
    "        store_time_step = -1*np.ones(x.shape[0], dtype=np.int32)\n",
    "        store_labels = np.zeros(x.shape[0], dtype=np.int32)\n",
    "        store_confidence = np.zeros(x.shape[0], dtype=np.float32)\n",
    "        all_kept_indices = []\n",
    "        time_step = 0\n",
    "        num_failed = 0\n",
    "        while len(set(all_kept_indices)) < x.shape[0]:\n",
    "            loss, gradients = self.value_and_grad(loss_fn, x)\n",
    "            gradients = self.normalize(gradients=gradients, x=x, bounds=model.bounds)\n",
    "            x = x + stepsize * gradients\n",
    "            x = self.project(x, x0, epsilon)\n",
    "            x = ep.clip(x, *model.bounds)\n",
    "            keep_indices, confidence_indices, adversarial_labels = get_adv_indices(\n",
    "                ep.softmax(model(x)).numpy().copy(),\n",
    "                all_kept_indices,\n",
    "                model.confidence_threshold,\n",
    "                x.shape[0],\n",
    "                labels.numpy())\n",
    "            if keep_indices.size > 0:\n",
    "                all_kept_indices.extend(keep_indices)\n",
    "                store_x[keep_indices, ...] = x.numpy()[keep_indices, ...]\n",
    "                store_labels[keep_indices] = adversarial_labels[keep_indices]\n",
    "                store_time_step[keep_indices] = time_step\n",
    "                store_confidence[keep_indices] = confidence_indices[keep_indices]\n",
    "            time_step += 1\n",
    "            if time_step == self.steps-1:\n",
    "                num_failed = x.shape[0] - len(set(all_kept_indices))\n",
    "                print(f'Max steps = {self.steps} reached for model {model.model_type}, {num_failed} images did not achieve adversarial confidence threshold of {model.confidence_threshold}')\n",
    "                break\n",
    "        batch_indices = np.arange(x.shape[0], dtype=np.int32)[:,None]\n",
    "        failed_indices = np.array([val for val in batch_indices if val not in all_kept_indices])\n",
    "        if len(failed_indices) > 0:\n",
    "            store_confidence[failed_indices] = confidence_indices[failed_indices]\n",
    "            store_x[failed_indices, ...] = x[failed_indices, ...]\n",
    "        reduc_dim = tuple(range(1, len(orig_x.shape)))\n",
    "        msd = np.mean((store_x - orig_x)**2, axis=reduc_dim)\n",
    "        model.adversarial_images.append(store_x)\n",
    "        model.adversarial_time_step.append(store_time_step)\n",
    "        model.adversarial_labels.append(store_labels)\n",
    "        model.adversarial_confidence.append(store_confidence)\n",
    "        model.success_indices.append(np.array(all_kept_indices, dtype=np.int32))\n",
    "        model.failed_indices.append(failed_indices)\n",
    "        model.mean_squared_distances.append(msd)\n",
    "        model.num_failed.append(len(failed_indices))\n",
    "        return restore_type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attack_params = {\n",
    "    'LinfPGD': {\n",
    "        'random_start':False,\n",
    "        'abs_stepsize':analysis_params.adversarial_step_size,\n",
    "        'steps':analysis_params.adversarial_num_steps # maximum number of steps\n",
    "    }\n",
    "}\n",
    "epsilons = [analysis_params.adversarial_max_change]\n",
    "attack = LinfProjectedGradientDescentAttackWithStopping(**attack_params['LinfPGD'])\n",
    "\n",
    "for fmodel in fmodels:\n",
    "    fmodel.confidence_threshold = analysis_params.confidence_threshold\n",
    "    fmodel.adversarial_images = []\n",
    "    fmodel.adversarial_labels = []\n",
    "    fmodel.adversarial_time_step = []\n",
    "    fmodel.adversarial_confidence = []\n",
    "    fmodel.failed_indices = []\n",
    "    fmodel.mean_squared_distances = []\n",
    "    fmodel.num_failed = []\n",
    "    fmodel.success_indices = []\n",
    "    fmodel.success = []\n",
    "    for batch_idx, (input_tensor, label_tensor) in enumerate(test_loader):\n",
    "        if not run_full_test_set and batch_idx >= 1:\n",
    "            pass\n",
    "        input_tensor = input_tensor.reshape((input_tensor.shape[0], 784))\n",
    "        input_tensor = input_tensor.to(fmodel.params.device)\n",
    "        label_tensor = label_tensor.to(fmodel.params.device)\n",
    "        input_tensor, label_tensor = ep.astensors(input_tensor, label_tensor)\n",
    "        advs, _, success = attack(\n",
    "            fmodel,\n",
    "            input_tensor,\n",
    "            label_tensor,\n",
    "            epsilons=epsilons\n",
    "        )\n",
    "        fmodel.success.append(success.numpy())\n",
    "     \n",
    "    #fmodel.num_batches, fmodel.batch_size, fmodel.num_classes = fmodel.softmaxes.shape\n",
    "    fmodel.num_batches = batch_idx+1\n",
    "    \n",
    "    fmodel.adversarial_images = np.stack(fmodel.adversarial_images, axis=0)\n",
    "    fmodel.adversarial_images = fmodel.adversarial_images.reshape(\n",
    "        (fmodel.num_batches*fmodel.batch_size,\n",
    "         *list(fmodel.adversarial_images.shape[2:]))\n",
    "    )\n",
    "    \n",
    "    fmodel.adversarial_labels = np.stack(fmodel.adversarial_labels, axis=0)\n",
    "    fmodel.adversarial_labels = fmodel.adversarial_labels.reshape(\n",
    "        (fmodel.num_batches*fmodel.batch_size,\n",
    "         *list(fmodel.adversarial_labels.shape[2:]))\n",
    "    )\n",
    "    \n",
    "    fmodel.adversarial_time_step = np.stack(fmodel.adversarial_time_step, axis=0)\n",
    "    fmodel.adversarial_time_step = fmodel.adversarial_time_step.reshape(\n",
    "        (fmodel.num_batches*fmodel.batch_size,\n",
    "         *list(fmodel.adversarial_time_step.shape[2:]))\n",
    "    )\n",
    "    \n",
    "    fmodel.adversarial_confidence = np.stack(fmodel.adversarial_confidence, axis=0)\n",
    "    fmodel.adversarial_confidence = fmodel.adversarial_confidence.reshape(\n",
    "        (fmodel.num_batches*fmodel.batch_size,\n",
    "         *list(fmodel.adversarial_confidence.shape[2:]))\n",
    "    )\n",
    "    \n",
    "    fmodel.failed_indices = np.stack(fmodel.failed_indices, axis=0)\n",
    "    \n",
    "    fmodel.mean_squared_distances = np.stack(fmodel.mean_squared_distances, axis=0)\n",
    "    fmodel.mean_squared_distances = fmodel.mean_squared_distances.reshape(\n",
    "        (fmodel.num_batches*fmodel.batch_size,\n",
    "         *list(fmodel.mean_squared_distances.shape[2:]))\n",
    "    )\n",
    "    \n",
    "    fmodel.num_failed = np.stack(fmodel.num_failed, axis=0)\n",
    "    \n",
    "    fmodel.success_indices = np.stack(fmodel.success_indices, axis=0)\n",
    "    fmodel.success_indices = fmodel.success_indices.reshape(\n",
    "        (fmodel.num_batches*fmodel.batch_size,\n",
    "         *list(fmodel.success_indices.shape[2:]))\n",
    "    )\n",
    "    \n",
    "    fmodel.success = np.stack(fmodel.success, axis=0)\n",
    "    print(f'model {fmodel.model_type} had {fmodel.num_failed.sum()} failed indices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare DeepSparseCoding & Foolbox adversarial attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analyzer in analyzers:\n",
    "    analyzer.accuracy = analyzer.adversarial_clean_accuracy.item()\n",
    "    print(f'DSC {analyzer.model_type} clean accuracy = {analyzer.accuracy} and adv accuracy = {analyzer.adversarial_adv_accuracy}')\n",
    "    \n",
    "for fmodel in fmodels:\n",
    "    fmodel.accuracy = foolbox.accuracy(fmodel, fb_image_batch.to(device), fb_label_batch.to(device))\n",
    "    print(f'FB {fmodel.model_type} clean accuracy = {fmodel.accuracy} and adv accuracy = {1.0 - fmodel.success[0].mean(axis=-1).round(2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stars(p):\n",
    "    if p < 0.0001:\n",
    "        return '****'\n",
    "    elif (p < 0.001):\n",
    "        return '***'\n",
    "    elif (p < 0.01):\n",
    "        return '**'\n",
    "    elif (p < 0.05):\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'n.s.'\n",
    "\n",
    "names = ['MLP 2L;768N','LCA 2L;768N']\n",
    "\n",
    "fb_all_success_indices = np.intersect1d(*[fmodel.success_indices for fmodel in fmodels]).astype(np.int32)\n",
    "fb_adv_results_list = [np.array(fmodel.mean_squared_distances)[fb_all_success_indices] for fmodel in fmodels]\n",
    "fb_all_results = np.stack(fb_adv_results_list, axis=-1).squeeze()\n",
    "fb_dataframe = pd.DataFrame(\n",
    "    fb_all_results,\n",
    "    columns=pd.Index(names, name='Model')\n",
    ")\n",
    "\n",
    "dsc_all_success_indices = np.intersect1d(*[analyzer.success_indices for analyzer in analyzers]).astype(np.int32)\n",
    "dsc_adv_results_list = [analyzer.mean_squared_distances[0][dsc_all_success_indices] for analyzer in analyzers]\n",
    "dsc_all_results = np.stack(dsc_adv_results_list, axis=-1).squeeze()\n",
    "dsc_dataframe = pd.DataFrame(\n",
    "    dsc_all_results,\n",
    "    columns=pd.Index(names, name='Model')\n",
    ")\n",
    "\n",
    "dsc_p_value = linear_correlation(dsc_all_results[:,0], dsc_all_results[:,1])[1]\n",
    "fb_p_value = linear_correlation(fb_all_results[:,0], fb_all_results[:,1])[1]\n",
    "\n",
    "fig, axs = plot.subplots(ncols=2, axwidth=2.5, share=0)\n",
    "axs.format(grid=False, suptitle='L infinity Attack Mean Squared Distances')\n",
    "ax = axs[0]\n",
    "obj1 = ax.boxplot(\n",
    "    fb_dataframe, linewidth=0.7, marker='.', fillcolor='gray5',\n",
    "    medianlw=1, mediancolor='k', meancolor='k', meanlw=1\n",
    ")\n",
    "ax_y_max = max(ax.get_ylim())\n",
    "ax.text(0.5, ax_y_max-0.1*(ax_y_max), stars(fb_p_value),\n",
    "       horizontalalignment='center',\n",
    "       verticalalignment='center',\n",
    "       fontsize=14)\n",
    "ax.format(title='Foolbox')\n",
    "\n",
    "ax = axs[1]\n",
    "obj2 = ax.boxplot(\n",
    "    dsc_dataframe, linewidth=0.7, marker='.', fillcolor='gray5',\n",
    "    medianlw=1, mediancolor='k', meancolor='k', meanlw=1\n",
    ")\n",
    "ax_y_max = max(ax.get_ylim())\n",
    "ax.text(0.5, ax_y_max-0.1*(ax_y_max), stars(dsc_p_value),\n",
    "       horizontalalignment='center',\n",
    "       verticalalignment='center',\n",
    "       fontsize=24)\n",
    "ax.format(title='Deep Sparse Coding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attack images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plot.subplots(nrows=3, ncols=len(fmodels+analyzers))\n",
    "pf.clear_axes(axs)\n",
    "top_level = zip(fmodels+analyzers, fb_adv_results_list+dsc_adv_results_list, ['FB', 'FB', 'DSC', 'DSC'])\n",
    "for model_idx, (model, adv_results_list, atk_type) in enumerate(top_level):\n",
    "    if atk_type == 'DSC':\n",
    "        adv_imgs = model.adversarial_images[0]\n",
    "        adv_labels = model.adversarial_labels[0]\n",
    "    else:\n",
    "        adv_imgs = model.adversarial_images\n",
    "        adv_labels = model.adversarial_labels\n",
    "    adv_results = adv_results_list#[0]\n",
    "    adv_min_idx = np.abs(adv_results - adv_results.min()).argmin()\n",
    "    adv_mean_idx = np.abs(adv_results - adv_results.mean()).argmin()\n",
    "    adv_max_idx = np.abs(adv_results - adv_results.max()).argmin()\n",
    "    for row_idx, image_idx in enumerate([adv_min_idx, adv_mean_idx, adv_max_idx]):\n",
    "        img = adv_imgs[image_idx, ...].reshape(28, 28).astype(np.float32)\n",
    "        h = axs[row_idx, model_idx].imshow(img, cmap='grays')\n",
    "        axs[row_idx, model_idx].colorbar(h, loc='r', ticks=1)\n",
    "        axs[row_idx, model_idx].format(title=f'{atk_type}_{model.model_type} adversarial label = {adv_labels[row_idx]}')\n",
    "    axs[row_idx, 0].format(llabels=['Min MSD', 'Mean MSD', 'Max MSD'])\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
