{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "if ROOT_DIR not in sys.path: sys.path.append(ROOT_DIR)\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import proplot as plot\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from DeepSparseCoding.utils.file_utils import Logger\n",
    "import DeepSparseCoding.utils.run_utils as run_utils\n",
    "import DeepSparseCoding.utils.dataset_utils as dataset_utils\n",
    "import DeepSparseCoding.utils.loaders as loaders\n",
    "import DeepSparseCoding.utils.plot_functions as pf\n",
    "import DeepSparseCoding.utils.data_processing as dp\n",
    "from DeepSparseCoding.params.lca_cifar10_params import params as LcaParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_dir = '/mnt/qb/bethge/dpaiton/'\n",
    "model_name = 'lca_pool_lca_pool_cifar10'\n",
    "model_version = '0'\n",
    "log_file = workspace_dir + os.path.join(*['Projects', model_name, 'logfiles', f'{model_name}_v{model_version}.log'])\n",
    "logger = Logger(log_file, overwrite=False)\n",
    "log_text = logger.load_file()\n",
    "params = logger.read_params(log_text)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_stats = logger.read_stats(log_text)\n",
    "x_key = \"epoch\"\n",
    "y_keys = [key for key in list(model_stats.keys()) if 'test_' not in key]\n",
    "stats_fig = pf.plot_stats(model_stats, x_key, y_keys=y_keys)\n",
    "\n",
    "if 'test_epoch' in list(model_stats.keys()):\n",
    "    x_key = \"test_epoch\"\n",
    "    y_keys = [key for key in list(model_stats.keys()) if 'test_' in key]\n",
    "    test_stats_fig = pf.plot_stats(model_stats, x_key, y_keys=y_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = loaders.load_model(params.model_type)\n",
    "model.setup(params, logger)\n",
    "model.to(params.device)\n",
    "model_state_str = model.load_checkpoint()\n",
    "print(model_state_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, data_stats, data_mean_std = dataset_utils.load_dataset(params)\n",
    "train_mean_image = data_mean_std['dataset_mean_image'].to(model.params.device)\n",
    "train_std_image = data_mean_std['dataset_std_image'].to(model.params.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lca_weights = model.lca_1.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def normalize_data_with_max(data):\n",
    "    \"\"\"\n",
    "    Normalize data by dividing by abs(max(data))\n",
    "    If abs(max(data)) is zero, then output is zero\n",
    "    Inputs:\n",
    "        data: [np.ndarray] data to be normalized\n",
    "    Outputs:\n",
    "        norm_data: [np.ndarray] normalized data\n",
    "        data_max: [float] max that was divided out\n",
    "    \"\"\"\n",
    "    data_max = np.max(np.abs(data), axis=(1,2), keepdims=True)\n",
    "    norm_data = np.divide(data, data_max, out=np.zeros_like(data), where=data_max!=0)\n",
    "    return norm_data, data_max\n",
    "\n",
    "def pad_matrix_to_image(matrix, pad_size=0, pad_value=0, normalize=False):\n",
    "    if normalize:\n",
    "        #matrix = normalize_data_with_max(matrix)[0]\n",
    "        matrix = dp.rescale_data_to_one(torch.from_numpy(matrix), eps=1e-10, samplewise=True)[0].numpy()\n",
    "    num_weights, img_c, img_h, img_w = matrix.shape\n",
    "    #if img_c == 1:\n",
    "    #    matrix = matrix.squeeze()\n",
    "    #else:\n",
    "    #    # TODO: separate channels, pad each individual one, then recombine.\n",
    "    #    assert False, (f'Multiple color channels are not currently supported') \n",
    "    num_extra_images = int(np.ceil(np.sqrt(num_weights))**2 - num_weights)\n",
    "    matrices = []\n",
    "    for channel_idx in range(img_c):\n",
    "        channel_matrix = matrix[:, channel_idx, ...].copy()\n",
    "        if num_extra_images > 0:\n",
    "            channel_matrix = np.concatenate(\n",
    "                [channel_matrix, np.zeros((num_extra_images, img_h, img_w))], axis=0)\n",
    "        channel_matrix = np.pad(channel_matrix,\n",
    "            pad_width=((0,0), (num_pad_pix, num_pad_pix), (num_pad_pix, num_pad_pix)),\n",
    "            mode='constant', constant_values=pad_value)\n",
    "        padded_img_h, padded_img_w = channel_matrix.shape[1:]\n",
    "        num_edge_tiles = int(np.sqrt(channel_matrix.shape[0]))\n",
    "        tiles = channel_matrix.reshape(num_edge_tiles, num_edge_tiles, padded_img_h, padded_img_w)\n",
    "        tiles = tiles.swapaxes(1, 2)\n",
    "        matrices.append(tiles.reshape(num_edge_tiles * padded_img_h, num_edge_tiles * padded_img_w))\n",
    "    padded_matrix = np.stack(matrices, axis=0) # channel dim first\n",
    "    return padded_matrix\n",
    "    \n",
    "def plot_matrix(matrix, title='', cmap=None):\n",
    "    fig, ax = plot.subplots(figsize=(10,10))\n",
    "    ax = pf.clear_axis(ax)\n",
    "    ax.imshow(matrix, cmap=cmap)#, vmin=0.0, vmax=1.0)#, cmap='greys_r')\n",
    "    ax.format(title=title)\n",
    "    plot.show()\n",
    "    return fig\n",
    "\n",
    "pad_value = 0.5\n",
    "num_pad_pix = 1\n",
    "padded_matrix = pad_matrix_to_image(lca_weights, num_pad_pix, pad_value, normalize=True)\n",
    "fig = plot_matrix(np.transpose(padded_matrix, axes=[1, 2, 0]), title=f'{model.params.model_name} weights')\n",
    "fig.savefig(\n",
    "    f'{model.params.disp_dir}/weights_plot_matrix.png',\n",
    "    transparent=False,\n",
    "    bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_gray(rgb):\n",
    "    num, chan, height, width = rgb.shape\n",
    "    gray = np.zeros((num, 1, height, width))\n",
    "    for neuron_idx in range(num):\n",
    "        gray[neuron_idx, ...] = 0.2125 * rgb[neuron_idx, 0, ...]\n",
    "        gray[neuron_idx, ...] += 0.7154 * rgb[neuron_idx, 1, ...]\n",
    "        gray[neuron_idx, ...] += 0.0721 * rgb[neuron_idx, 2, ...]\n",
    "    return gray\n",
    "\n",
    "gray_lca_weights = rgb_to_gray(lca_weights)\n",
    "pad_value = 0.5\n",
    "num_pad_pix = 1\n",
    "padded_matrix = pad_matrix_to_image(gray_lca_weights, num_pad_pix, pad_value, normalize=True)\n",
    "fig = plot_matrix(np.squeeze(np.transpose(padded_matrix, axes=[1, 2, 0])), title=f'{model.params.model_name} weights', cmap='grays_r')\n",
    "fig.savefig(\n",
    "    f'{model.params.disp_dir}/weights_grayscale_plot_matrix.png',\n",
    "    transparent=False,\n",
    "    bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian(shape, mean, cov):\n",
    "    \"\"\"\n",
    "    Generate a Gaussian PDF from given mean & cov\n",
    "    Inputs:\n",
    "        shape: [tuple] specifying (num_rows, num_cols)\n",
    "        mean: [np.ndarray] of shape (2,) specifying the 2-D Gaussian center\n",
    "        cov: [np.ndarray] of shape (2,2) specifying the 2-D Gaussian covariance matrix\n",
    "    Outputs:\n",
    "        tuple containing (Gaussian PDF, grid_points used to generate PDF)\n",
    "            grid_points are specified as a tuple of (y,x) points\n",
    "    \"\"\"\n",
    "    (y_size, x_size) = shape\n",
    "    y = np.linspace(0, y_size, np.int32(np.floor(y_size)))\n",
    "    x = np.linspace(0, x_size, np.int32(np.floor(x_size)))\n",
    "    y, x = np.meshgrid(y, x)\n",
    "    pos = np.empty(x.shape + (2,)) #x.shape == y.shape\n",
    "    pos[:, :, 0] = y; pos[:, :, 1] = x\n",
    "    gauss = scipy.stats.multivariate_normal(mean, cov)\n",
    "    return (gauss.pdf(pos), (y,x))\n",
    "\n",
    "\n",
    "def gaussian_fit(pyx):\n",
    "    \"\"\"\n",
    "    Compute the expected mean & covariance matrix for a 2-D gaussian fit of input distribution\n",
    "    Inputs:\n",
    "        pyx: [np.ndarray] of shape [num_rows, num_cols] that indicates the probability function to fit\n",
    "    Outputs:\n",
    "        mean: [np.ndarray] of shape (2,) specifying the 2-D Gaussian center\n",
    "        cov: [np.ndarray] of shape (2,2) specifying the 2-D Gaussian covariance matrix\n",
    "    \"\"\"\n",
    "    assert pyx.ndim == 2, (\n",
    "        \"Input must have 2 dimensions specifying [num_rows, num_cols]\")\n",
    "    mean = np.zeros((1,2), dtype=np.float32) # [mu_y, mu_x]\n",
    "    for idx in np.ndindex(pyx.shape): # [y, x] ticks columns (x) first, then rows (y)\n",
    "        mean += np.asarray([pyx[idx]*idx[0], pyx[idx]*idx[1]])[None,:]\n",
    "    cov = np.zeros((2,2), dtype=np.float32)\n",
    "    for idx in np.ndindex(pyx.shape): # ticks columns first, then rows\n",
    "        cov += np.dot((idx-mean).T, (idx-mean))*pyx[idx] # typically an outer-product\n",
    "    return (np.squeeze(mean), cov)\n",
    "\n",
    "\n",
    "def get_gauss_fit(prob_map, num_attempts=1, perc_mean=0.33):\n",
    "    \"\"\"\n",
    "    Returns a gaussian fit for a given probability map\n",
    "    Fitting is done via robust regression, where a fit is\n",
    "    continuously refined by deleting outliers num_attempts times\n",
    "    Inputs:\n",
    "        prob_map: 2-D probability map to be fit\n",
    "        num_attempts: Number of times to fit & remove outliers\n",
    "        perc_mean: All probability values below perc_mean*mean(gauss_fit) will be\n",
    "            considered outliers for repeated attempts\n",
    "    Outputs:\n",
    "        gauss_fit: [np.ndarray] specifying the 2-D Gaussian PDF\n",
    "        grid: [tuple] containing (y,x) points with which the Gaussian PDF can be plotted\n",
    "        gauss_mean: [np.ndarray] of shape (2,) specifying the 2-D Gaussian center\n",
    "        gauss_cov: [np.ndarray] of shape (2,2) specifying the 2-D Gaussian covariance matrix\n",
    "    \"\"\"\n",
    "    assert prob_map.ndim==2, (\n",
    "        \"get_gauss_fit: Input prob_map must have 2 dimension specifying [num_rows, num_cols\")\n",
    "    if num_attempts < 1:\n",
    "        num_attempts = 1\n",
    "    orig_prob_map = prob_map.copy()\n",
    "    gauss_success = False\n",
    "    while not gauss_success:\n",
    "        prob_map = orig_prob_map.copy()\n",
    "        try:\n",
    "            for i in range(num_attempts):\n",
    "                map_min = np.min(prob_map)\n",
    "                prob_map -= map_min\n",
    "                map_sum = np.sum(prob_map)\n",
    "                if map_sum != 1.0:\n",
    "                    prob_map /= map_sum\n",
    "                gauss_mean, gauss_cov = gaussian_fit(prob_map)\n",
    "                gauss_fit, grid = generate_gaussian(prob_map.shape, gauss_mean, gauss_cov)\n",
    "                gauss_fit = (gauss_fit * map_sum) + map_min\n",
    "                if i < num_attempts-1:\n",
    "                    gauss_mask = gauss_fit.copy().T\n",
    "                    mask_slice = np.where(gauss_mask<perc_mean*np.mean(gauss_mask))\n",
    "                    gauss_mask[mask_slice] = 0\n",
    "                    gauss_mask[np.where(gauss_mask>0)] = 1\n",
    "                    prob_map *= gauss_mask\n",
    "            gauss_success = True\n",
    "        except np.linalg.LinAlgError: # Usually means cov matrix is singular\n",
    "            print(\"get_gauss_fit: Failed to fit Gaussian at attempt \",i,\", trying again.\"+\n",
    "                \"\\n  To avoid this try decreasing perc_mean.\")\n",
    "            num_attempts = i-1\n",
    "            if num_attempts <= 0:\n",
    "                assert False, (\"get_gauss_fit: np.linalg.LinAlgError - Unable to fit gaussian.\")\n",
    "    return (gauss_fit, grid, gauss_mean, gauss_cov)\n",
    "\n",
    "\n",
    "def hilbert_amplitude(weights, padding=None):\n",
    "    \"\"\"\n",
    "    Compute Hilbert amplitude envelope of weight matrix\n",
    "    Inputs:\n",
    "        weights: [np.ndarray] of shape [num_inputs, num_outputs]\n",
    "            num_inputs must have an even square root\n",
    "        padding: [int] specifying how much 0-padding to use for FFT\n",
    "            default is the closest power of 2 of sqrt(num_inputs)\n",
    "    Outputs:\n",
    "        env: [np.ndarray] of shape [num_outputs, num_inputs]\n",
    "            Hilbert envelope\n",
    "        bff_filt: [np.ndarray] of shape [num_outputs, padded_num_inputs]\n",
    "            Filtered Fourier transform of basis function\n",
    "        hil_filt: [np.ndarray] of shape [num_outputs, sqrt(num_inputs), sqrt(num_inputs)]\n",
    "            Hilbert filter to be applied in Fourier space\n",
    "        bffs: [np.ndarray] of shape [num_outputs, padded_num_inputs, padded_num_inputs]\n",
    "            Fourier transform of input weights\n",
    "    \"\"\"\n",
    "    cart2pol = lambda x,y: (np.arctan2(y,x), np.hypot(x, y))\n",
    "    num_inputs, num_outputs = weights.shape\n",
    "    assert np.sqrt(num_inputs) == np.floor(np.sqrt(num_inputs)), (\n",
    "        \"weights.shape[0] must have an even square root.\")\n",
    "    patch_edge_size = int(np.sqrt(num_inputs))\n",
    "    if padding is None or padding <= patch_edge_size:\n",
    "        # Amount of zero padding for fft2 (closest power of 2)\n",
    "        N = np.int(2**(np.ceil(np.log2(patch_edge_size))))\n",
    "    else:\n",
    "        N = np.int(padding)\n",
    "    # Analytic signal envelope for weights\n",
    "    # (Hilbet transform of each basis function)\n",
    "    env = np.zeros((num_outputs, num_inputs), dtype=complex)\n",
    "    # Fourier transform of weights\n",
    "    bffs = np.zeros((num_outputs, N, N), dtype=complex)\n",
    "    # Filtered Fourier transform of weights\n",
    "    bff_filt = np.zeros((num_outputs, N**2), dtype=complex)\n",
    "    # Hilbert filters\n",
    "    hil_filt = np.zeros((num_outputs, N, N))\n",
    "    # Grid for creating filter\n",
    "    f = (2/N) * np.pi * np.arange(-N/2.0, N/2.0)\n",
    "    (fx, fy) = np.meshgrid(f, f)\n",
    "    (theta, r) = cart2pol(fx, fy)\n",
    "    for neuron_idx in range(num_outputs):\n",
    "        # Grab single basis function, reshape to a square image\n",
    "        bf = weights[:, neuron_idx].reshape(patch_edge_size, patch_edge_size)\n",
    "        # Convert basis function into DC-centered Fourier domain\n",
    "        bff = np.fft.fftshift(np.fft.fft2(bf-np.mean(bf), [N, N]))\n",
    "        bffs[neuron_idx, ...] = bff\n",
    "        # Find indices of the peak amplitude\n",
    "        max_ys = np.abs(bff).argmax(axis=0) # Returns row index for each col\n",
    "        max_x = np.argmax(np.abs(bff).max(axis=0))\n",
    "        # Convert peak amplitude location into angle in freq domain\n",
    "        fx_ang = f[max_x]\n",
    "        fy_ang = f[max_ys[max_x]]\n",
    "        theta_max = np.arctan2(fy_ang, fx_ang)\n",
    "        # Define the half-plane with respect to the maximum\n",
    "        ang_diff = np.abs(theta-theta_max)\n",
    "        idx = (ang_diff>np.pi).nonzero()\n",
    "        ang_diff[idx] = 2.0 * np.pi - ang_diff[idx]\n",
    "        hil_filt[neuron_idx, ...] = (ang_diff < np.pi/2.0).astype(int)\n",
    "        # Create analytic signal from the inverse FT of the half-plane filtered bf\n",
    "        abf = np.fft.ifft2(np.fft.fftshift(hil_filt[neuron_idx, ...]*bff))\n",
    "        env[neuron_idx, ...] = abf[0:patch_edge_size, 0:patch_edge_size].reshape(num_inputs)\n",
    "        bff_filt[neuron_idx, ...] = (hil_filt[neuron_idx, ...]*bff).reshape(N**2)\n",
    "    return (env, bff_filt, hil_filt, bffs)\n",
    "\n",
    "\n",
    "def get_dictionary_stats(weights, padding=None, num_gauss_fits=20, gauss_thresh=0.2):\n",
    "    \"\"\"\n",
    "    Compute summary statistics on dictionary elements using Hilbert amplitude envelope\n",
    "    Inputs:\n",
    "        weights: [np.ndarray] of shape [num_inputs, num_outputs]\n",
    "        padding: [int] total image size to pad out to in the FFT computation\n",
    "        num_gauss_fits: [int] total number of attempts to make when fitting the BFs\n",
    "        gauss_thresh: All probability values below gauss_thresh*mean(gauss_fit) will be\n",
    "            considered outliers for repeated fits\n",
    "    Outputs:\n",
    "      The function output is a dictionary containing the keys for each type of analysis\n",
    "      Each key dereferences a list of len num_outputs (i.e. one entry for each weight vector)\n",
    "      The keys and their list entries are as follows:\n",
    "          basis_functions: [np.ndarray] of shape [patch_edge_size, patch_edge_size]\n",
    "          envelopes: [np.ndarray] of shape [N, N], where N is the amount of padding\n",
    "              for the hilbert_amplitude function\n",
    "          envelope_centers: [tuples of ints] indicating the (y, x) position of the\n",
    "              center of the Hilbert envelope\n",
    "          gauss_fits: [list of np.ndarrays] containing (gaussian_fit, grid) where gaussian_fit\n",
    "              is returned from get_gauss_fit and specifies the 2D Gaussian PDF fit to the Hilbert\n",
    "              envelope and grid is a tuple containing (y,x) points with which the Gaussian PDF\n",
    "              can be plotted\n",
    "          gauss_centers: [list of ints] containing the (y,x) position of the center of\n",
    "              the Gaussian fit\n",
    "          gauss_orientations: [list of np.ndarrays] containing the (eigenvalues, eigenvectors) of\n",
    "              the covariance matrix for the Gaussian fit of the Hilbert amplitude envelope. They are\n",
    "              both sorted according to the highest to lowest Eigenvalue.\n",
    "          fourier_centers: [list of ints] containing the (y,x) position of the center (max) of\n",
    "              the Fourier amplitude map\n",
    "          num_inputs: [int] dim[0] of input weights\n",
    "          num_outputs: [int] dim[1] of input weights\n",
    "          patch_edge_size: [int] int(floor(sqrt(num_inputs)))\n",
    "          areas: [list of floats] area of enclosed ellipse\n",
    "          spatial_frequncies: [list of floats] dominant spatial frequency for basis function\n",
    "    \"\"\"\n",
    "    envelope, bff_filt, hil_filter, bffs = hilbert_amplitude(weights, padding)\n",
    "    num_inputs, num_outputs = weights.shape\n",
    "    patch_edge_size = np.int(np.floor(np.sqrt(num_inputs)))\n",
    "    basis_funcs = [None]*num_outputs\n",
    "    envelopes = [None]*num_outputs\n",
    "    gauss_fits = [None]*num_outputs\n",
    "    gauss_centers = [None]*num_outputs\n",
    "    diameters = [None]*num_outputs\n",
    "    gauss_orientations = [None]*num_outputs\n",
    "    envelope_centers = [None]*num_outputs\n",
    "    fourier_centers = [None]*num_outputs\n",
    "    ellipse_orientations = [None]*num_outputs\n",
    "    fourier_maps = [None]*num_outputs\n",
    "    spatial_frequencies = [None]*num_outputs\n",
    "    areas = [None]*num_outputs\n",
    "    phases = [None]*num_outputs\n",
    "    for bf_idx in range(num_outputs):\n",
    "        # Reformatted individual basis function\n",
    "        basis_funcs[bf_idx] = weights.T[bf_idx,...].reshape((patch_edge_size, patch_edge_size))\n",
    "        # Reformatted individual envelope filter\n",
    "        envelopes[bf_idx] = np.abs(envelope[bf_idx,...]).reshape((patch_edge_size, patch_edge_size))\n",
    "        # Basis function center\n",
    "        max_ys = envelopes[bf_idx].argmax(axis=0) # Returns row index for each col\n",
    "        max_x = np.argmax(envelopes[bf_idx].max(axis=0))\n",
    "        y_cen = max_ys[max_x]\n",
    "        x_cen = max_x\n",
    "        envelope_centers[bf_idx] = (y_cen, x_cen)\n",
    "        # Gaussian fit to Hilbet amplitude envelope\n",
    "        gauss_fit, grid, gauss_mean, gauss_cov = get_gauss_fit(envelopes[bf_idx],\n",
    "            num_gauss_fits, gauss_thresh)\n",
    "        gauss_fits[bf_idx] = (gauss_fit, grid)\n",
    "        gauss_centers[bf_idx] = gauss_mean\n",
    "        evals, evecs = np.linalg.eigh(gauss_cov)\n",
    "        sort_indices = np.argsort(evals)[::-1]\n",
    "        gauss_orientations[bf_idx] = (evals[sort_indices], evecs[:,sort_indices])\n",
    "        width, height = evals[sort_indices] # Width & height are relative to orientation\n",
    "        diameters[bf_idx] = np.sqrt(width**2+height**2)\n",
    "        # Fourier function center, spatial frequency, orientation\n",
    "        fourier_map = np.sqrt(np.real(bffs[bf_idx, ...])**2+np.imag(bffs[bf_idx, ...])**2)\n",
    "        fourier_maps[bf_idx] = fourier_map\n",
    "        N = fourier_map.shape[0]\n",
    "        center_freq = int(np.floor(N/2))\n",
    "        fourier_map[center_freq, center_freq] = 0 # remove DC component\n",
    "        max_fys = fourier_map.argmax(axis=0)\n",
    "        max_fx = np.argmax(fourier_map.max(axis=0))\n",
    "        fy_cen = (max_fys[max_fx] - (N/2)) * (patch_edge_size/N)\n",
    "        fx_cen = (max_fx - (N/2)) * (patch_edge_size/N)\n",
    "        fourier_centers[bf_idx] = [fy_cen, fx_cen]\n",
    "        # NOTE: we flip fourier_centers because fx_cen is the peak of the x frequency,\n",
    "        # which would be a y coordinate\n",
    "        ellipse_orientations[bf_idx] = np.arctan2(*fourier_centers[bf_idx][::-1])\n",
    "        spatial_frequencies[bf_idx] = np.sqrt(fy_cen**2 + fx_cen**2)\n",
    "        areas[bf_idx] = np.pi * np.prod(evals)\n",
    "        phases[bf_idx] = np.angle(bffs[bf_idx])[y_cen, x_cen]\n",
    "    output = {\"basis_functions\":basis_funcs, \"envelopes\":envelopes, \"gauss_fits\":gauss_fits,\n",
    "        \"gauss_centers\":gauss_centers, \"gauss_orientations\":gauss_orientations, \"areas\":areas,\n",
    "        \"fourier_centers\":fourier_centers, \"fourier_maps\":fourier_maps, \"num_inputs\":num_inputs,\n",
    "        \"spatial_frequencies\":spatial_frequencies, \"envelope_centers\":envelope_centers,\n",
    "        \"num_outputs\":num_outputs, \"patch_edge_size\":patch_edge_size, \"phases\":phases,\n",
    "        \"ellipse_orientations\":ellipse_orientations, \"diameters\":diameters}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_stats = get_dictionary_stats(\n",
    "    gray_lca_weights.reshape(gray_lca_weights.shape[0], -1).T,\n",
    "    padding=32,\n",
    "    num_gauss_fits=20,\n",
    "    gauss_thresh=0.2)\n",
    "\n",
    "np.savez(\n",
    "    model.params.save_dir+'bf_summary_stats.npz',\n",
    "    data={'bf_stats':bf_stats})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_axis(ax, spines=\"none\"):\n",
    "    for ax_loc in [\"top\", \"bottom\", \"left\", \"right\"]:\n",
    "        ax.spines[ax_loc].set_color(spines)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.tick_params(axis=\"both\", bottom=False, top=False, left=False, right=False)\n",
    "    return ax\n",
    "\n",
    "def plot_ellipse(axis, center, shape, angle, color_val=\"auto\", alpha=1.0, lines=False,\n",
    "    fill_ellipse=False):\n",
    "    \"\"\"\n",
    "    Add an ellipse to given axis\n",
    "    Inputs:\n",
    "        axis [matplotlib.axes._subplots.AxesSubplot] axis on which ellipse should be drawn\n",
    "        center [tuple or list] specifying [y, x] center coordinates\n",
    "        shape [tuple or list] specifying [width, height] shape of ellipse\n",
    "        angle [float] specifying angle of ellipse\n",
    "        color_val [matplotlib color spec] specifying the color of the edge & face of the ellipse\n",
    "        alpha [float] specifying the transparency of the ellipse\n",
    "        lines [bool] if true, output will be a line, where the secondary axis of the ellipse\n",
    "            is collapsed\n",
    "        fill_ellipse [bool] if true and lines is false then a filled ellipse will be plotted\n",
    "    Outputs:\n",
    "        ellipse [matplotlib.patches.ellipse] ellipse object\n",
    "    \"\"\"\n",
    "    if fill_ellipse:\n",
    "        face_color_val = \"none\" if color_val==\"auto\" else color_val\n",
    "    else:\n",
    "        face_color_val = \"none\"\n",
    "    y_cen, x_cen = center\n",
    "    width, height = shape\n",
    "    if lines:\n",
    "        min_length = 0.1\n",
    "        if width < height:\n",
    "            width = min_length\n",
    "        elif width > height:\n",
    "            height = min_length\n",
    "    ellipse = matplotlib.patches.Ellipse(xy=[x_cen, y_cen], width=width,\n",
    "        height=height, angle=angle, edgecolor=color_val, facecolor=face_color_val,\n",
    "        alpha=alpha, fill=True)\n",
    "    axis.add_artist(ellipse)\n",
    "    ellipse.set_clip_box(axis.bbox)\n",
    "    return ellipse\n",
    "\n",
    "def plot_ellipse_summaries(bf_stats, num_bf=-1, lines=False, rand_bf=False):\n",
    "    \"\"\"\n",
    "    Plot basis functions with summary ellipses drawn over them\n",
    "    Inputs:\n",
    "        bf_stats [dict] output of dp.get_dictionary_stats()\n",
    "        num_bf [int] number of basis functions to plot (<=0 is all; >total is all)\n",
    "        lines [bool] If true, will plot lines instead of ellipses\n",
    "        rand_bf [bool] If true, will choose a random set of basis functions\n",
    "    \"\"\"\n",
    "    tot_num_bf = len(bf_stats[\"basis_functions\"])\n",
    "    if num_bf <= 0 or num_bf > tot_num_bf:\n",
    "        num_bf = tot_num_bf\n",
    "    SFs = np.asarray([np.sqrt(fcent[0]**2 + fcent[1]**2)\n",
    "        for fcent in bf_stats[\"fourier_centers\"]], dtype=np.float32)\n",
    "    sf_sort_indices = np.argsort(SFs)\n",
    "    if rand_bf:\n",
    "        bf_range = np.random.choice([i for i in range(tot_num_bf)], num_bf, replace=False)\n",
    "    num_plots_y = int(np.ceil(np.sqrt(num_bf)))\n",
    "    num_plots_x = int(np.ceil(np.sqrt(num_bf)))\n",
    "    gs = gridspec.GridSpec(num_plots_y, num_plots_x)\n",
    "    fig = plt.figure(figsize=(17,17))\n",
    "    filter_idx = 0\n",
    "    for plot_id in  np.ndindex((num_plots_y, num_plots_x)):\n",
    "        ax = clear_axis(fig.add_subplot(gs[plot_id]))\n",
    "        if filter_idx < tot_num_bf and filter_idx < num_bf:\n",
    "            if rand_bf:\n",
    "                bf_idx = bf_range[filter_idx]\n",
    "            else:\n",
    "                bf_idx = filter_idx\n",
    "            bf = bf_stats[\"basis_functions\"][bf_idx]\n",
    "            ax.imshow(bf, interpolation=\"Nearest\", cmap=\"grays_r\")\n",
    "            ax.set_title(str(bf_idx), fontsize=\"8\")\n",
    "            center = bf_stats[\"gauss_centers\"][bf_idx]\n",
    "            evals, evecs = bf_stats[\"gauss_orientations\"][bf_idx]\n",
    "            orientations = bf_stats[\"fourier_centers\"][bf_idx]\n",
    "            angle = np.rad2deg(np.pi/2 + np.arctan2(*orientations))\n",
    "            alpha = 1.0\n",
    "            ellipse = plot_ellipse(ax, center, evals, angle, color_val=\"b\", alpha=alpha, lines=lines)\n",
    "            filter_idx += 1\n",
    "        ax.set_aspect(\"equal\")\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_ellipse_summaries(bf_stats, lines=False)\n",
    "\n",
    "fig.savefig(\n",
    "    f'{model.params.disp_dir}/basis_function_fits.png',\n",
    "    transparent=False,\n",
    "    bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgr_colormap():\n",
    "    \"\"\"\n",
    "    In cdict, the first column is interpolated between 0.0 & 1.0 - this indicates the value to be plotted\n",
    "    the second column specifies how interpolation should be done from below\n",
    "    the third column specifies how interpolation should be done from above\n",
    "    if the second column does not equal the third, then there will be a break in the colors\n",
    "    \"\"\"\n",
    "    darkness = 0.85 #0 is black, 1 is white\n",
    "    cdict = {\n",
    "        'red': ((0.0, 0.0, 0.0),\n",
    "            (0.5, darkness, darkness),\n",
    "            (1.0, 1.0, 1.0)),\n",
    "        'green': ((0.0, 0.0, 0.0),\n",
    "            (0.5, darkness, darkness),\n",
    "            (1.0, 0.0, 0.0)),\n",
    "        'blue': ((0.0, 1.0, 1.0),\n",
    "            (0.5, darkness, darkness),\n",
    "            (1.0, 0.0, 0.0))\n",
    "    }\n",
    "    return LinearSegmentedColormap(\"bgr\", cdict)\n",
    "\n",
    "def plot_pooling_centers(bf_stats, pooling_filters, num_pooling_filters, num_connected_weights,\n",
    "    spot_size=10, figsize=None):\n",
    "    \"\"\"\n",
    "    Plot 2nd layer (fully-connected) weights in terms of spatial/frequency centers of\n",
    "        1st layer weights\n",
    "    Inputs:\n",
    "        bf_stats [dict] Output of dp.get_dictionary_stats() which was run on the 1st layer weights\n",
    "        pooling_filters [np.ndarray] 2nd layer weights\n",
    "            should be shape [num_1st_layer_neurons, num_2nd_layer_neurons]\n",
    "        num_pooling_filters [int] How many 2nd layer neurons to plot\n",
    "        figsize [tuple] Containing the (width, height) of the figure, in inches\n",
    "        spot_size [int] How big to make the points\n",
    "    \"\"\"\n",
    "    num_filters_y = int(np.ceil(np.sqrt(num_pooling_filters)))\n",
    "    num_filters_x = int(np.ceil(np.sqrt(num_pooling_filters)))\n",
    "    tot_pooling_filters = pooling_filters.shape[1]\n",
    "    #filter_indices = np.random.choice(tot_pooling_filters, num_pooling_filters, replace=False)\n",
    "    filter_indices = np.arange(tot_pooling_filters, dtype=np.int32)\n",
    "    cmap = plt.get_cmap(bgr_colormap())# Could also use \"nipy_spectral\", \"coolwarm\", \"bwr\"\n",
    "    cNorm = matplotlib.colors.SymLogNorm(linthresh=0.03, linscale=0.01, vmin=-1.0, vmax=1.0)\n",
    "    scalarMap = matplotlib.cm.ScalarMappable(norm=cNorm, cmap=cmap)\n",
    "    x_p_cent = [x for (y,x) in bf_stats[\"gauss_centers\"]]# Get raw points\n",
    "    y_p_cent = [y for (y,x) in bf_stats[\"gauss_centers\"]]\n",
    "    x_f_cent = [x for (y,x) in bf_stats[\"fourier_centers\"]]\n",
    "    y_f_cent = [y for (y,x) in bf_stats[\"fourier_centers\"]]\n",
    "    max_sf = np.max(np.abs(x_f_cent+y_f_cent))\n",
    "    pair_w_gap = 0.01\n",
    "    group_w_gap = 0.03\n",
    "    h_gap = 0.03\n",
    "    plt_w = (num_filters_x/num_pooling_filters)\n",
    "    plt_h = plt_w\n",
    "    if figsize is None:\n",
    "        fig = plt.figure()\n",
    "        figsize = (fig.get_figwidth(), fig.get_figheight())\n",
    "    else:\n",
    "        fig = plt.figure(figsize=figsize) #figsize is (w,h)\n",
    "    axes = []\n",
    "    filter_id = 0\n",
    "    for plot_id in np.ndindex((num_filters_y, num_filters_x)):\n",
    "        if all(pid == 0 for pid in plot_id):\n",
    "            axes.append(clear_axis(fig.add_axes([0, plt_h+h_gap, 2*plt_w, plt_h])))\n",
    "            scalarMap._A = []\n",
    "            cbar = fig.colorbar(scalarMap, ax=axes[-1], ticks=[-1, 0, 1], aspect=10, location=\"bottom\")\n",
    "            cbar.ax.set_xticklabels([\"-1\", \"0\", \"1\"])\n",
    "            cbar.ax.xaxis.set_ticks_position('top')\n",
    "            cbar.ax.xaxis.set_label_position('top')\n",
    "            for label in cbar.ax.xaxis.get_ticklabels():\n",
    "                label.set_weight(\"bold\")\n",
    "                label.set_fontsize(10+figsize[0])\n",
    "        if (filter_id < num_pooling_filters):\n",
    "            example_filter = pooling_filters[:, filter_indices[filter_id]]\n",
    "            top_indices = np.argsort(np.abs(example_filter))[::-1] #descending\n",
    "            selected_indices = top_indices[:num_connected_weights][::-1] #select top, plot weakest first\n",
    "            filter_norm = np.max(np.abs(example_filter))\n",
    "            connection_colors = [scalarMap.to_rgba(example_filter[bf_idx]/filter_norm)\n",
    "                for bf_idx in range(bf_stats[\"num_outputs\"])]\n",
    "            if num_connected_weights < top_indices.size:\n",
    "                black_indices = top_indices[num_connected_weights:][::-1]\n",
    "                xp = [x_p_cent[i] for i in black_indices]+[x_p_cent[i] for i in selected_indices]\n",
    "                yp = [y_p_cent[i] for i in black_indices]+[y_p_cent[i] for i in selected_indices]\n",
    "                xf = [x_f_cent[i] for i in black_indices]+[x_f_cent[i] for i in selected_indices]\n",
    "                yf = [y_f_cent[i] for i in black_indices]+[y_f_cent[i] for i in selected_indices]\n",
    "                c = [(0.1,0.1,0.1,1.0) for i in black_indices]+[connection_colors[i] for i in selected_indices]\n",
    "            else:\n",
    "                xp = [x_p_cent[i] for i in selected_indices]\n",
    "                yp = [y_p_cent[i] for i in selected_indices]\n",
    "                xf = [x_f_cent[i] for i in selected_indices]\n",
    "                yf = [y_f_cent[i] for i in selected_indices]\n",
    "                c = [connection_colors[i] for i in selected_indices]\n",
    "            (y_id, x_id) = plot_id\n",
    "            if x_id == 0:\n",
    "                ax_l = 0\n",
    "                ax_b = - y_id * (plt_h+h_gap)\n",
    "            else:\n",
    "                bbox = axes[-1].get_position().get_points()[0]#bbox is [[x0,y0],[x1,y1]]\n",
    "                prev_l = bbox[0]\n",
    "                prev_b = bbox[1]\n",
    "                ax_l = prev_l + plt_w + group_w_gap\n",
    "                ax_b = prev_b\n",
    "            ax_w = plt_w\n",
    "            ax_h = plt_h\n",
    "            axes.append(clear_axis(fig.add_axes([ax_l, ax_b, ax_w, ax_h])))\n",
    "            axes[-1].invert_yaxis()\n",
    "            axes[-1].scatter(xp, yp, c=c, s=spot_size, alpha=0.8)\n",
    "            axes[-1].set_xlim(0, bf_stats[\"patch_edge_size\"]-1)\n",
    "            axes[-1].set_ylim(bf_stats[\"patch_edge_size\"]-1, 0)\n",
    "            axes[-1].set_aspect(\"equal\")\n",
    "            axes[-1].set_facecolor(\"w\")\n",
    "            axes.append(clear_axis(fig.add_axes([ax_l+ax_w+pair_w_gap, ax_b, ax_w, ax_h])))\n",
    "            axes[-1].scatter(xf, yf, c=c, s=spot_size, alpha=0.8)\n",
    "            axes[-1].set_xlim([-max_sf, max_sf])\n",
    "            axes[-1].set_ylim([-max_sf, max_sf])\n",
    "            axes[-1].xaxis.set_major_locator(matplotlib.ticker.MaxNLocator(integer=True))\n",
    "            axes[-1].yaxis.set_major_locator(matplotlib.ticker.MaxNLocator(integer=True))\n",
    "            axes[-1].set_aspect(\"equal\")\n",
    "            axes[-1].set_facecolor(\"w\")\n",
    "            filter_id += 1\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_pos = 0\n",
    "pool_weights = model.pool_1.layer.weight.detach().cpu().numpy()\n",
    "outputs, inputs, kernel_h, kernel_w = pool_weights.shape\n",
    "\n",
    "fig = plot_pooling_centers(\n",
    "    bf_stats,\n",
    "    pool_weights[:, :, kernel_pos, kernel_pos].T,\n",
    "    num_pooling_filters=outputs,\n",
    "    num_connected_weights=inputs,\n",
    "    spot_size=3,\n",
    "    figsize=(5, 5))\n",
    "\n",
    "fig.savefig(\n",
    "    f'{model.params.disp_dir}/pooling_spots.png',\n",
    "    transparent=False,\n",
    "    bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pooling_summaries(bf_stats, pooling_filters, num_pooling_filters,\n",
    "    num_connected_weights, lines=False, figsize=None):\n",
    "    \"\"\"\n",
    "    Plot 2nd layer (fully-connected) weights in terms of connection strengths to 1st layer weights\n",
    "    Inputs:\n",
    "        bf_stats [dict] output of dp.get_dictionary_stats() which was run on the 1st layer weights\n",
    "        pooling_filters [np.ndarray] 2nd layer weights\n",
    "            should be shape [num_1st_layer_neurons, num_2nd_layer_neurons]\n",
    "        num_pooling_filters [int] How many 2nd layer neurons to plot\n",
    "        num_connected_weights [int] How many 1st layer weight summaries to include\n",
    "            for a given 2nd layer neuron\n",
    "        lines [bool] if True, 1st layer weight summaries will appear as lines instead of ellipses\n",
    "    \"\"\"\n",
    "    num_inputs = bf_stats[\"num_inputs\"]\n",
    "    num_outputs = bf_stats[\"num_outputs\"]\n",
    "    tot_pooling_filters = pooling_filters.shape[1]\n",
    "    patch_edge_size = np.int32(np.sqrt(num_inputs))\n",
    "    filter_idx_list = np.arange(num_pooling_filters, dtype=np.int32)\n",
    "    assert num_pooling_filters <= num_outputs, (\n",
    "        \"num_pooling_filters must be less than or equal to bf_stats['num_outputs']\")\n",
    "    cmap = bgr_colormap()#plt.get_cmap('bwr')\n",
    "    cNorm = matplotlib.colors.SymLogNorm(linthresh=0.03, linscale=0.01, vmin=-1.0, vmax=1.0)\n",
    "    scalarMap = matplotlib.cm.ScalarMappable(norm=cNorm, cmap=cmap)\n",
    "    num_plots_y = np.int32(np.ceil(np.sqrt(num_pooling_filters)))\n",
    "    num_plots_x = np.int32(np.ceil(np.sqrt(num_pooling_filters)))+1 # +cbar col\n",
    "    gs_widths = [1 for _ in range(num_plots_x-1)]+[0.3]\n",
    "    gs = gridspec.GridSpec(num_plots_y, num_plots_x, width_ratios=gs_widths)\n",
    "    if figsize is None:\n",
    "        fig = plt.figure()\n",
    "        figsize = (fig.get_figwidth(), fig.get_figheight())\n",
    "    else:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "    filter_total = 0\n",
    "    for plot_id in  np.ndindex((num_plots_y, num_plots_x-1)):\n",
    "        (y_id, x_id) = plot_id\n",
    "        ax = fig.add_subplot(gs[plot_id])\n",
    "        if (filter_total < num_pooling_filters and x_id != num_plots_x-1):\n",
    "            ax = clear_axis(ax, spines=\"k\")\n",
    "            filter_idx = filter_idx_list[filter_total]\n",
    "            example_filter = pooling_filters[:, filter_idx]\n",
    "            top_indices = np.argsort(np.abs(example_filter))[::-1] #descending\n",
    "            filter_norm = np.max(np.abs(example_filter))\n",
    "            SFs = np.asarray([np.sqrt(fcent[0]**2 + fcent[1]**2)\n",
    "                for fcent in bf_stats[\"fourier_centers\"]], dtype=np.float32)\n",
    "            # Plot weakest of the top connected filters first because of occlusion\n",
    "            for bf_idx in top_indices[:num_connected_weights][::-1]:\n",
    "                connection_strength = example_filter[bf_idx]/filter_norm\n",
    "                color_val = scalarMap.to_rgba(connection_strength)\n",
    "                center = bf_stats[\"gauss_centers\"][bf_idx]\n",
    "                evals, evecs = bf_stats[\"gauss_orientations\"][bf_idx]\n",
    "                orientations = bf_stats[\"fourier_centers\"][bf_idx]\n",
    "                angle = np.rad2deg(np.pi/2 + np.arctan2(*orientations))\n",
    "                alpha = 0.5#todo:spatial_freq for filled ellipses?\n",
    "                ellipse = plot_ellipse(ax, center, evals, angle, color_val, alpha=alpha, lines=lines)\n",
    "            ax.set_xlim(0, patch_edge_size-1)\n",
    "            ax.set_ylim(patch_edge_size-1, 0)\n",
    "            filter_total += 1\n",
    "        else:\n",
    "            ax = clear_axis(ax, spines=\"none\")\n",
    "        ax.set_aspect(\"equal\")\n",
    "    scalarMap._A = []\n",
    "    ax = clear_axis(fig.add_subplot(gs[0, -1]))\n",
    "    cbar = fig.colorbar(scalarMap, ax=ax, ticks=[-1, 0, 1])\n",
    "    cbar.ax.set_yticklabels([\"-1\", \"0\", \"1\"])\n",
    "    for label in cbar.ax.yaxis.get_ticklabels():\n",
    "        label.set_weight(\"bold\")\n",
    "        label.set_fontsize(14)\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_pooling_summaries(\n",
    "    bf_stats,\n",
    "    pool_weights[:, :, kernel_pos, kernel_pos].T,\n",
    "    num_pooling_filters=outputs,\n",
    "    num_connected_weights=40,\n",
    "    lines=True,\n",
    "    figsize=(18,18))\n",
    "\n",
    "fig.savefig(\n",
    "    f'{model.params.disp_dir}/pooling_lines.png',\n",
    "    transparent=False,\n",
    "    bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = pool_weights[:, :, kernel_pos, kernel_pos] # [inputs, outputs]\n",
    "p_norm = np.linalg.norm(P, ord=2, axis=0)\n",
    "affinity = np.dot(P.T, P) # cosyne similarity of neurons in embedded space\n",
    "for i in range(affinity.shape[0]):\n",
    "    for j in range(affinity.shape[1]):\n",
    "        affinity[i, j] = affinity[i, j] /  (p_norm[i] * p_norm[j])\n",
    "affinity = affinity.T # [inputs, inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_pooling_centers(\n",
    "    bf_stats,\n",
    "    affinity,\n",
    "    num_pooling_filters=outputs,\n",
    "    num_connected_weights=128, \n",
    "    spot_size=30,\n",
    "    figsize=(5, 5))\n",
    "\n",
    "fig.savefig(\n",
    "    f'{model.params.disp_dir}/affinity_spots.png',\n",
    "    transparent=False,\n",
    "    bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_pooling_summaries(\n",
    "    bf_stats,\n",
    "    affinity,\n",
    "    num_pooling_filters=outputs,\n",
    "    num_connected_weights=20,\n",
    "    lines=True,\n",
    "    figsize=(10, 10))\n",
    "\n",
    "fig.savefig(\n",
    "    f'{model.params.disp_dir}/affinity_lines.png',\n",
    "    transparent=False,\n",
    "    bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = next(iter(train_loader))[0].to(model.params.device)\n",
    "example_batch = model[0].preprocess_data(example_batch)\n",
    "example_batch *= train_std_image\n",
    "example_batch += train_mean_image\n",
    "batch_min = example_batch.min().item()\n",
    "batch_max = example_batch.max().item()\n",
    "\n",
    "example_image = example_batch[0, ...]\n",
    "print(\n",
    "    f'min = {example_image.min().item()}'+\n",
    "    f'\\nmean = {example_image.mean().item()}'+\n",
    "    f'\\nmax = {example_image.max().item()}'+\n",
    "    f'\\nstd = {example_image.std().item()}')\n",
    "\n",
    "plot_example_image = ((example_image * train_std_image) + train_mean_image).cpu().numpy().transpose(1,2,0)\n",
    "fig, ax = plot.subplots(nrows=1, ncols=1)\n",
    "ax = pf.clear_axis(ax)\n",
    "ax.imshow(plot_example_image, vmin=0, vmax=1)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_2 = model(example_image[None,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lca_2_recon_params(LcaParams):\n",
    "    def set_params(self):\n",
    "        super(lca_2_recon_params, self).set_params()\n",
    "        self.model_type = 'lca'\n",
    "        self.model_name = 'lca_2_recon'\n",
    "        self.version = '0'\n",
    "        self.layer_types = ['fc']\n",
    "        self.standardize_data = False\n",
    "        self.rescale_data_to_one = False\n",
    "        self.center_dataset = False\n",
    "        self.batch_size = 1\n",
    "        self.dt = 0.001\n",
    "        self.tau = 0.2\n",
    "        self.num_steps = 75\n",
    "        self.rectify_a = True\n",
    "        self.thresh_type = 'hard'\n",
    "        self.compute_helper_params()\n",
    "        \n",
    "params = lca_2_recon_params()\n",
    "params.set_params()\n",
    "params.layer_channels = model.lca_2.params.layer_channels\n",
    "params.sparse_mult = model.lca_2.params.sparse_mult\n",
    "params.data_shape = list(beta_2.shape)\n",
    "params.epoch_size = 1\n",
    "params.num_pixels = np.prod(params.data_shape)\n",
    "\n",
    "lca_2_recon_model = loaders.load_model(params.model_type)\n",
    "lca_2_recon_model.setup(params)\n",
    "lca_2_recon_model.to(params.device)\n",
    "lca_2_recon_model.eval()\n",
    "with torch.no_grad():\n",
    "    lca_2_recon_model.weight = nn.Parameter(model.pool_2.weight)\n",
    "alpha_2_hat = lca_2_recon_model(beta_2)[:, :, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    beta_1_hat = F.conv_transpose2d(\n",
    "        input=alpha_2_hat,\n",
    "        weight=model.lca_2.weight,\n",
    "        bias=None,\n",
    "        stride=model.lca_2.params.stride,\n",
    "        padding=model.lca_2.params.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepSparseCoding.modules.lca_module import LcaModule\n",
    "from DeepSparseCoding.models.base_model import BaseModel\n",
    "from DeepSparseCoding.utils.run_utils import compute_deconv_output_shape\n",
    "import DeepSparseCoding.modules.losses as losses\n",
    "\n",
    "class TransposedLcaModule(LcaModule):\n",
    "    def setup_module(self, params):\n",
    "        super(TransposedLcaModule, self).setup_module(params)\n",
    "        if self.params.layer_types[0] == 'conv':\n",
    "            self.layer_output_shapes = [self.params.data_shape] # [channels, height, width]\n",
    "            assert (self.params.data_shape[-1] % self.params.stride == 0), (\n",
    "                f'Stride = {self.params.stride} must divide evenly into input edge size = {self.params.data_shape[-1]}')\n",
    "            self.w_shape = [\n",
    "                self.params.layer_channels,\n",
    "                self.params.data_shape[0], # channels = 1\n",
    "                self.params.kernel_size,\n",
    "                self.params.kernel_size\n",
    "            ]\n",
    "            output_height = compute_deconv_output_shape(\n",
    "                self.layer_output_shapes[-1][1],\n",
    "                self.params.kernel_size,\n",
    "                self.params.stride,\n",
    "                self.params.padding,\n",
    "                output_padding=self.params.output_padding,\n",
    "                dilation=1)\n",
    "            output_width = compute_deconv_output_shape(\n",
    "                self.layer_output_shapes[-1][2],\n",
    "                self.params.kernel_size,\n",
    "                self.params.stride,\n",
    "                self.params.padding,\n",
    "                output_padding=self.params.output_padding,\n",
    "                dilation=1)\n",
    "            self.layer_output_shapes.append([self.params.layer_channels, output_height, output_width])\n",
    "        w_init = torch.randn(self.w_shape)\n",
    "        w_init_normed = dp.l2_normalize_weights(w_init, eps=self.params.eps)\n",
    "        self.weight = nn.Parameter(w_init_normed, requires_grad=True)\n",
    "\n",
    "    def compute_excitatory_current(self, input_tensor, a_in, weight=None):\n",
    "        if weight is None:\n",
    "            weight = self.weight\n",
    "        if self.params.layer_types[0] == 'fc':\n",
    "            excitatory_current = torch.matmul(input_tensor, weight)\n",
    "        else:\n",
    "            recon = self.get_recon_from_latents(a_in, weight)\n",
    "            recon_error = input_tensor - recon\n",
    "            error_injection = F.conv_transpose2d(\n",
    "                input=recon_error,\n",
    "                weight=weight,\n",
    "                bias=None,\n",
    "                stride=self.params.stride,\n",
    "                padding=self.params.padding,\n",
    "                output_padding=self.params.output_padding,\n",
    "                dilation=1\n",
    "            )\n",
    "            excitatory_current = error_injection + a_in\n",
    "        return excitatory_current\n",
    "\n",
    "    def get_recon_from_latents(self, a_in, weight=None):\n",
    "        if weight is None:\n",
    "            weight = self.weight\n",
    "        if self.params.layer_types[0] == 'fc':\n",
    "            recon = torch.matmul(a_in, torch.transpose(weight, dim0=0, dim1=1))\n",
    "        else:\n",
    "            recon = F.conv2d(\n",
    "                input=a_in,\n",
    "                weight=weight,\n",
    "                bias=None,\n",
    "                stride=self.params.stride,\n",
    "                padding=self.params.padding,\n",
    "                dilation=1\n",
    "            )\n",
    "        return recon\n",
    "\n",
    "class TransposedLcaModel(BaseModel, TransposedLcaModule):\n",
    "    def setup(self, params, logger=None):\n",
    "        super(TransposedLcaModel, self).setup(params, logger)\n",
    "        self.setup_module(params)\n",
    "        self.setup_optimizer()\n",
    "        if params.checkpoint_boot_log != '':\n",
    "            checkpoint = self.get_checkpoint_from_log(params.checkpoint_boot_log)\n",
    "            self.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.module.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    def get_total_loss(self, input_tuple):\n",
    "        input_tensor, input_labels = input_tuple\n",
    "        latents = self.get_encodings(input_tensor)\n",
    "        recon = self.get_recon_from_latents(latents)\n",
    "        recon_loss = losses.half_squared_l2(input_tensor, recon)\n",
    "        sparse_loss = self.params.sparse_mult * losses.l1_norm(latents)\n",
    "        total_loss = recon_loss + sparse_loss\n",
    "        return total_loss\n",
    "\n",
    "    def generate_update_dict(self, input_data, input_labels=None, batch_step=0, update_dict=None):\n",
    "        if update_dict is None:\n",
    "            update_dict = super(TransposedLcaModel, self).generate_update_dict(input_data, input_labels, batch_step)\n",
    "        stat_dict = dict()\n",
    "        latents = self.get_encodings(input_data)\n",
    "        recon = self.get_recon_from_latents(latents)\n",
    "        recon_loss = losses.half_squared_l2(input_data, recon).item()\n",
    "        sparse_loss = self.params.sparse_mult * losses.l1_norm(latents).item()\n",
    "        stat_dict['weight_lr'] = self.scheduler.get_lr()[0]\n",
    "        stat_dict['loss_recon'] = recon_loss\n",
    "        stat_dict['loss_sparse'] = sparse_loss\n",
    "        stat_dict['loss_total'] = recon_loss + sparse_loss\n",
    "        stat_dict['input_max_mean_min'] = [\n",
    "                input_data.max().item(), input_data.mean().item(), input_data.min().item()]\n",
    "        stat_dict['recon_max_mean_min'] = [\n",
    "                recon.max().item(), recon.mean().item(), recon.min().item()]\n",
    "        def count_nonzero(array, dim):\n",
    "            # TODO: github issue 23907 requests torch.count_nonzero, integrated in torch 1.7\n",
    "            return torch.sum(array !=0, dim=dim, dtype=torch.float)\n",
    "        latent_dims = tuple([i for i in range(len(latents.shape))])\n",
    "        latent_nnz = count_nonzero(latents, dim=latent_dims).item()\n",
    "        stat_dict['fraction_active_all_latents'] = latent_nnz / latents.numel()\n",
    "        if self.params.layer_types[0] == 'conv':\n",
    "            latent_map_dims = latent_dims[2:]\n",
    "            latent_map_size = np.prod(list(latents.shape[2:]))\n",
    "            latent_channel_nnz = count_nonzero(latents, dim=latent_map_dims)/latent_map_size\n",
    "            latent_channel_mean_nnz = torch.mean(latent_channel_nnz).item()\n",
    "            stat_dict['fraction_active_latents_per_channel'] = latent_channel_mean_nnz\n",
    "            num_channels = latents.shape[1]\n",
    "            latent_patch_mean_nnz = torch.mean(count_nonzero(latents, dim=1)/num_channels).item()\n",
    "            stat_dict['fraction_active_latents_per_patch'] = latent_patch_mean_nnz\n",
    "        update_dict.update(stat_dict)\n",
    "        return update_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lca_1_recon_params(LcaParams):\n",
    "    def set_params(self):\n",
    "        super(lca_1_recon_params, self).set_params()\n",
    "        self.model_type = 'lca'\n",
    "        self.model_name = 'lca_1_recon'\n",
    "        self.version = '0'\n",
    "        self.layer_types = ['conv']\n",
    "        self.standardize_data = False\n",
    "        self.rescale_data_to_one = False\n",
    "        self.center_dataset = False\n",
    "        self.batch_size = 1\n",
    "        self.dt = 0.001\n",
    "        self.tau = 0.2\n",
    "        self.num_steps = 75\n",
    "        self.rectify_a = True\n",
    "        self.thresh_type = 'hard'\n",
    "        self.compute_helper_params()\n",
    "        \n",
    "params = lca_1_recon_params()\n",
    "params.set_params()\n",
    "params.layer_channels = model.pool_1.params.layer_channels[0]\n",
    "params.kernel_size = model.pool_1.params.pool_ksize\n",
    "params.stride = model.pool_1.params.pool_stride\n",
    "params.padding = 0\n",
    "params.sparse_mult = 0.01#model.lca_1.params.sparse_mult\n",
    "params.data_shape = list(beta_1_hat.shape[1:])\n",
    "params.epoch_size = 1\n",
    "params.output_padding = 1\n",
    "params.num_pixels = np.prod(params.data_shape)\n",
    "\n",
    "lca_1_recon_model = TransposedLcaModel()\n",
    "lca_1_recon_model.setup(params)\n",
    "lca_1_recon_model.to(params.device)\n",
    "lca_1_recon_model.eval()\n",
    "with torch.no_grad():\n",
    "    lca_1_recon_model.weight = nn.Parameter(model.pool_1.weight)\n",
    "alpha_1_hat = lca_1_recon_model(beta_1_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    recon = F.conv_transpose2d(\n",
    "        input=alpha_1_hat,\n",
    "        weight=model.lca_1.weight,\n",
    "        bias=None,\n",
    "        stride=model.lca_1.params.stride,\n",
    "        padding=model.lca_1.params.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_2_nnz = torch.sum(alpha_2_hat !=0,\n",
    "          dim=tuple([i for i in range(len(alpha_2_hat.shape))]),\n",
    "          dtype=torch.float)/alpha_2_hat.numel()\n",
    "alpha_1_nnz = torch.sum(alpha_1_hat !=0,\n",
    "          dim=tuple([i for i in range(len(alpha_1_hat.shape))]),\n",
    "          dtype=torch.float)/alpha_1_hat.numel()\n",
    "print(\n",
    "    f'beta2 shape = {beta_2.shape}' + \n",
    "    f'\\nalpha2^ nnz = {alpha_2_nnz}'+\n",
    "    f'\\nalpha2^ shape = {alpha_2_hat.shape}'+\n",
    "    f'\\nbeta1^ shape = {beta_1_hat.shape}'\n",
    "    f'\\nalpha1^ nnz = {alpha_1_nnz}'+\n",
    "    f'\\nalpha1^ shape = {alpha_1_hat.shape}'+\n",
    "    f'\\nimage^ shape = {recon.shape}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'recon min = {recon.min().item()}'+\n",
    "    f'\\nrecon mean = {recon.mean().item()}'+\n",
    "    f'\\nrecon max = {recon.max().item()}'+\n",
    "    f'\\nrecon std = {recon.std().item()}')\n",
    "\n",
    "plot_recon = ((recon.squeeze() * train_std_image) + train_mean_image).cpu().numpy().transpose(1,2,0)\n",
    "fig, ax = plot.subplots(nrows=1, ncols=1)\n",
    "ax = pf.clear_axis(ax)\n",
    "ax.imshow(plot_recon, vmin=0, vmax=1)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_recon = recon.squeeze().cpu().numpy().transpose(1,2,0)\n",
    "plot_recon = (plot_recon - plot_recon.min()) / (plot_recon.max() - plot_recon.min())\n",
    "fig, ax = plot.subplots(nrows=1, ncols=1)\n",
    "ax = pf.clear_axis(ax)\n",
    "ax.imshow(plot_recon)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
