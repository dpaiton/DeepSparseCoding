{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#from typing import Union, Any, Optional, Callable, Tuple\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "if ROOT_DIR not in sys.path: sys.path.append(ROOT_DIR)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import proplot as plot\n",
    "from scipy.stats import pearsonr as linear_correlation\n",
    "import tensorflow as tf\n",
    "#import eagerpy as ep\n",
    "#import torch\n",
    "#from torch import nn, optim\n",
    "#from torch.nn import functional as F\n",
    "\n",
    "from DeepSparseCoding.tf1x.utils.logger import Logger as tfLogger\n",
    "import DeepSparseCoding.tf1x.analysis.analysis_picker as ap\n",
    "#from DeepSparseCoding.tf1x.data.dataset import Dataset\n",
    "import DeepSparseCoding.tf1x.data.data_selector as ds\n",
    "import DeepSparseCoding.tf1x.utils.data_processing as tfdp\n",
    "import DeepSparseCoding.tf1x.params.param_picker as pp\n",
    "import DeepSparseCoding.tf1x.models.model_picker as mp\n",
    "\n",
    "#from DeepSparseCoding.utils.file_utils import Logger\n",
    "#import DeepSparseCoding.utils.dataset_utils as dataset_utils\n",
    "#import DeepSparseCoding.utils.loaders as loaders\n",
    "import DeepSparseCoding.utils.plot_functions as pf\n",
    "\n",
    "#import foolbox\n",
    "#from foolbox import PyTorchModel\n",
    "#from foolbox.attacks.projected_gradient_descent import LinfProjectedGradientDescentAttack\n",
    "#from foolbox.types import Bounds\n",
    "#from foolbox.models.base import Model\n",
    "#from foolbox.attacks.base import T\n",
    "#from foolbox.criteria import Misclassification\n",
    "#from foolbox.attacks.base import raise_if_kwargs\n",
    "#from foolbox.attacks.base import get_criterion\n",
    "\n",
    "rand_seed = 123\n",
    "rand_state = np.random.RandomState(rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DeepSparseCoding analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class params(object):\n",
    "  def __init__(self):\n",
    "    self.device = \"/gpu:0\"\n",
    "    self.analysis_dataset = \"test\"\n",
    "    self.save_info = \"analysis_\" + self.analysis_dataset\n",
    "    self.overwrite_analysis_log = False\n",
    "    self.do_class_adversaries = False\n",
    "    self.do_run_analysis = False\n",
    "    self.do_evals = False\n",
    "    self.do_basis_analysis = False\n",
    "    self.do_inference = False\n",
    "    self.do_atas = False \n",
    "    self.do_recon_adversaries = False\n",
    "    self.do_neuron_visualization = False\n",
    "    self.do_full_recon = False\n",
    "    self.do_orientation_analysis = False \n",
    "    self.do_group_recons = False\n",
    "    \n",
    "    self.data_dir = os.path.join(ROOT_DIR, 'Datasets')\n",
    "    self.data_type = 'vanhateren'\n",
    "    self.vectorize_data = True\n",
    "    self.rescale_data = False\n",
    "    self.standardize_data = False\n",
    "    self.contrast_normalize = False\n",
    "    self.whiten_data = True\n",
    "    self.whiten_method = \"FT\"\n",
    "    self.whiten_batch_size = 2\n",
    "    self.extract_patches = True\n",
    "    self.num_patches = 1e4\n",
    "    self.patch_edge_size = 16\n",
    "    self.overlapping_patches = True\n",
    "    self.randomize_patches = True\n",
    "    self.patch_variance_threshold = 0.0\n",
    "    self.lpf_data = False # whitening automatically includes lpf\n",
    "    self.lpf_cutoff = 0.7\n",
    "    self.batch_size = 100\n",
    "    self.random_seed = rand_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_params = params()\n",
    "analysis_params.projects_dir = os.path.expanduser(\"~\")+\"/Work/Projects/\"\n",
    "\n",
    "model_names = ['lca_768_vh']#, 'sae_768_vh', 'rica_768_vh']\n",
    "model_types = ['LCA']#, 'SAE', 'ICA']\n",
    "analyzers = []\n",
    "for model_type, model_name in zip(model_types, model_names):\n",
    "    analysis_params.model_name = model_name\n",
    "    analysis_params.version = '0.0'\n",
    "    analysis_params.model_dir = analysis_params.projects_dir+analysis_params.model_name\n",
    "    model_log_file = (analysis_params.model_dir+\"/logfiles/\"+analysis_params.model_name\n",
    "      +\"_v\"+analysis_params.version+\".log\")\n",
    "    #model_logger = tfLogger(model_log_file, overwrite=False)\n",
    "    #model_log_text = model_logger.load_file()\n",
    "    #model_params = model_logger.read_params(model_log_text)[-1]\n",
    "    analysis_params.model_type = model_type\n",
    "    analyzer = ap.get_analyzer(analysis_params.model_type)\n",
    "    analysis_params.save_info = \"analysis_selectivity\"\n",
    "    analyzer.setup(analysis_params)\n",
    "    analyzer.model_type = model_type\n",
    "    analyzer.setup_model(analyzer.model_params)\n",
    "    analyzers.append(analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ds.get_data(analysis_params)\n",
    "data = analyzers[0].model.preprocess_dataset(data, analysis_params)\n",
    "data = analyzers[0].model.reshape_dataset(data, analysis_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VH data examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_imgs = 6\n",
    "img_idx = np.random.randint(analysis_params.batch_size)\n",
    "fig, axs = plot.subplots(ncols=6)\n",
    "for inc_img in range(num_imgs):\n",
    "    im = axs[inc_img].imshow(data['train'].images[img_idx+inc_img,...].reshape(16, 16), cmap='greys_r')\n",
    "axs.format(suptitle=f'DSC van hateren example')\n",
    "pf.clear_axes(axs)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [np.squeeze(analyzer.eval_analysis(data['train'].images[0,...][None,...], ['lca/weights/w:0'], analyzer.analysis_params.save_info)['lca/weights/w:0']) for analyzer in analyzers]\n",
    "lca_weights = weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [np.squeeze(analyzer.compute_activations(data['train'].images[0:100,...],\n",
    "    activation_operation=analyzer.model.get_encodings))\n",
    "    for analyzer in analyzers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def compute_lambda_activations(images, model, weights, batch_size=None, activation_operation=None):\n",
    "    \"\"\"\n",
    "    Computes the output code for a set of images.\n",
    "    Outputs:\n",
    "      evaluated activation_operation on the input images\n",
    "    Inputs:\n",
    "      images [np.ndarray] of shape (num_imgs, num_img_pixels)\n",
    "      batch_size [int] how many inputs to use in a batch\n",
    "      activation_operation [tf operation] that produces the output activation\n",
    "        if None then it defaults to `model.get_encodings()`\n",
    "    \"\"\"\n",
    "    if activation_operation is None:\n",
    "        activation_operation = model.get_encodings\n",
    "    images_shape = list(images.shape)\n",
    "    num_images = images_shape[0]\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.compat.v1.Session(config=config, graph=model.graph) as sess:\n",
    "        if batch_size is not None and batch_size < num_images:\n",
    "            assert num_images % batch_size == 0, (\n",
    "                \"batch_size=%g must divide evenly into num_images=%g\"%(batch_size, num_images))\n",
    "            num_batches = int(np.ceil(num_images / batch_size))\n",
    "            batch_image_shape = [batch_size] + images_shape[1:]\n",
    "            sess.run(model.init_op, {model.input_placeholder:np.zeros(batch_image_shape)})\n",
    "            activations = []\n",
    "            for batch_idx in range(num_batches):\n",
    "                im_batch_start_idx = int(batch_idx * batch_size)\n",
    "                im_batch_end_idx = int(np.min([im_batch_start_idx + batch_size, num_images]))\n",
    "                batch_images = images[im_batch_start_idx:im_batch_end_idx, ...]\n",
    "                feed_dict = model.get_feed_dict(batch_images, is_test=True)\n",
    "                feed_dict[model.weight_placeholder] = weights\n",
    "                outputs = sess.run(activation_operation(), feed_dict)\n",
    "                activations.append(outputs.copy())\n",
    "            activations = np.stack(activations, axis=0)\n",
    "            num_batches, batch_size, num_outputs = activations.shape\n",
    "            activations = activations.reshape((num_batches*batch_size, num_outputs))\n",
    "        else:\n",
    "            feed_dict = model.get_feed_dict(images, is_test=True)\n",
    "            feed_dict[model.weight_placeholder] = weights\n",
    "            sess.run(model.init_op, feed_dict)\n",
    "            activations = sess.run(activation_operation(), feed_dict)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lamb_activation = lambda x : tf.identity(x) # linear\n",
    "lambda_params = pp.get_params(\"lambda\")\n",
    "lambda_params.set_data_params(\"vanhateren\")\n",
    "lambda_params.batch_size = 100\n",
    "lambda_params.data_shape = [lambda_params.patch_edge_size**2] # assumes vector inputs (i.e. not convoultional)\n",
    "lambda_params.activation_function = lamb_activation\n",
    "lambda_model = mp.get_model(\"lambda\")\n",
    "lambda_model.setup(lambda_params)\n",
    "\n",
    "linear_activations = compute_lambda_activations(data['train'].images[0:100,...], lambda_model, lca_weights, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
