{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import tensorflow as tf\n",
    "\n",
    "root_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "if root_path not in sys.path: sys.path.append(root_path)\n",
    "\n",
    "import DeepSparseCoding.tf1x.data.data_selector as ds\n",
    "import DeepSparseCoding.tf1x.utils.data_processing as dp\n",
    "import DeepSparseCoding.tf1x.utils.plot_functions as pf\n",
    "import DeepSparseCoding.tf1x.analysis.analysis_picker as ap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: this notebook uses old parameter specifications. New (class) parameter specifications are required for the notebook to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_params = [{\n",
    "  \"model_type\": \"ica\",\n",
    "  \"model_name\": \"ica\",\n",
    "  \"version\": \"0.0\",\n",
    "  \"weights\": \"weights/w_analysis:0\",\n",
    "  \"save_info\": \"analysis\",\n",
    "  \"overwrite_analysis\": False},\n",
    "\n",
    "  {\"model_type\": \"sparse_autoencoder\",\n",
    "  \"model_name\": \"sparse_autoencoder\",\n",
    "  \"version\": \"0.0\",\n",
    "  \"weights\": \"weights/w_enc:0\",\n",
    "  \"save_info\": \"analysis\",\n",
    "  \"overwrite_analysis\": False},\n",
    "\n",
    "  {\"model_type\": \"lca\",\n",
    "  \"model_name\": \"lca_256_l0_2.5\",\n",
    "  \"version\": \"1.0\",\n",
    "  \"weights\": \"weights/phi:0\",\n",
    "  \"save_info\": \"full_imgs\",\n",
    "  \"overwrite_analysis\": False}]\n",
    "\n",
    "# Computed params\n",
    "for params in analysis_params:\n",
    "  params[\"model_dir\"] = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+params[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class params_obj(object):\n",
    "  def __init__(self, dictionary):\n",
    "    for key, val in dictionary.items():\n",
    "      setattr(self, key, val)\n",
    "analyzers = []\n",
    "for params in analysis_params:\n",
    "  pobj = params_obj(params)\n",
    "  analyzer = ap.get_analyzer(pobj)\n",
    "  analyzer.model.setup(analyzer.model_params)\n",
    "  analyzer.load_analysis(save_info=pobj.save_info)\n",
    "  analyzers.append(analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_angle_list = []\n",
    "for analyzer in analyzers:\n",
    "  num_pixels = analyzer.model_params.num_pixels\n",
    "  neuron_angles = np.zeros((analyzer.bf_stats[\"num_outputs\"], analyzer.bf_stats[\"num_outputs\"]))\n",
    "  for neuron1 in range(analyzer.bf_stats[\"num_outputs\"]):\n",
    "    #for neuron2 in range(0,neuron1):\n",
    "    #  neuron_angles[neuron1, neuron2] = None\n",
    "    #for neuron2 in range(neuron1, analyzer.bf_stats[\"num_outputs\"]):\n",
    "    for neuron2 in range(analyzer.bf_stats[\"num_outputs\"]):\n",
    "      bf1 = analyzer.bf_stats[\"basis_functions\"][neuron1].reshape((num_pixels,1))\n",
    "      bf2 = analyzer.bf_stats[\"basis_functions\"][neuron2].reshape((num_pixels,1))\n",
    "      inner_products = np.dot((bf1/np.linalg.norm(bf1)).T, bf2/np.linalg.norm(bf2))\n",
    "      inner_products[inner_products>1.0] = 1.0\n",
    "      inner_products[inner_products<-1.0] = -1.0\n",
    "      angle = np.arccos(inner_products)\n",
    "      neuron_angles[neuron1, neuron2] = angle\n",
    "  neuron_angle_list.append(neuron_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('viridis')\n",
    "vmin = 0\n",
    "vmax = np.pi\n",
    "cNorm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "scalarMap = matplotlib.cm.ScalarMappable(norm=cNorm, cmap=cmap)\n",
    "scalarMap._A = []\n",
    "fig, ax = plt.subplots(1, len(neuron_angle_list)+1, figsize=(14,14))\n",
    "fig.subplots_adjust(wspace=1.0)\n",
    "for idx, neuron_angles in enumerate(neuron_angle_list):\n",
    "  im = ax[idx].imshow(neuron_angles, clim=[vmin,vmax])\n",
    "  ax[idx].set_xticks([0, analyzers[0].bf_stats[\"num_outputs\"]-1])\n",
    "  ax[idx].tick_params(\"both\", labelsize=10)\n",
    "  ax[idx].get_yaxis().set_visible(False)\n",
    "  ax[idx].set_title(analyzers[idx].model_name, fontsize=10)\n",
    "ax[0].set_yticks([0, analyzers[0].bf_stats[\"num_outputs\"]-1])\n",
    "ax[0].get_yaxis().set_visible(True)\n",
    "im = ax[-1].imshow(np.zeros_like(neuron_angles), vmin=vmin, vmax=vmax)\n",
    "ax[-1].set_visible(False)\n",
    "cbar = pf.add_colorbar_to_im(im, ticks=[vmin, vmax])\n",
    "cbar.ax.set_yticklabels([\"0\",\"pi\"])\n",
    "fig.suptitle(\"Angles between neurons\", y=0.6, x=0.4, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(neuron_angle_list), figsize=(5,5*len(neuron_angle_list)))\n",
    "#fig.subplots_adjust(hspace=0.1)\n",
    "for idx, neuron_angles in enumerate(neuron_angle_list):\n",
    "  angles = neuron_angles[np.isfinite(neuron_angles)].flatten()*(180/np.pi)\n",
    "  xlims = [0, np.max(angles)]\n",
    "  ax[idx].hist(angles, rwidth=0.5, log=True)\n",
    "  ax[idx].set_xlim(xlims)\n",
    "  ax[idx].set_ylabel(\"Count\\n\"+analyzers[idx].model_name, fontsize=14)\n",
    "ax[0].set_title(\"Neuron angle histogram\", fontsize=16)\n",
    "ax[-1].set_xlabel(\"Angle (Degrees)\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(analyzers), figsize=(5, 5*len(analyzers)))\n",
    "for idx, analyzer in enumerate(analyzers):\n",
    "  nn_angles = np.zeros(analyzer.bf_stats[\"num_outputs\"])\n",
    "  for neuron_id in range(analyzer.bf_stats[\"num_outputs\"]): \n",
    "    neighbors = neuron_angle_list[idx][neuron_id,:] * (180/np.pi)\n",
    "    nn_angles[neuron_id] = np.min(np.delete(neighbors, neuron_id))\n",
    "  ax[idx].hist(nn_angles.flatten(), rwidth=0.5, bins=np.linspace(0,90,10), log=True)\n",
    "  ax[idx].set_xlim([0, np.max(nn_angles)])\n",
    "  ax[idx].set_ylabel(\"Count\\n\"+analyzer.model_name, fontsize=14)\n",
    "ax[0].set_title(\"Neuron Nearest Neighbor Angles\", fontsize=16)\n",
    "ax[-1].set_xlabel(\"Angle (Degrees)\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx, analyzer in enumerate(analyzers):\n",
    "  weight_name = analysis_params[idx][\"weights\"]\n",
    "  weight_shape = [analyzer.bf_stats[\"num_outputs\"], analyzer.bf_stats[\"patch_edge_size\"],\n",
    "    analyzer.bf_stats[\"patch_edge_size\"]]\n",
    "  dict_fig = pf.plot_weights(analyzer.evals[weight_name].T.reshape(weight_shape),\n",
    "    title=analyzer.model_name+\" Weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loc_freq_summaries(analyzer_list, spot_size=13, spot_alpha=0.5):\n",
    "  max_sfs = []\n",
    "  max_envelopes = []\n",
    "  fig, sub_ax = plt.subplots(1, 3, figsize=(15,5))\n",
    "  for analyzer in analyzer_list:\n",
    "    bf_stats = analyzer.bf_stats\n",
    "    x_pos = [x for (y,x) in bf_stats[\"gauss_centers\"]]\n",
    "    y_pos = [y for (y,x) in bf_stats[\"gauss_centers\"]]\n",
    "    sub_ax[0].scatter(x_pos, y_pos, s=spot_size, alpha=spot_alpha, label=analyzer.model_name)\n",
    "    sub_ax[0].xaxis.set_major_locator(matplotlib.ticker.MaxNLocator(integer=True))\n",
    "    sub_ax[0].yaxis.set_major_locator(matplotlib.ticker.MaxNLocator(integer=True))\n",
    "    x_sf = [x for (y,x) in bf_stats[\"fourier_centers\"]]\n",
    "    y_sf = [y for (y,x) in bf_stats[\"fourier_centers\"]]\n",
    "    max_sfs.append(np.max(np.abs(x_sf+y_sf)))\n",
    "    sub_ax[1].scatter(x_sf, y_sf, s=spot_size, alpha=spot_alpha, label=analyzer.model_name)\n",
    "    sub_ax[1].xaxis.set_major_locator(matplotlib.ticker.MaxNLocator(integer=True))\n",
    "    sub_ax[1].yaxis.set_major_locator(matplotlib.ticker.MaxNLocator(integer=True))\n",
    "    widths = [width for ((width, height), _) in bf_stats[\"gauss_orientations\"]]\n",
    "    heights = [height for ((width, height), _) in bf_stats[\"gauss_orientations\"]]\n",
    "    max_envelopes.append(np.max([widths, heights]))\n",
    "    sub_ax[2].scatter(widths, heights, s=spot_size, alpha=spot_alpha, label=analyzer.model_name)\n",
    "  patch_edge_size = analyzers[0].bf_stats[\"patch_edge_size\"]\n",
    "  sub_ax[0].set_xlim([0, patch_edge_size-1])\n",
    "  sub_ax[0].set_ylim([patch_edge_size-1, 0])\n",
    "  sub_ax[0].set_aspect(\"equal\")\n",
    "  sub_ax[0].set_ylabel(\"Pixels\")\n",
    "  sub_ax[0].set_xlabel(\"Pixels\")\n",
    "  sub_ax[0].set_title(\"Basis Function Centers\", fontsize=12)\n",
    "  sub_ax[1].set_aspect(\"equal\")\n",
    "  sub_ax[1].set_ylabel(\"Cycles / Patch\")\n",
    "  sub_ax[1].set_xlabel(\"Cycles / Patch\")\n",
    "  sub_ax[1].set_title(\"Basis Function Spatial Frequencies\", fontsize=12)\n",
    "  sub_ax[1].set_xlim([-np.max(max_sfs), np.max(max_sfs)])\n",
    "  sub_ax[1].set_ylim([-np.max(max_sfs), np.max(max_sfs)])\n",
    "  sub_ax[2].set_aspect(\"equal\")\n",
    "  sub_ax[2].set_ylabel(\"Envelope Length\")\n",
    "  sub_ax[2].set_xlabel(\"Envelope Width\")\n",
    "  sub_ax[2].set_title(\"Basis Function Spatial Receptive Field\", fontsize=12)\n",
    "  sub_ax[2].set_xlim([0, patch_edge_size])\n",
    "  sub_ax[2].set_ylim([0, patch_edge_size])\n",
    "  sub_ax[2].legend()\n",
    "  plt.show()\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_loc_freq_summaries(analyzers)\n",
    "fig.savefig(\"/home/dpaiton/fig_location_frequency_centers.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_loc = \"/home/dpaiton/Work/DeepSparseCoding/data_batches.npz\"\n",
    "#data_batches = []\n",
    "#for analyzer in analyzers:\n",
    "#  data = ds.get_data(analyzer.model_params)\n",
    "#  data = analyzer.model.preprocess_dataset(data)\n",
    "#  data = analyzer.model.reshape_dataset(data, analyzer.model_params)\n",
    "#  analyzer.model_params.data_shape = [data[\"train\"].num_rows*data[\"train\"].num_cols*data[\"train\"].num_channels]     \n",
    "#  data_batches.append(data[\"train\"].next_batch(1000)[0])\n",
    "#np.savez(file_loc, data=data_batches)\n",
    "data_batches = np.load(file_loc)[\"data\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[\"train\"].w_filter\n",
    "inv_w_filter = np.where(data[\"train\"].w_filter == 0, np.zeros_like(data[\"train\"].w_filter), 1/data[\"train\"].w_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising\n",
    "* It is unclear whether we should be whitening the stimulus\n",
    "  * LCA assumes a standard normal noise model, so it denoises best when the noise is white\n",
    "  * This means that we would want to add white noise to the whitened images. If we add white noise to the images then whiten, we will be coloring the white noise\n",
    "  * If we add noise to the whitened images, how would we compare to other models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = [\"output/image_estimate/reconstruction:0\"]\n",
    "noise_std = 0.01\n",
    "noise_mean = 0.0\n",
    "mse_list = []\n",
    "for data_batch, analyzer in zip(data_batches, analyzers):\n",
    "  noisy_data_batch = data_batch + analyzer.rand_state.normal(noise_mean, noise_std, data_batch.shape)\n",
    "  recons = analyzer.evaluate_model(noisy_data_batch, var_names)[var_names[0]]\n",
    "  mse_list.append(dp.compute_mse(data_batch, recons))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
