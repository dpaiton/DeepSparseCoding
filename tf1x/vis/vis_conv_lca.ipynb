{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                                              \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "from scipy.ndimage import imread as imread\n",
    "import tensorflow as tf                                                         \n",
    "\n",
    "root_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "if root_path not in sys.path: sys.path.append(root_path)\n",
    "\n",
    "import DeepSparseCoding.tf1x.data.data_selector as ds\n",
    "import DeepSparseCoding.tf1x.utils.data_processing as dp\n",
    "import DeepSparseCoding.tf1x.utils.plot_functions as pf\n",
    "import DeepSparseCoding.tf1x.analysis.analysis_picker as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class analysis_params(object):\n",
    "  model_type = \"conv_lca\"\n",
    "  model_name = \"conv_lca\"\n",
    "  version = \"0.0\"\n",
    "  save_info = \"analysis\"\n",
    "\n",
    "# Computed params\n",
    "analysis_params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"\n",
    "  +analysis_params.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analyzer = ap.get_analyzer(analysis_params)\n",
    "analyzer.model.setup(analyzer.model_params)\n",
    "analyzer.load_analysis(save_info=analysis_params.save_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pixels = analyzer.model_params.patch_size_x*analyzer.model_params.patch_size_y\n",
    "weights = analyzer.evals[\"weights/phi:0\"].reshape((num_pixels, analyzer.model_params.num_neurons))\n",
    "dict_fig = pf.plot_data_tiled(weights.T, normalize=False, title=\"Weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(analyzer.adversarial_target_adv_mses, 'b')\n",
    "ax.plot(analyzer.adversarial_input_adv_mses, 'r')\n",
    "ax.plot(analyzer.adversarial_target_recon_mses, 'g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(np.squeeze(analyzer.adversarial_images[-1]), cmap=\"Greys_r\")\n",
    "ax[1].imshow(np.squeeze(analyzer.adversarial_recons[-1]), cmap=\"Greys_r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_movie(frames):\n",
    "  fig, ax = plt.subplots()\n",
    "  im = ax.imshow(np.zeros_like(frames[0]), cmap=\"Greys_r\", vmin=0.0, vmax=1.0)\n",
    "  \n",
    "  def init():\n",
    "    im.set_data(np.zeros_like(frames[0]))\n",
    "    return (im,)\n",
    "    \n",
    "  def animate(i):\n",
    "    im.set_data(frames[i])\n",
    "    return (im,)\n",
    "  \n",
    "  anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "    frames=len(frames), interval=20, blit=True)\n",
    "  return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = imread(analyzer.analysis_out_dir+\"/vis/inputs/downtown_sf.png\", flatten=True)[:,:,None]\n",
    "#input_image = dp.downsample_data(input_image, scale_factor=[0.5, 0.5, 1], order=3) # 128x128\n",
    "#input_image = imread(analyzer.analysis_out_dir+\"/vis/inputs/coffee_mug.png\", flatten=True)[:,:,None]\n",
    "input_image /= 255.0\n",
    "wht_input_image, input_mean, input_filter = dp.whiten_data(input_image, method=\"FT\")\n",
    "unwht_input = dp.unwhiten_data(wht_input_image, input_mean, input_filter, method=\"FT\")\n",
    "\n",
    "target_image = imread(analyzer.analysis_out_dir+\"/vis/inputs/trees.png\", flatten=True)[:,:,None]\n",
    "#target_image = dp.downsample_data(target_image, scale_factor=[0.5, 0.5, 1], order=3) # 128x128\n",
    "#target_image = imread(analyzer.analysis_out_dir+\"/vis/inputs/donut.png\", flatten=True)[:,:,None]\n",
    "target_image /= 255.0\n",
    "wht_target_image, target_mean, target_filter = dp.whiten_data(target_image, method=\"FT\")\n",
    "unwht_target = dp.unwhiten_data(wht_target_image, target_mean, target_filter, method=\"FT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.do_adversaries = True\n",
    "analyzer.setup_model(analyzer.model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_images, recons, mses = analyzer.construct_adversarial_stimulus(wht_input_image[None,...], wht_target_image[None,...],\n",
    "  eps=0.001, num_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwht_adv_images = [np.squeeze(dp.unwhiten_data(image, input_mean, input_filter, method=\"FT\")) for image in adv_images]\n",
    "norm_adv_images = [np.squeeze((image-np.min(image))/(np.max(image)-np.min(image))) for image in unwht_adv_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwht_recons = [np.squeeze(dp.unwhiten_data(image, input_mean, input_filter, method=\"FT\")) for image in recons]\n",
    "norm_recons = [np.squeeze((image-np.min(image))/(np.max(image)-np.min(image))) for image in unwht_recons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_interp = [np.squeeze((1-a)*input_image+a*target_image) for a in np.linspace(0, 1, 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = make_movie(norm_adv_images)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_bars(weights, bf_idx=0):\n",
    "  bf_stats = dp.get_dictionary_stats(weights, padding=32)\n",
    "  center = bf_stats[\"gauss_centers\"][bf_idx]\n",
    "  orientations = bf_stats[\"fourier_centers\"][bf_idx]\n",
    "  angle = np.rad2deg(np.pi/2 + np.arctan2(*orientations)) \n",
    "  patch_edge_size = bf_stats[\"patch_edge_size\"]\n",
    "  vals = np.linspace(0,patch_edge_size,patch_edge_size)\n",
    "  X,Y = np.meshgrid(vals,vals)\n",
    "  output = np.zeros((patch_edge_size, patch_edge_size))\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bf_idx = 35\n",
    "bf = bf_stats[\"basis_functions\"][bf_idx]\n",
    "center = bf_stats[\"gauss_centers\"][bf_idx]\n",
    "evals, evecs = bf_stats[\"gauss_orientations\"][bf_idx]\n",
    "width, height = evals\n",
    "min_length = 0.3\n",
    "if width < height:\n",
    "  width = min_length\n",
    "elif width > height:\n",
    "  height = min_length\n",
    "orientations = bf_stats[\"fourier_centers\"][bf_idx]\n",
    "patch_edge_size = bf_stats[\"patch_edge_size\"]\n",
    "\n",
    "y0,x0 = center\n",
    "x = np.linspace(0, patch_edge_size, 100)  # x values of interest\n",
    "y = np.linspace(0, patch_edge_size, 100)[:,None]  # y values of interest, as a \"column\" array\n",
    "angle = np.pi/2+np.arctan2(*orientations)\n",
    "ellipse = ((np.cos(angle)*(x-x0) + np.sin(angle)*(y-y0))**2/width**2\n",
    "  + (np.sin(angle)*(x-x0) - np.cos(angle)*(y-y0))**2/height**2) <= 1\n",
    "\n",
    "fig, ax = plt.subplots(1,3)\n",
    "ax[0] = pf.clear_axis(ax[0], spines=\"k\")\n",
    "ax[0].imshow(bf, cmap=\"Greys_r\", interpolation=\"Nearest\")\n",
    "ax[1] = pf.clear_axis(ax[1], spines=\"k\")\n",
    "ax[1].imshow(ellipse)\n",
    "ax[2] = pf.clear_axis(ax[2], spines=\"k\")\n",
    "el = pf.plot_ellipse(ax[2], center, evals, np.rad2deg(angle), color_val=\"b\", alpha=1.0, lines=True)\n",
    "ax[2].set_xlim(0, patch_edge_size-1)\n",
    "ax[2].set_ylim(patch_edge_size-1, 0)\n",
    "ax[2].set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_stats = plot_weight_bars(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = ds.get_data(analyzer.model_params)\n",
    "#data = analyzer.model.preprocess_dataset(data, analyzer.model_params)\n",
    "#data = analyzer.model.reshape_dataset(data, analyzer.model_params)\n",
    "#\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.imshow(data[\"train\"].images[0,...])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Analysis params:\\n\")\n",
    "#print(\"\\n\".join([key.ljust(20)+\"\\t\"+str(getattr(analyzer.model_params, key)) for key in analyzer.model_params.__dict__.keys()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "a_vals during inference are extremely high for conv lca on cifar\n",
    "this is causing inf in loss values\n",
    "need to train new conv lca on VH whitened\n",
    "figure out a_vals for those\n",
    "what in conv LCA would be causing the values to be so high??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_fig = pf.plot_stats(analyzer.run_stats,\n",
    "  keys=[\"a_fraction_active\", \"recon_loss\", \"sparse_loss\", \"total_loss\"],\n",
    "  labels=[\"activity\", \"recon loss\", \"sparse loss\", \"total loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits import axes_grid1\n",
    "import utils.data_processing as dp\n",
    "\n",
    "def plot_inference_stats(data, title=\"\", save_filename=None):\n",
    "  \"\"\"\n",
    "  Plot loss values during LCA inference\n",
    "  Inputs:\n",
    "    data: [dict] that must contain the \"losses\"\n",
    "      this can be created by using the LCA analyzer objects\n",
    "  \"\"\"\n",
    "  labels = [key for key in data[\"losses\"].keys()]\n",
    "  losses = [val for val in data[\"losses\"].values()]\n",
    "  num_im, num_steps = losses[0].shape\n",
    "  means = [None,]*len(labels)\n",
    "  sems = [None,]*len(labels)\n",
    "  for loss_id, loss in enumerate(losses):\n",
    "    means[loss_id] = np.mean(loss, axis=0) # mean across num_imgs\n",
    "    sems[loss_id] = np.std(loss, axis=0) / np.sqrt(num_im)\n",
    "  num_plots_y = np.int32(np.ceil(np.sqrt(len(labels))))+1\n",
    "  num_plots_x = np.int32(np.ceil(np.sqrt(len(labels))))\n",
    "  gs = gridspec.GridSpec(num_plots_y, num_plots_x)\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  loss_id = 0\n",
    "  for plot_id in np.ndindex((num_plots_y, num_plots_x)):\n",
    "    (y_id, x_id) = plot_id\n",
    "    ax = fig.add_subplot(gs[plot_id])\n",
    "    if loss_id < len(labels):\n",
    "      time_steps = np.arange(num_steps)\n",
    "      ax.plot(time_steps, means[loss_id], \"k-\")\n",
    "      ax.fill_between(time_steps, means[loss_id]-sems[loss_id],\n",
    "        means[loss_id]+sems[loss_id], alpha=0.2)\n",
    "      ax.set_ylabel(labels[loss_id].replace('_', ' '), fontsize=16)\n",
    "      ax.set_xlim([1, np.max(time_steps)])\n",
    "      ax.set_xticks([1, int(np.floor(np.max(time_steps)/2)), np.max(time_steps)])\n",
    "      ax.set_xlabel(\"Time Step\", fontsize=16)\n",
    "      ax.tick_params(\"both\", labelsize=14)\n",
    "      loss_id += 1\n",
    "    else:\n",
    "      ax = pf.clear_axis(ax, spines=\"none\")\n",
    "  fig.tight_layout()\n",
    "  fig.suptitle(title, y=1.03, x=0.5, fontsize=20)\n",
    "  if save_filename is not None:\n",
    "    fig.savefig(save_filename, transparent=True)\n",
    "    plt.close(fig)\n",
    "    return None\n",
    "  plt.show()\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pf.plot_inference_stats(analyzer.inference_stats, title=\"Loss During Inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_pixels, num_neurons = analyzer.atas.shape\n",
    "atas_fig = pf.plot_data_tiled(analyzer.atas.T.reshape(num_neurons,\n",
    "  int(np.sqrt(num_pixels)), int(np.sqrt(num_pixels))), normalize=False,\n",
    "  title=\"Activity triggered averages on image data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_images = np.random.standard_normal(data[\"train\"].images.shape) \n",
    "noise_evals = analyzer.evaluate_model(noise_images, analyzer.var_names)\n",
    "noise_atas = analyzer.compute_atas(noise_evals[\"inference/activity:0\"],\n",
    "  noise_images)\n",
    "noise_atas_fig = pf.plot_data_tiled(noise_atas.T.reshape(num_neurons,\n",
    "  int(np.sqrt(num_pixels)), int(np.sqrt(num_pixels))), normalize=False,\n",
    "  title=\"Activity triggered averages on standard normal noise data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inference_fig = pf.plot_inference_traces(analyzer.inference_stats, analyzer.model_schedule[0][\"sparse_mult\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = pf.plot_inference_stats(analyzer.inference_stats, title=\"Loss During Inference\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
