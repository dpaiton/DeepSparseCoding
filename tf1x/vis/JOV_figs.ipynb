{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journal of Vision Paper Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "root_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "if root_path not in sys.path: sys.path.append(root_path)\n",
    "#%env CUDA_VISIBLE_DEVICES=0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from functools import reduce as reduce\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "from skimage.util import crop\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker as plticker\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "import matplotlib.font_manager\n",
    "import matplotlib.cm as cm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import proplot as plot\n",
    "\n",
    "import response_contour_analysis.utils.histogram_analysis as ha\n",
    "import response_contour_analysis.utils.dataset_generation as dg\n",
    "import response_contour_analysis.utils.plotting as resp_pf\n",
    "\n",
    "from DeepSparseCoding.tf1x.utils.logger import Logger\n",
    "from DeepSparseCoding.tf1x.data.dataset import Dataset\n",
    "import DeepSparseCoding.tf1x.data.data_selector as ds\n",
    "import DeepSparseCoding.tf1x.analysis.analysis_picker as ap\n",
    "import DeepSparseCoding.tf1x.utils.data_processing as dp\n",
    "import DeepSparseCoding.tf1x.utils.plot_functions as pf\n",
    "import DeepSparseCoding.tf1x.utils.jov_funcs as nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "textwidth in pt: 540.60236pt\n",
    "textwidth in cm: 18.9973cm\n",
    "textwidth in in: 7.48178in\n",
    "\"\"\"\n",
    "text_width = 540.60236 #pt 416.83269 #pt = 14.65cm\n",
    "text_width_cm = 18.9973 # 14.705\n",
    "fontsize = 10\n",
    "dpi = 300\n",
    "file_extensions = ['.pdf']#, '.eps', '.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_settings = {\n",
    "        \"text.usetex\": True,\n",
    "        \"font.family\": 'serif',\n",
    "        \"font.serif\": 'Computer Modern Roman',\n",
    "        \"axes.labelsize\": fontsize,\n",
    "        \"axes.titlesize\": fontsize,\n",
    "        \"figure.titlesize\": fontsize+2,\n",
    "        \"font.size\": fontsize,\n",
    "        \"legend.fontsize\": fontsize,\n",
    "        \"xtick.labelsize\": fontsize-2,\n",
    "        \"ytick.labelsize\": fontsize-2,\n",
    "}\n",
    "mpl.rcParams.update(font_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lca_512_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_512_vh\"\n",
    "    self.display_name = \"Sparse Coding 512\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_768_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_768_vh\"\n",
    "    self.display_name = \"Sparse Coding\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_1024_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_1024_vh\"\n",
    "    self.display_name = \"Sparse Coding 1024\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_2560_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_2560_vh\"\n",
    "    self.display_name = \"Sparse Coding\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class sae_768_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"sae\"\n",
    "    self.model_name = \"sae_768_vh\"\n",
    "    self.display_name = \"Sparse\\nAutoencoder\"\n",
    "    self.version = \"1.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class rica_768_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"rica\"\n",
    "    self.model_name = \"rica_768_vh\"\n",
    "    self.display_name = \"Linear\\nAutoencoder\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class ae_768_vh_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"ae\"\n",
    "    self.model_name = \"ae_768_vh\"\n",
    "    self.display_name = \"ReLU\\nAutoencoder\"\n",
    "    self.version = \"1.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_768_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_768_mnist\"\n",
    "    self.display_name = \"Sparse Coding 768\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class lca_1536_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"lca\"\n",
    "    self.model_name = \"lca_1536_mnist\"\n",
    "    self.display_name = \"Sparse Coding 1536\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class ae_768_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"ae\"\n",
    "    self.model_name = \"ae_768_mnist\"\n",
    "    self.display_name = \"Leaky ReLU\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class sae_768_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"sae\"\n",
    "    self.model_name = \"sae_768_mnist\"\n",
    "    self.display_name = \"Sparse Autoencoder\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class rica_768_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"rica\"\n",
    "    self.model_name = \"rica_768_mnist\"\n",
    "    self.display_name = \"Linear Autoencoder\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_train_kurakin_targeted\"\n",
    "    self.overwrite_analysis_log = False\n",
    "\n",
    "class ae_deep_mnist_params(object):\n",
    "  def __init__(self):\n",
    "    self.model_type = \"ae\"\n",
    "    self.model_name = \"ae_deep_mnist\"\n",
    "    self.display_name = \"Leaky ReLU\"\n",
    "    self.version = \"0.0\"\n",
    "    self.save_info = \"analysis_test_carlini_targeted\"\n",
    "    self.overwrite_analysis_log = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_levels = 10\n",
    "color_vals = dict(zip([\"blk\", \"lt_green\", \"md_green\", \"dk_green\", \"lt_blue\", \"md_blue\", \"dk_blue\", \"lt_red\", \"md_red\", \"dk_red\"],\n",
    "  [\"#000000\", \"#A9DFBF\", \"#196F3D\", \"#27AE60\", \"#AED6F1\", \"#3498DB\", \"#21618C\", \"#F5B7B1\", \"#E74C3C\", \"#943126\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_analysis(params):\n",
    "  params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+params.model_name)\n",
    "  analyzer = ap.get_analyzer(params.model_type)\n",
    "  analyzer.setup(params)\n",
    "  analyzer.model.setup(analyzer.model_params)\n",
    "  analyzer.load_analysis(save_info=params.save_info)\n",
    "  analyzer.model_name = params.model_name\n",
    "  return analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_analyzer_keys(analyzer):\n",
    "    # backwards compatibility\n",
    "    for name_option in ['iso_params', 'attn_params']:\n",
    "        if hasattr(analyzer, name_option):\n",
    "            if 'num_comparisons' in getattr(analyzer, name_option).keys():\n",
    "                getattr(analyzer, name_option)['num_comparison_vects'] = getattr(analyzer, name_option)['num_comparisons']\n",
    "            if 'num_comparisons' in getattr(analyzer, name_option).keys():\n",
    "                getattr(analyzer, name_option)['num_comparison_vects'] = getattr(analyzer, name_option)['num_comparisons']\n",
    "    for name_option in ['comp_contour_dataset', 'iso_comp_contour_dataset', 'rand_comp_contour_dataset']:\n",
    "        if hasattr(analyzer, name_option):\n",
    "            if 'proj_target_neuron' in getattr(analyzer, name_option).keys(): \n",
    "                analyzer.comp_contour_dataset['proj_target_vect'] = getattr(analyzer, name_option)['proj_target_neuron']\n",
    "            if 'proj_comparison_neuron' in getattr(analyzer, name_option).keys(): \n",
    "                analyzer.comp_contour_dataset['proj_comparison_vect'] = getattr(analyzer, name_option)['proj_comparison_neuron']\n",
    "    return analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iso-contour activations comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_names = ['', '', '', 'rescaled_']\n",
    "params_list = [rica_768_vh_params(), ae_768_vh_params(), sae_768_vh_params(), lca_768_vh_params()]\n",
    "\n",
    "analyzer_list = [load_analysis(params) for params in params_list]\n",
    "for analyzer, save_name in zip(analyzer_list, save_names):\n",
    "    analyzer.iso_params = np.load(analyzer.analysis_out_dir+'savefiles/iso_params_'+save_name\n",
    "        +analyzer.analysis_params.save_info+'.npz', allow_pickle=True)['data'].item()\n",
    "    x_range = analyzer.iso_params['x_range']\n",
    "    y_range = analyzer.iso_params['y_range']\n",
    "\n",
    "    iso_vectors = np.load(analyzer.analysis_out_dir+'savefiles/iso_vectors_'+save_name\n",
    "        +analyzer.analysis_params.save_info+'.npz', allow_pickle=True)['data'].item()\n",
    "    analyzer.target_neuron_ids = iso_vectors['target_neuron_ids']\n",
    "    analyzer.comparison_neuron_ids = iso_vectors['comparison_neuron_ids']\n",
    "    analyzer.target_vectors = iso_vectors['target_vectors']\n",
    "    analyzer.rand_orth_vectors = iso_vectors['rand_orth_vectors']\n",
    "    analyzer.comparison_vectors = iso_vectors['comparison_vectors']\n",
    "  \n",
    "    analyzer.comp_activations = np.load(analyzer.analysis_out_dir+'savefiles/iso_comp_activations_'+save_name\n",
    "        +analyzer.analysis_params.save_info+'.npz', allow_pickle=True)['data']\n",
    "    \n",
    "    analyzer.comp_contour_dataset = np.load(analyzer.analysis_out_dir+'savefiles/iso_comp_contour_dataset_'+save_name\n",
    "        +analyzer.analysis_params.save_info+'.npz', allow_pickle=True)['data'].item()\n",
    "\n",
    "    analyzer.rand_activations = np.load(analyzer.analysis_out_dir+'savefiles/iso_rand_activations_'+save_name\n",
    "        +analyzer.analysis_params.save_info+'.npz', allow_pickle=True)['data']\n",
    "    analyzer.rand_contour_dataset = np.load(analyzer.analysis_out_dir+'savefiles/iso_rand_contour_dataset_'+save_name\n",
    "        +analyzer.analysis_params.save_info+'.npz', allow_pickle=True)['data'].item()\n",
    "    \n",
    "    analyzer = add_analyzer_keys(analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_act = 0.5 # target activity spot between min & max value of normalized activity (btwn 0 and 1)\n",
    "lca_activations = analyzer_list[-1].comp_activations\n",
    "curvatures, fits, contours = ha.iso_response_curvature_poly_fits(\n",
    "  lca_activations,\n",
    "  target_act=target_act\n",
    ")\n",
    "max_comp_indices = []\n",
    "max_vals = []\n",
    "for target_neuron_id in range(len(curvatures)):\n",
    "    max_idx = np.argmax(curvatures[target_neuron_id])\n",
    "    max_comp_indices.append(max_idx)\n",
    "    max_vals.append(curvatures[target_neuron_id][max_idx])\n",
    "max_target_id = np.argmax(max_vals)\n",
    "max_comparison_id = max_comp_indices[max_target_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_angles = [[]]\n",
    "target_min_idx = []\n",
    "for target_neuron_id in range(len(analyzer_list[-1].target_vectors)):\n",
    "    target_vect = dg.normalize_vector(analyzer_list[-1].target_vectors[target_neuron_id]).reshape((-1, 1))\n",
    "    for comparison_vect in analyzer_list[-1].comparison_vectors[target_neuron_id]:\n",
    "        comparison_vect = dg.normalize_vector(comparison_vect).reshape((-1, 1))\n",
    "        angle = dg.angle_between_vectors(target_vect, comparison_vect) * (180 / np.pi)\n",
    "        all_angles[-1].append(angle.item())\n",
    "    target_min_idx.append(np.argmin(all_angles[-1][:analyzer_list[-1].iso_params['num_comparison_vects']]))\n",
    "    all_angles.append([])\n",
    "min_target = []\n",
    "for target_angles, min_id in zip(all_angles, target_min_idx):\n",
    "    min_target.append(target_angles[min_id])\n",
    "min_target_id = np.argmin(min_target)\n",
    "min_comparison_id = target_min_idx[min_target_id]\n",
    "\n",
    "# 8(.039), 17(.028) 23(.037), 25(.036), 41(.033), 48(.035), 49(0.039)\n",
    "neuron_indices = [0, 0, 0, max_target_id]#min_target_id]\n",
    "orth_indices = [0, 0, 0, max_comparison_id]#min_comparison_id]\n",
    "num_plots_y = 2\n",
    "num_plots_x = 2\n",
    "width_fraction = 1.0\n",
    "show_contours = True\n",
    "\n",
    "lca_activations = analyzer_list[-1].comp_activations[neuron_indices[-1], orth_indices[-1], ...][None, None, ...]\n",
    "curvatures, fits, contours = ha.iso_response_curvature_poly_fits(\n",
    "  lca_activations,\n",
    "  target_act=target_act\n",
    ")\n",
    "curvature = [None, None, None, curvatures[0][0]]\n",
    "\n",
    "#for analyzer in analyzer_list:\n",
    "#    analyzer.comp_activations = analyzer.comp_activations - analyzer.comp_activations.min()\n",
    "#    analyzer.comp_activations = analyzer.comp_activations / analyzer.comp_activations.max()\n",
    "\n",
    "contour_fig, contour_handles = nc.plot_group_iso_contours(analyzer_list, neuron_indices, orth_indices,\n",
    "  num_levels, x_range, y_range, show_contours, curvature, text_width, width_fraction, dpi)\n",
    "\n",
    "for analyzer, neuron_index, orth_index, save_suffix in zip(analyzer_list, neuron_indices, orth_indices, save_names):\n",
    "    for ext in file_extensions:\n",
    "        neuron_str = str(analyzer.target_neuron_ids[neuron_index])\n",
    "        orth_str = str(analyzer.comparison_neuron_ids[neuron_index][orth_index])\n",
    "        save_name = analyzer.analysis_out_dir+\"/vis/iso_contour_comparison_\"\n",
    "        if not show_contours:\n",
    "            save_name += \"continuous_\"\n",
    "        save_name += \"bf0id\"+neuron_str+\"_bf1id\"+orth_str+\"_\"+save_suffix+analyzer.analysis_params.save_info+ext\n",
    "        contour_fig.savefig(save_name, dpi=dpi, transparent=False, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curvature histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width_fraction = 1.0\n",
    "show_contours = True\n",
    "num_levels = 10\n",
    "num_x = 5\n",
    "num_y = 5\n",
    "iso_save_name='rescaled_closecomp_'\n",
    "params = lca_2560_vh_params()\n",
    "\n",
    "analyzer = load_analysis(params)\n",
    "\n",
    "cont_analysis = np.load(\n",
    "    analyzer.analysis_out_dir+'savefiles/group_iso_vectors_'\n",
    "    +iso_save_name+analyzer.analysis_params.save_info+'.npz',\n",
    "    allow_pickle=True)['data'].item()\n",
    "\n",
    "curvatures = cont_analysis['curvatures']\n",
    "target_act = cont_analysis['target_act']\n",
    "\n",
    "contour_fig, contour_handles = nc.plot_iso_contour_set(\n",
    "    cont_analysis,\n",
    "    curvatures,\n",
    "    num_levels,\n",
    "    num_x,\n",
    "    num_y,\n",
    "    show_contours,\n",
    "    text_width,\n",
    "    1.00,\n",
    "    dpi\n",
    ")\n",
    "\n",
    "for ext in file_extensions:\n",
    "    save_name = analyzer.analysis_out_dir+\"/vis/scaled_iso_contours_set_\"\n",
    "    if not show_contours:\n",
    "        save_name += \"continuous_\"\n",
    "    save_name += analyzer.analysis_params.save_info+ext\n",
    "    contour_fig.savefig(save_name, dpi=dpi, transparent=False, bbox_inches=\"tight\", pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = [lca_512_vh_params(), lca_1024_vh_params(), lca_2560_vh_params()]\n",
    "iso_save_name = 'rescaled_randomcomp_'#\"iso_curvature_xrange1.3_yrange-2.2_\"\n",
    "attn_save_name = 'rescaled_randomcomp_'#'1d_'\n",
    "\n",
    "for params in params_list:\n",
    "  params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+params.model_name)\n",
    "\n",
    "analyzer_list = [ap.get_analyzer(params.model_type) for params in params_list]\n",
    "for analyzer, params in zip(analyzer_list, params_list):\n",
    "    analyzer.setup(params)\n",
    "    analyzer.model.setup(analyzer.model_params)\n",
    "    analyzer.load_analysis(save_info=params.save_info)\n",
    "    analyzer.model_name = params.model_name\n",
    "    \n",
    "    analyzer.iso_params = np.load(analyzer.analysis_out_dir+\"savefiles/iso_params_\"+iso_save_name+analyzer.analysis_params.save_info+\".npz\", allow_pickle=True)[\"data\"].item()\n",
    "    analyzer = add_analyzer_keys(analyzer)\n",
    "\n",
    "    analyzer.iso_comp_activations = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_activations_\"+iso_save_name+analyzer.analysis_params.save_info+\".npz\", allow_pickle=True)[\"data\"]\n",
    "\n",
    "    analyzer.iso_comp_contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_contour_dataset_\"+iso_save_name+analyzer.analysis_params.save_info+\".npz\", allow_pickle=True)[\"data\"].item()\n",
    "\n",
    "    analyzer.iso_rand_activations = np.load(analyzer.analysis_out_dir+\"savefiles/iso_rand_activations_\"+iso_save_name+analyzer.analysis_params.save_info+\".npz\", allow_pickle=True)[\"data\"]\n",
    "\n",
    "    analyzer.iso_rand_contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_rand_contour_dataset_\"+iso_save_name+analyzer.analysis_params.save_info+\".npz\", allow_pickle=True)[\"data\"].item()\n",
    "\n",
    "    analyzer.iso_num_target_neurons = analyzer.iso_params[\"num_neurons\"]\n",
    "    analyzer.iso_num_comparison_vectors = analyzer.iso_params[\"num_comparison_vects\"]\n",
    "\n",
    "    analyzer.attn_params = np.load(analyzer.analysis_out_dir+\"savefiles/iso_params_\"+attn_save_name+analyzer.analysis_params.save_info+\".npz\", allow_pickle=True)[\"data\"].item()\n",
    "    analyzer = add_analyzer_keys(analyzer)\n",
    "\n",
    "    analyzer.attn_comp_activations = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_activations_\"+attn_save_name+analyzer.analysis_params.save_info+\".npz\", allow_pickle=True)[\"data\"]\n",
    "      \n",
    "    analyzer.attn_comp_contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_contour_dataset_\"+attn_save_name+analyzer.analysis_params.save_info+\".npz\", allow_pickle=True)[\"data\"].item()\n",
    "\n",
    "    analyzer.attn_rand_activations = np.load(analyzer.analysis_out_dir+\"savefiles/iso_rand_activations_\"+attn_save_name+analyzer.analysis_params.save_info+\".npz\", allow_pickle=True)[\"data\"]\n",
    "\n",
    "    analyzer.attn_rand_contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_rand_contour_dataset_\"+attn_save_name+analyzer.analysis_params.save_info+\".npz\", allow_pickle=True)[\"data\"].item()\n",
    "    \n",
    "    analyzer.attn_num_target_neurons = analyzer.attn_params[\"num_neurons\"]\n",
    "    analyzer.attn_num_comparison_vectors = analyzer.attn_params[\"num_comparison_vects\"]\n",
    "    \n",
    "    analyzer = add_analyzer_keys(analyzer)\n",
    "    \n",
    "mesh_save_name = \"iso_curvature_ryan_\"\n",
    "contour_activity = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_activations_\"+mesh_save_name\n",
    "    +analyzer.analysis_params.save_info+\".npz\", allow_pickle=True)[\"data\"][0, 1, ...]\n",
    "comp_contour_dataset = np.load(analyzer.analysis_out_dir+\"savefiles/iso_comp_contour_dataset_\"+mesh_save_name\n",
    "  +analyzer.analysis_params.save_info+\".npz\", allow_pickle=True)[\"data\"].item()\n",
    "x = range(comp_contour_dataset[\"x_pts\"].size)\n",
    "y = range(comp_contour_dataset[\"y_pts\"].size)\n",
    "contour_pts = (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 50\n",
    "target_act = 0.5\n",
    "nc.compute_curvature_fits(analyzer_list, target_act)\n",
    "nc.compute_curvature_hists(analyzer_list, num_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [[\"2x\", \"4x\", \"10x\"]]*2\n",
    "color_list = [[color_vals[\"lt_red\"], color_vals[\"md_red\"], color_vals[\"dk_red\"]],\n",
    "  [color_vals[\"lt_blue\"], color_vals[\"md_blue\"], color_vals[\"dk_blue\"]]]\n",
    "curve_lims = { \n",
    "    \"x\":[min(comp_contour_dataset[\"x_pts\"]), max(comp_contour_dataset[\"x_pts\"])],\n",
    "    \"y\":[min(comp_contour_dataset[\"y_pts\"]), max(comp_contour_dataset[\"y_pts\"])]\n",
    "}\n",
    "\n",
    "iso_title = \"Iso-Response\"\n",
    "iso_hist_list = [[analyzer.iso_comp_hist for analyzer in analyzer_list],\n",
    "  [analyzer.iso_rand_hist for analyzer in analyzer_list]]\n",
    "plot_bin_lefts, plot_bin_rights = analyzer_list[0].iso_bin_edges[:-1], analyzer_list[0].iso_bin_edges[1:]\n",
    "iso_plot_bin_centers = plot_bin_lefts + (plot_bin_rights - plot_bin_lefts)\n",
    "\n",
    "attn_title = \"Response Attenuation\"\n",
    "attn_hist_list = [[analyzer.attn_comp_hist for analyzer in analyzer_list],\n",
    "  [analyzer.attn_rand_hist for analyzer in analyzer_list]]\n",
    "plot_bin_lefts, plot_bin_rights = analyzer_list[0].attn_bin_edges[:-1], analyzer_list[0].attn_bin_edges[1:]\n",
    "attn_plot_bin_centers = plot_bin_lefts + (plot_bin_rights - plot_bin_lefts)\n",
    "\n",
    "full_hist_list = [iso_hist_list, attn_hist_list]\n",
    "full_label_list = [label_list,]*2\n",
    "full_color_list = [color_list,]*2\n",
    "full_bin_centers = [iso_plot_bin_centers, attn_plot_bin_centers]\n",
    "full_title = [iso_title, attn_title]\n",
    "full_xlabel = [\"Curvature (Comparison)\", \"Curvature (Random)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scatter = False\n",
    "mesh_color = \"#A6A6A6\"\n",
    "contour_angle = 200 #195\n",
    "view_elevation = 25 #30\n",
    "# The following two variables set the curvature line label locations.\n",
    "# The numbering is sort of [z x y], where z is up-down, x is into the page, y is right-left\n",
    "resp_att_loc = [108, 38, 0.85]#[105, 38, 0.8] \n",
    "iso_resp_loc = [-27, 20, 0.11]#[-18, 20, 0.11]\n",
    "activity_loc = [-27, 150, 1.5]\n",
    "contour_text_loc = [iso_resp_loc, resp_att_loc, activity_loc]\n",
    "\n",
    "curvature_log_fig = nc.plot_curvature_histograms(contour_activity, contour_pts, contour_angle, view_elevation, \n",
    "    contour_text_loc, full_hist_list, full_label_list, full_color_list, mesh_color, full_bin_centers,\n",
    "    full_title, full_xlabel, curve_lims, scatter, log=True, text_width=text_width, width_ratio=0.75, dpi=dpi)\n",
    "\n",
    "for analyzer in analyzer_list:\n",
    "    for ext in file_extensions:\n",
    "        save_name = (analyzer.analysis_out_dir+\"/vis/\"+iso_save_name+\"curvatures_and_histograms_logy\"\n",
    "            +\"_\"+analyzer.analysis_params.save_info+ext)\n",
    "        curvature_log_fig.savefig(save_name, transparent=False, bbox_inches=\"tight\", pad_inches=0.05, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "curvature_lin_fig = nc.plot_curvature_histograms(contour_activity, contour_pts, contour_angle, view_elevation,\n",
    "    contour_text_loc, full_hist_list, full_label_list, full_color_list, mesh_color, full_bin_centers,\n",
    "    full_title, full_xlabel, curve_lims, scatter, log=False, text_width=text_width, width_ratio=0.75, dpi=dpi)\n",
    "\n",
    "for analyzer in analyzer_list:\n",
    "    for ext in file_extensions:\n",
    "        save_name = (analyzer.analysis_out_dir+\"/vis/\"+iso_save_name+\"curvatures_and_histograms_liny\"\n",
    "            +\"_\"+analyzer.analysis_params.save_info+ext)\n",
    "        curvature_lin_fig.savefig(save_name, transparent=False, bbox_inches=\"tight\", pad_inches=0.05, dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orientation Selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_list = [rica_768_vh_params(), sae_768_vh_params(), lca_768_vh_params()]\n",
    "params_list[-1].display_name = \"Sparse Coding\"\n",
    "for params in params_list:\n",
    "  params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+params.model_name)\n",
    "analyzer_list = [ap.get_analyzer(params.model_type) for params in params_list]\n",
    "for analyzer, params in zip(analyzer_list, params_list):\n",
    "  analyzer.setup(params)\n",
    "  analyzer.model.setup(analyzer.model_params)\n",
    "  analyzer.load_analysis(save_info=params.save_info)\n",
    "  analyzer.model_name = params.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circ_var_list = []\n",
    "for analyzer in analyzer_list:\n",
    "  analyzer.bf_indices = np.random.choice(analyzer.ot_grating_responses[\"neuron_indices\"], 12)\n",
    "  num_orientation_samples = len(analyzer.ot_grating_responses['orientations'])\n",
    "  corresponding_angles_deg = (180 * np.arange(num_orientation_samples) / num_orientation_samples) - 90\n",
    "  corresponding_angles_rad = (np.pi * np.arange(num_orientation_samples) / num_orientation_samples) - (np.pi/2)\n",
    "  analyzer.metrics_list = {\"fwhm\":[], \"circ_var\":[], \"osi\":[], \"nonvarying_indices\":[]}\n",
    "  contrast_idx = -1\n",
    "  for bf_idx in range(analyzer.bf_stats[\"num_outputs\"]):\n",
    "    ot_curve = nc.center_curve(analyzer.ot_grating_responses[\"mean_responses\"][bf_idx, contrast_idx, :])\n",
    "    if np.max(ot_curve) - np.min(ot_curve) == 0:\n",
    "      analyzer.metrics_list[\"nonvarying_indices\"].append(bf_idx)\n",
    "      analyzer.metrics_list[\"circ_var\"].append([None, None, 1.0])\n",
    "    else:\n",
    "      fwhm = nc.compute_fwhm(ot_curve, corresponding_angles_deg)\n",
    "      analyzer.metrics_list[\"fwhm\"].append(fwhm)\n",
    "      circ_var = nc.compute_circ_var(ot_curve, corresponding_angles_rad)\n",
    "      analyzer.metrics_list[\"circ_var\"].append(circ_var)\n",
    "      osi = nc.compute_osi(ot_curve)\n",
    "      analyzer.metrics_list[\"osi\"].append(osi)\n",
    "  circ_var_list.append(np.array([val[2] for val in analyzer.metrics_list[\"circ_var\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "color_list = [color_vals[\"md_green\"], color_vals[\"md_blue\"], color_vals[\"md_red\"]]\n",
    "label_list = [\"Linear Autoencoder\", \"Sparse Autoencoder\", \"Sparse Coding\"]\n",
    "num_bins = 30\n",
    "width_ratios = [0.5, 0.25, 0.25]\n",
    "height_ratios = [0.13, 0.25, 0.25, 0.25]\n",
    "density = False\n",
    "\n",
    "circ_var_fig = nc.plot_circ_variance_histogram(analyzer_list, circ_var_list, color_list, label_list, num_bins,\n",
    "  density, width_ratios, height_ratios, text_width=text_width, width_ratio=0.75, dpi=dpi)\n",
    "\n",
    "for analyzer in analyzer_list:\n",
    "  for ext in file_extensions:\n",
    "    save_name = (analyzer.analysis_out_dir+\"/vis/circular_variance_combo\"\n",
    "      +\"_\"+analyzer.analysis_params.save_info+ext)\n",
    "    circ_var_fig.savefig(save_name, transparent=False, bbox_inches=\"tight\", pad_inches=0.01, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_frequencies = np.stack([np.array(analyzer.bf_stats[\"spatial_frequencies\"]) for analyzer in analyzer_list], axis=0)\n",
    "circular_variances = np.stack([variance for variance in circ_var_list], axis=0)\n",
    "\n",
    "cv_vs_sf_fig = plt.figure(figsize=nc.set_size(text_width, fraction=0.75), dpi=dpi)\n",
    "ax = cv_vs_sf_fig.add_subplot()\n",
    "for analyzer_idx in range(len(analyzer_list)):\n",
    "  ax.scatter(spatial_frequencies[analyzer_idx, :], circular_variances[analyzer_idx, :],\n",
    "    s=12, color=color_list[analyzer_idx], label=label_list[analyzer_idx])\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_xlabel(\"Spatial Frequency (Cycles/Patch)\")\n",
    "ax.set_ylabel(\"Circular Variance\")\n",
    "#ax.set_title(\"Weight spatial frequency alone does\\nnot account for improved selectivity\")\n",
    "legend = ax.legend(loc=\"upper center\", framealpha=1.0, ncol=1, borderaxespad=0., borderpad=0.,\n",
    "  handlelength=0., labelspacing=0.1, bbox_to_anchor=(0.38, 0.98))\n",
    "legend.get_frame().set_linewidth(0.0)\n",
    "for text, color in zip(legend.get_texts(), color_list):\n",
    "  text.set_color(color)\n",
    "for item in legend.legendHandles:\n",
    "  item.set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "for analyzer in analyzer_list:\n",
    "  for ext in file_extensions:\n",
    "    save_name = (analyzer.analysis_out_dir+\"/vis/spatial_freq_vs_circular_variance\"\n",
    "      +\"_\"+analyzer.analysis_params.save_info+ext)\n",
    "    cv_vs_sf_fig.savefig(save_name, transparent=False, bbox_inches=\"tight\", pad_inches=0.01, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = [lca_512_vh_params(), lca_1024_vh_params(), lca_2560_vh_params()]\n",
    "display_names = [\"512 Neurons\", \"1024 Neurons\", \"2560 Neurons\"]\n",
    "for params, display_name in zip(params_list, display_names):\n",
    "  params.display_name = display_name\n",
    "  params.model_dir = (os.path.expanduser(\"~\")+\"/Work/Projects/\"+params.model_name)\n",
    "analyzer_list = [ap.get_analyzer(params.model_type) for params in params_list]\n",
    "for analyzer, params in zip(analyzer_list, params_list):\n",
    "  analyzer.setup(params)\n",
    "  analyzer.model.setup(analyzer.model_params)\n",
    "  analyzer.load_analysis(save_info=params.save_info)\n",
    "  analyzer.model_name = params.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circ_var_list = []\n",
    "for analyzer in analyzer_list:\n",
    "  analyzer.bf_indices = np.random.choice(analyzer.ot_grating_responses[\"neuron_indices\"], 12)\n",
    "  num_orientation_samples = len(analyzer.ot_grating_responses['orientations'])\n",
    "  corresponding_angles_deg = (180 * np.arange(num_orientation_samples) / num_orientation_samples) - 90\n",
    "  corresponding_angles_rad = (np.pi * np.arange(num_orientation_samples) / num_orientation_samples) - (np.pi/2)\n",
    "  analyzer.metrics_list = {\"fwhm\":[], \"circ_var\":[], \"osi\":[], \"skipped_indices\":[]}\n",
    "  contrast_idx = -1\n",
    "  for bf_idx in range(analyzer.bf_stats[\"num_outputs\"]):\n",
    "    ot_curve = nc.center_curve(analyzer.ot_grating_responses[\"mean_responses\"][bf_idx, contrast_idx, :])\n",
    "    if np.max(ot_curve) - np.min(ot_curve) == 0:\n",
    "      analyzer.metrics_list[\"skipped_indices\"].append(bf_idx)\n",
    "    else:\n",
    "      fwhm = nc.compute_fwhm(ot_curve, corresponding_angles_deg)\n",
    "      analyzer.metrics_list[\"fwhm\"].append(fwhm)\n",
    "      circ_var = nc.compute_circ_var(ot_curve, corresponding_angles_rad)\n",
    "      analyzer.metrics_list[\"circ_var\"].append(circ_var)\n",
    "      osi = nc.compute_osi(ot_curve)\n",
    "      analyzer.metrics_list[\"osi\"].append(osi)\n",
    "  circ_var_list.append(np.array([val[2] for val in analyzer.metrics_list[\"circ_var\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = [color_vals[\"md_green\"], color_vals[\"md_blue\"], color_vals[\"md_red\"]]#, color_vals[\"blk\"]]\n",
    "label_list = display_names\n",
    "num_bins = 30\n",
    "width_ratios = [0.5, 0.25, 0.25]\n",
    "height_ratios = [0.13, 0.25, 0.25, 0.25]\n",
    "density = True\n",
    "\n",
    "oc_vs_cv_fig = nc.plot_circ_variance_histogram(analyzer_list, circ_var_list, color_list, label_list, num_bins,\n",
    "  density, width_ratios, height_ratios, text_width=text_width, width_ratio=0.75, dpi=dpi)\n",
    "\n",
    "for analyzer in analyzer_list:\n",
    "  for ext in file_extensions:\n",
    "    save_name = (analyzer.analysis_out_dir+\"/vis/overcompleteness_vs_circular_variance\"\n",
    "      +\"_\"+analyzer.analysis_params.save_info+ext)\n",
    "    oc_vs_cv_fig.savefig(save_name, transparent=False, bbox_inches=\"tight\", pad_inches=0.05, dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural scene selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params_list = [lca_512_vh_params(), lca_768_vh_params(), lca_2560_vh_params()]\n",
    "model_names = ['lca_512_vh', 'lca_1024_vh', 'lca_2560_vh']\n",
    "model_types = ['LCA', 'LCA', 'LCA']\n",
    "model_labels = ['2x', '4x', '10x']\n",
    "analyzer_list = []\n",
    "for model_type, model_name, model_label, analysis_params in zip(model_types, model_names, model_labels, params_list):\n",
    "    analysis_params.projects_dir = os.path.expanduser(\"~\")+\"/Work/Projects/\"\n",
    "    analysis_params.model_name = model_name\n",
    "    analysis_params.version = '0.0'\n",
    "    analysis_params.model_dir = analysis_params.projects_dir+analysis_params.model_name\n",
    "    model_log_file = (analysis_params.model_dir+\"/logfiles/\"+analysis_params.model_name\n",
    "      +\"_v\"+analysis_params.version+\".log\")\n",
    "    analysis_params.model_type = model_type\n",
    "    analyzer = ap.get_analyzer(analysis_params.model_type)\n",
    "    analysis_params.save_info = \"analysis_selectivity\"\n",
    "    analyzer.setup(analysis_params)\n",
    "    analyzer.model_label = model_label\n",
    "    analyzer.model_type = model_type\n",
    "    analyzer.nat_selectivity = np.load(analyzer.analysis_out_dir+'savefiles/natural_image_selectivity.npz',\n",
    "        allow_pickle=True)['data'].item()\n",
    "    analyzer_list.append(analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_val_in_array(num, arr):\n",
    "    curr = arr[0]\n",
    "    for val in arr:\n",
    "        if abs(num - val) < abs(num - curr):\n",
    "            curr = val\n",
    "    curr_idx = np.argwhere(np.array(arr) == curr).item()\n",
    "    return arr[curr_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_interesting_vals = [\n",
    "    np.array([analyzer.nat_selectivity['num_interesting_img_nl'],\n",
    "    analyzer.nat_selectivity['num_interesting_img_l']])\n",
    "    for analyzer in analyzer_list]\n",
    "\n",
    "num_interesting_medians = np.stack(\n",
    "    [np.array([np.median(np.array(analyzer.nat_selectivity['num_interesting_img_nl'])),\n",
    "    np.median(np.array(analyzer.nat_selectivity['num_interesting_img_l']))])\n",
    "    for analyzer in analyzer_list], axis=0)\n",
    "\n",
    "num_interesting_means = np.stack(\n",
    "    [np.array([analyzer.nat_selectivity['num_interesting_img_nl_mean'],\n",
    "    analyzer.nat_selectivity['num_interesting_img_l_mean']])\n",
    "    for analyzer in analyzer_list], axis=0)\n",
    "\n",
    "num_interesting_stds = np.stack(\n",
    "    [np.array([analyzer.nat_selectivity['num_interesting_img_nl_std'],\n",
    "    analyzer.nat_selectivity['num_interesting_img_l_std']])\n",
    "    for analyzer in analyzer_list], axis=0)\n",
    "\n",
    "array = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "]\n",
    "\n",
    "scale = 1\n",
    "rc_kwargs = {\n",
    "    'fontsize':scale*matplotlib.rcParams['font.size'],\n",
    "    'fontfamily':scale*matplotlib.rcParams['font.family'],\n",
    "    'legend.fontsize': scale*matplotlib.rcParams['font.size'],\n",
    "    'text.labelsize': scale*matplotlib.rcParams['font.size']\n",
    "}\n",
    "figsize = nc.set_size(text_width, fraction=1.00)\n",
    "with plot.rc.context(**rc_kwargs):\n",
    "    interesting_imgs_fig, axs = plot.subplots(array, sharey=False, sharex=False, aspect=3.0, figsize=figsize)\n",
    "    for ovc_idx, overcompleteness in enumerate(num_interesting_vals):\n",
    "        ax = axs[ovc_idx]\n",
    "        df = pd.DataFrame(\n",
    "            overcompleteness.T,\n",
    "            columns=pd.Index(['Sparse Coding', 'Linear'])#, name='xlabel')\n",
    "        )\n",
    "        box_parts = ax.boxplot(\n",
    "            df,\n",
    "            notch=True,\n",
    "            fill=False,\n",
    "            whis=(5, 95),\n",
    "            marker='*',\n",
    "            markersize=1.0,\n",
    "            lw=1.2\n",
    "        )\n",
    "        colors = ['md_red', 'md_green']\n",
    "        for pc_idx, box in enumerate(box_parts['boxes']):\n",
    "            box.set_color(color_vals[colors[pc_idx]])\n",
    "        ax.format(\n",
    "            ylocator=50,\n",
    "            ylim=[0, np.max([np.max(val) for val in num_interesting_vals])],\n",
    "            title=analyzer_list[ovc_idx].nat_selectivity['oc_label'],\n",
    "            ylabel='Average number of\\nintersting images',\n",
    "            xtickminor=False,\n",
    "            xgrid=False\n",
    "        )\n",
    "\n",
    "    for idx, analyzer in enumerate(analyzer_list):\n",
    "        ax = axs[idx+3]\n",
    "        angle_min = 0.0\n",
    "        angle_max = 90.0\n",
    "        nbins=20\n",
    "        bins = np.linspace(angle_min, angle_max, nbins)\n",
    "        lin_data = [mean for mean in analyzer.nat_selectivity['lin_means'] if mean>0]\n",
    "        non_lin_data = [mean for mean in analyzer.nat_selectivity['lca_means'] if mean>0]\n",
    "        hist_list = []\n",
    "        color_list = [color_vals['md_green'], color_vals['md_red']]\n",
    "        label_list = ['Linear Autoencoder', 'Sparse Coding']\n",
    "        handles = []\n",
    "        hist_max_list = []\n",
    "        for angles, label, color in zip([lin_data, non_lin_data], label_list, color_list):\n",
    "          # density means the y vals are probability density function at the bin, normalized such that the integral over the range is 1.\n",
    "          hist, bin_edges = np.histogram(np.array(angles).flatten(), bins, density=False)\n",
    "          hist_max_list.append(hist.max())\n",
    "          hist_list.append(hist)\n",
    "          bin_left, bin_right = bin_edges[:-1], bin_edges[1:]\n",
    "          bin_centers = bin_left + (bin_right - bin_left)/2\n",
    "          handles.append(ax.plot(bin_centers, hist, linestyle='-', drawstyle='steps-mid', color=color, label=label))\n",
    "        oc = analyzer.nat_selectivity['oc_label']\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.set_xticks(bin_left, minor=True)\n",
    "        ax.set_xticks(bin_left[::2], minor=False)\n",
    "        ax.xaxis.set_major_formatter(plticker.FormatStrFormatter('%0.0f'))\n",
    "        ax.set_xticks([angle_min, angle_max//2, angle_max])\n",
    "        mid_val = max(hist_max_list)//2\n",
    "        max_val = int(max(hist_max_list))\n",
    "        #interval_list = list(range(0, mid_val+51, 50))\n",
    "        #new_mid = closest_val_in_array(mid_val, interval_list)\n",
    "        interval_list = list(range(0, max_val+51, 50))\n",
    "        new_max = closest_val_in_array(max_val, interval_list)\n",
    "        new_mid = new_max//2\n",
    "        ax.set_ylim([0, new_max+0.1*new_max])\n",
    "        ax.set_yticks([0, new_mid, new_max])\n",
    "    #axs[-1].legend(handles, ncol=1, frameon=False, loc='ur', bbox_to_anchor=[1, 1.02])\n",
    "    hist_ax_idx = 3\n",
    "    axs[hist_ax_idx].format(ylabel='Total number of\\ninteresting images')\n",
    "    axs[hist_ax_idx:].format(\n",
    "        suptitle='Sparse Coding Increases Neuron Selectivity for Natural Signals',\n",
    "        xlabel='Mean image-to-weight angle',\n",
    "        xlim=[0, 90],\n",
    "        ygrid=False\n",
    "    )\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for analyzer in analyzer_list:\n",
    "    for ext in file_extensions:\n",
    "        save_name = (analyzer.analysis_out_dir+'/vis/natural_img_selectivity_box_'\n",
    "            +analyzer.analysis_params.save_info+ext)\n",
    "        interesting_imgs_fig.savefig(save_name, transparent=False, pad_inches=0.005, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nbins=20\n",
    "\n",
    "color_list = [color_vals['md_green'], color_vals['md_red']]\n",
    "label_list = ['Linear Autoencoder', 'Sparse Coding']\n",
    "\n",
    "num_interesting_vals = [\n",
    "    np.array([analyzer.nat_selectivity['num_interesting_img_nl'],\n",
    "    analyzer.nat_selectivity['num_interesting_img_l']])\n",
    "    for analyzer in analyzer_list]\n",
    "\n",
    "num_interesting_medians = np.stack(\n",
    "    [np.array([analyzer.nat_selectivity['num_interesting_img_nl_mean'],\n",
    "    analyzer.nat_selectivity['num_interesting_img_l_mean']])\n",
    "    for analyzer in analyzer_list], axis=0)\n",
    "\n",
    "num_interesting_means = np.stack(\n",
    "    [np.array([analyzer.nat_selectivity['num_interesting_img_nl_mean'],\n",
    "    analyzer.nat_selectivity['num_interesting_img_l_mean']])\n",
    "    for analyzer in analyzer_list], axis=0)\n",
    "\n",
    "num_interesting_stds = np.stack(\n",
    "    [np.array([analyzer.nat_selectivity['num_interesting_img_nl_std'],\n",
    "    analyzer.nat_selectivity['num_interesting_img_l_std']])\n",
    "    for analyzer in analyzer_list], axis=0)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    num_interesting_means,\n",
    "    index=pd.Index(['',]*3, name=''),#'Overcompleteness'),\n",
    "    columns=['LCA', 'Linear']\n",
    ")\n",
    "\n",
    "array = [\n",
    "    [1, 1, 1],\n",
    "    [2, 3, 4],\n",
    "]\n",
    "\n",
    "scale = 1\n",
    "rc_kwargs = {\n",
    "    'fontsize':scale*matplotlib.rcParams['font.size'],\n",
    "    'fontfamily':scale*matplotlib.rcParams['font.family'],\n",
    "    'legend.fontsize': scale*matplotlib.rcParams['font.size'],\n",
    "    'text.labelsize': scale*matplotlib.rcParams['font.size']\n",
    "}\n",
    "figsize = nc.set_size(text_width, fraction=1.00)\n",
    "with plot.rc.context(**rc_kwargs):\n",
    "    interesting_imgs_fig, axs = plot.subplots(array, sharey=False, aspect=3.0, figsize=figsize)#, width=0.4*text_width_cm)\n",
    "    ax = axs[0]\n",
    "    obj = ax.bar(\n",
    "        df,\n",
    "        width=0.6,\n",
    "        cycle=[color_vals['md_red'], color_vals['md_green']],\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    half_bar_width = np.abs(obj[1].patches[0].xy[0] - obj[0].patches[0].xy[0])/2\n",
    "    lca_bar_locs = [patch.xy[0]+half_bar_width for patch in obj[0].patches]\n",
    "    lin_bar_locs = [patch.xy[0]+half_bar_width for patch in obj[1].patches]\n",
    "    ax.errorbar(lca_bar_locs, num_interesting_means[:,0] , yerr=num_interesting_stds[:,0], color='k', fmt='.')\n",
    "    ax.errorbar(lin_bar_locs, num_interesting_means[:,1] , yerr=num_interesting_stds[:,1], color='k', fmt='.')\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "\n",
    "    ax.legend(obj, frameon=False, loc='ur', bbox_to_anchor=[1,1.02])\n",
    "    ax.format(\n",
    "        xlocator=1,\n",
    "        xminorlocator=0.5,\n",
    "        ytickminor=False,\n",
    "        #ylim=[0, np.max(num_interesting_means)+np.max(num_interesting_stds)],\n",
    "        #suptitle='Average number of intersting images'\n",
    "        ylabel='Average number of\\nintersting images',\n",
    "        xgrid=False\n",
    "    )\n",
    "    hist_max_list = []\n",
    "    for idx, analyzer in enumerate(analyzer_list):\n",
    "        ax = axs[idx+1]\n",
    "        angle_min = 0.0\n",
    "        angle_max = 90.0\n",
    "        bins = np.linspace(angle_min, angle_max, nbins)\n",
    "        lin_data = [mean for mean in analyzer.nat_selectivity['lin_means'] if mean>0]\n",
    "        non_lin_data = [mean for mean in analyzer.nat_selectivity['lca_means'] if mean>0]\n",
    "        hist_list = []\n",
    "        for angles, label, color in zip([lin_data, non_lin_data], label_list, color_list):\n",
    "          # density means the y vals are probability density function at the bin, normalized such that the integral over the range is 1.\n",
    "          hist, bin_edges = np.histogram(np.array(angles).flatten(), bins, density=False)\n",
    "          hist_max_list.append(hist.max())\n",
    "          hist_list.append(hist)\n",
    "          bin_left, bin_right = bin_edges[:-1], bin_edges[1:]\n",
    "          bin_centers = bin_left + (bin_right - bin_left)/2\n",
    "          ax.plot(bin_centers, hist, linestyle='-', drawstyle='steps-mid', color=color, label=label)\n",
    "        oc = analyzer.nat_selectivity['oc_label']\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.set_xticks(bin_left, minor=True)\n",
    "        ax.set_xticks(bin_left[::2], minor=False)\n",
    "        ax.xaxis.set_major_formatter(plticker.FormatStrFormatter('%0.0f'))\n",
    "        ax.set_xticks([angle_min, angle_max//2, angle_max])\n",
    "        ax.set_ylim([0, max(hist_max_list)+0.1*max(hist_max_list)])\n",
    "        ax.set_yticks([0, max(hist_max_list)//2, int(max(hist_max_list))])\n",
    "        ax.format(title=f'{oc}\\n')#, ygrid=False)\n",
    "        #ax.grid(b=False, which='both', axis='both')\n",
    "    axs[1].format(ylabel='Total number of\\ninteresting images')\n",
    "    axs[1:].format(\n",
    "        suptitle='Sparse Coding Increases Neuron Selectivity for Natural Signals',\n",
    "        xlabel='Mean image-to-weight angle',\n",
    "        xlim=[0, 90],\n",
    "        ygrid=False\n",
    "    )\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for analyzer in analyzer_list:\n",
    "    for ext in file_extensions:\n",
    "        save_name = (analyzer.analysis_out_dir+'/vis/natural_img_selectivity_bar_'\n",
    "            +analyzer.analysis_params.save_info+ext)\n",
    "        interesting_imgs_fig.savefig(save_name, transparent=False, pad_inches=0.005, dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence attacks on MLP & LCA network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_adv_indices(softmax_conf, all_kept_indices, confidence_threshold, num_images, labels):\n",
    "  softmax_conf[np.arange(num_images, dtype=np.int32), labels] = 0 # zero confidence at true label\n",
    "  confidence_indices = np.max(softmax_conf, axis=-1) # highest non-true label confidence\n",
    "  adversarial_labels = np.argmax(softmax_conf, axis=-1) # index of highest non-true label\n",
    "  all_above_thresh = np.nonzero(np.squeeze(confidence_indices>confidence_threshold))[0]\n",
    "  keep_indices = np.array([], dtype=np.int32)\n",
    "  for adv_index in all_above_thresh:\n",
    "    if adv_index not in set(all_kept_indices):\n",
    "      keep_indices = np.append(keep_indices, adv_index)\n",
    "  return keep_indices, confidence_indices, adversarial_labels\n",
    "\n",
    "def find_untargeted_conf_index(analysis): # for untargeted attacks\n",
    "  labels = dp.one_hot_to_dense(analysis['input_labels'].astype(np.int32))\n",
    "  store_time_step = -1*np.ones(data.shape[0], dtype=np.int32)\n",
    "  store_labels = np.zeros(data.shape[0], dtype=np.int32)\n",
    "  store_confidence = np.zeros(data.shape[0], dtype=np.float32)\n",
    "  store_mses = np.zeros(data.shape[0], dtype=np.float32)\n",
    "  all_kept_indices = []\n",
    "  for adv_step in range(1, analysis['num_steps']+1): # first one is original\n",
    "    keep_indices, confidence_indices, adversarial_labels = get_adv_indices(\n",
    "      analysis['adversarial_outputs'][0, adv_step, ...],\n",
    "      all_kept_indices,\n",
    "      analysis['confidence_threshold'],\n",
    "      labels.shape[0],\n",
    "      labels)\n",
    "    if keep_indices.size > 0:\n",
    "      all_kept_indices.extend(keep_indices)\n",
    "      store_time_step[keep_indices] = adv_step\n",
    "      store_confidence[keep_indices] = confidence_indices[keep_indices]\n",
    "      store_mses[keep_indices] = analysis['adversarial_input_adv_mses'][0, adv_step, keep_indices]\n",
    "      store_labels[keep_indices] = adversarial_labels[keep_indices]\n",
    "  batch_indices = np.arange(labels.shape[0], dtype=np.int32)[:,None]\n",
    "  failed_indices = np.array([val for val in batch_indices if val not in all_kept_indices])\n",
    "  if len(failed_indices) > 0:\n",
    "    store_confidence[failed_indices] = confidence_indices[failed_indices]\n",
    "    store_labels[failed_indices] = adversarial_labels[failed_indices]\n",
    "    store_mses[failed_indices] = analysis['adversarial_input_adv_mses'][0, -1, failed_indices]\n",
    "  output = {}\n",
    "  output['adversarial_time_step'] = [store_time_step]\n",
    "  output['adversarial_confidence'] = [store_confidence]\n",
    "  output['failed_indices'] = [failed_indices]\n",
    "  output['success_indices'] = [list(set(all_kept_indices))]\n",
    "  output['adversarial_labels'] = [store_labels]\n",
    "  output['mean_squared_distances'] = [store_mses]\n",
    "  output['num_failed'] = [labels.shape[0] - len(set(all_kept_indices))]\n",
    "  return output\n",
    "\n",
    "def std_conf(outputs, labels, index):\n",
    "  return np.std(np.sum(outputs[index] * labels, axis=1))\n",
    "\n",
    "def get_cifar_mse_data(saved_info, model_names):\n",
    "  data = []; means = []; stds = []\n",
    "  for model_name in model_names:\n",
    "    target_adv_mses = saved_info[model_name][\"target_adv_mses\"]\n",
    "    data.append(target_adv_mses)\n",
    "    means.append(np.mean(target_adv_mses))\n",
    "    stds.append(np.std(target_adv_mses))\n",
    "  return (data, means, stds)\n",
    "\n",
    "def adjacent_values(vals, q1, q3):\n",
    "    upper_adjacent_value = q3 + (q3 - q1) * 1.5\n",
    "    upper_adjacent_value = np.clip(upper_adjacent_value, q3, vals[-1])\n",
    "    lower_adjacent_value = q1 - (q3 - q1) * 1.5\n",
    "    lower_adjacent_value = np.clip(lower_adjacent_value, vals[0], q1)\n",
    "    return lower_adjacent_value, upper_adjacent_value\n",
    "\n",
    "def make_violin(ax, group_data, group_means, x_pos, bar_width, color='k', plot_means=True, plot_medians=False):\n",
    "  for data, means, pos in zip(group_data, group_means, x_pos):\n",
    "    parts = ax.violinplot(data, [pos], widths=bar_width,\n",
    "      showmeans=False, showextrema=False, showmedians=False, bw_method=\"silverman\")#, bw_method=0.5)\n",
    "    for pc in parts['bodies']:\n",
    "      pc.set_facecolor(color)\n",
    "      pc.set_edgecolor('k')\n",
    "      pc.set_alpha(1)\n",
    "    quartile1, medians, quartile3 = np.percentile(np.array(data), [25, 50, 75])#, axis=1)\n",
    "    whiskers = np.array([adjacent_values(data, quartile1, quartile3)])\n",
    "    whiskersMin, whiskersMax = whiskers[:, 0], whiskers[:, 1]\n",
    "    if plot_medians:\n",
    "      ax.scatter(pos, medians, marker='o', color='white', s=10, zorder=3, alpha=1)\n",
    "    if plot_means:\n",
    "      ax.scatter(pos, means, marker='o', color='white', s=10, zorder=3, alpha=1)\n",
    "    ax.vlines(pos, quartile1, quartile3, color='k', linestyle='-', lw=5, alpha=1)\n",
    "    ax.vlines(pos, whiskersMin, whiskersMax, color='k', linestyle='-', lw=1, alpha=1)\n",
    "  return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_mnist(file):\n",
    "    attack_results = np.load(file, allow_pickle=True)['data'].item()\n",
    "    input_images = attack_results['input_images'].reshape(-1, 28, 28)\n",
    "    input_labels = np.argmax(attack_results['input_labels'], axis=1)\n",
    "    adv_images = attack_results['conf_adversarial_images'][0].reshape(-1, 28, 28)\n",
    "    adv_labels = attack_results['conf_adversarial_labels'][0]\n",
    "    adv_mse = attack_results['conf_mean_squared_distances'][0]\n",
    "    adv_mse_means = np.mean(adv_mse)\n",
    "    adv_mse_stds = np.std(adv_mse)\n",
    "    return input_images, input_labels, adv_images, adv_labels, adv_mse, adv_mse_means, adv_mse_stds\n",
    "\n",
    "def get_cifar(results_dict, key):\n",
    "    attack_results = results_dict[key]\n",
    "    input_images = attack_results['orig_img'].reshape(-1, 32, 32)\n",
    "    input_labels = attack_results['orig_label']\n",
    "    adv_images = attack_results['adv_img'].reshape(-1, 32, 32)\n",
    "    adv_labels = attack_results['target_label']\n",
    "    adv_mse = attack_results['target_adv_mses']\n",
    "    adv_mse_means = np.mean(adv_mse)\n",
    "    adv_mse_stds = np.std(adv_mse)\n",
    "    return input_images, input_labels, adv_images, adv_labels, adv_mse, adv_mse_means, adv_mse_stds\n",
    "\n",
    "def get_adv_data(results_files):\n",
    "    \"\"\"\n",
    "    results_files nested lists with indices that specify [data_type][model_type][num_neurons][num_layers], for example: [mnist/cifar][mlp/lca][768/1568][2L/3L]\n",
    "    \"\"\"\n",
    "    input_images = []\n",
    "    input_labels = []\n",
    "    adv_images = []\n",
    "    adv_labels = []\n",
    "    adv_mse = []\n",
    "    adv_mse_means = []\n",
    "    adv_mse_stds = []\n",
    "    data_types = ['mnist', 'cifar']\n",
    "    for data_idx in range(len(data_types)):\n",
    "        input_images.append([])\n",
    "        input_labels.append([])\n",
    "        adv_images.append([])\n",
    "        adv_labels.append([])\n",
    "        adv_mse.append([])\n",
    "        adv_mse_means.append([])\n",
    "        adv_mse_stds.append([])\n",
    "        for model_idx in range(len(results_files[0])):\n",
    "            input_images[-1].append([])\n",
    "            input_labels[-1].append([])\n",
    "            adv_images[-1].append([])\n",
    "            adv_labels[-1].append([])\n",
    "            adv_mse[-1].append([])\n",
    "            adv_mse_means[-1].append([])\n",
    "            adv_mse_stds[-1].append([])\n",
    "            for neurons_idx in range(len(results_files[0][model_idx])):\n",
    "                input_images[-1][-1].append([])\n",
    "                input_labels[-1][-1].append([])\n",
    "                adv_images[-1][-1].append([])\n",
    "                adv_labels[-1][-1].append([])\n",
    "                adv_mse[-1][-1].append([])\n",
    "                adv_mse_means[-1][-1].append([])\n",
    "                adv_mse_stds[-1][-1].append([])\n",
    "                for layers_idx in range(len(results_files[0][model_idx][neurons_idx])):\n",
    "                    if data_types[data_idx] == 'mnist':\n",
    "                        file = results_files[0][model_idx][neurons_idx][layers_idx]\n",
    "                        outputs = get_mnist(file)\n",
    "                    elif data_types[data_idx] == 'cifar':\n",
    "                        outputs = get_cifar(results_files[1], cifar_keys[model_idx][neurons_idx][layers_idx])\n",
    "                    input_images[data_idx][model_idx][neurons_idx].append(outputs[0])\n",
    "                    input_labels[data_idx][model_idx][neurons_idx].append(outputs[1])\n",
    "                    adv_images[data_idx][model_idx][neurons_idx].append(outputs[2])\n",
    "                    adv_labels[data_idx][model_idx][neurons_idx].append(outputs[3])\n",
    "                    adv_mse[data_idx][model_idx][neurons_idx].append(outputs[4])\n",
    "                    adv_mse_means[data_idx][model_idx][neurons_idx].append(outputs[5])\n",
    "                    adv_mse_stds[data_idx][model_idx][neurons_idx].append(outputs[6])\n",
    "    return input_images, input_labels, adv_images, adv_labels, adv_mse, adv_mse_means, adv_mse_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def make_boxplot(ax, group_data, group_means, x_pos, bar_width, linewidth=2, color='k', plot_means=True, plot_medians=False):\n",
    "  boxprops = dict(linestyle='-', linewidth=linewidth, color=color)\n",
    "  whiskerprops = boxprops\n",
    "  capprops = boxprops\n",
    "  medianprops = dict(linestyle='--', linewidth=linewidth, color='k')\n",
    "  meanprops = dict(linestyle='-', linewidth=linewidth, color='k')\n",
    "  for data, means, pos in zip(group_data, group_means, x_pos): # indexing 4 neurons;layers comparisons\n",
    "    ax.boxplot(data, sym='', positions=[pos], whis=(5, 95), widths=bar_width, meanline=True, showmeans=True, boxprops=boxprops,\n",
    "      whiskerprops=whiskerprops, capprops=capprops, medianprops=medianprops, meanprops=meanprops)\n",
    "  return ax\n",
    "\n",
    "def plot_data_mse(ax, mses, means, stds, num_groups, num_per_group, bar_groups, bar_width, COLORS,\n",
    "             inner_group_names, outer_group_names, title):\n",
    "  linewidth = 1\n",
    "  for i_g in range(num_per_group): \n",
    "    group_data = [item for sublist in mses[::-1][i_g] for item in sublist]\n",
    "    group_means = [item for sublist in means[::-1][i_g] for item in sublist]\n",
    "    group_stds = [item for sublist in stds[::-1][i_g] for item in sublist]\n",
    "    x_pos = np.arange(num_groups) + i_g * bar_width\n",
    "    ax = make_boxplot(ax, group_data, group_means, x_pos, bar_width, linewidth, COLORS[i_g])\n",
    "  legend_elements = [Line2D([0], [0], color=COLORS[0], lw=8),\n",
    "                     Line2D([0], [0], color=COLORS[1], lw=8)]\n",
    "  legend = ax.legend(legend_elements, inner_group_names, framealpha=1.0)\n",
    "  legend.get_frame().set_linewidth(0.0)\n",
    "  ax.set_xticks([r + (bar_width)/2 for r in range(num_groups)])\n",
    "  ax.set_xticklabels(outer_group_names)\n",
    "  ax.set_ylabel(\"Input to Adversarial MSD\")\n",
    "  ax.set_xlabel('Number of Layers and Neurons')\n",
    "  ax.tick_params(\"x\", labelrotation=labelrotation)\n",
    "  ax.spines[\"top\"].set_visible(False)\n",
    "  ax.spines[\"right\"].set_visible(False)\n",
    "  ylim = ax.get_ylim()\n",
    "  ax.set_ylim([0, ylim[1]])\n",
    "  ax.set_title(title)\n",
    "  return ax\n",
    "\n",
    "def plot_adv_robustness(mse_list, mean_list, std_list, num_groups, num_per_group, bar_groups, bar_width,\n",
    "                        colors, inner_group_names, outer_group_names, titles, text_width=200,\n",
    "                        width_ratio=1.0, dpi=100):\n",
    "  mnist_mse, cifar_mse = mse_list\n",
    "  mnist_means, cifar_means = mean_list\n",
    "  mnist_stds, cifar_stds = std_list\n",
    "  mnist_outer_group_names, cifar_outer_group_names = outer_group_names\n",
    "  mnist_title, cifar_title = titles\n",
    "  num_y_plots = 2\n",
    "  num_x_plots = 2\n",
    "  fig = plt.figure(figsize=nc.set_size(text_width, width_ratio, [num_y_plots, num_x_plots]), dpi=dpi)\n",
    "  gs_base = plt.GridSpec(num_y_plots, num_x_plots, figure=fig, wspace=0.3)\n",
    "  ax_mnist_mse = fig.add_subplot(gs_base[:, 0])\n",
    "  ax_mnist_mse = plot_data_mse(ax_mnist_mse, mnist_mse, mnist_means, mnist_stds, num_groups, num_per_group,\n",
    "    bar_groups, bar_width, colors, inner_group_names, mnist_outer_group_names, mnist_title)\n",
    "  mnist_legend = ax_mnist_mse.get_legend()\n",
    "  mnist_legend.set_visible(False)\n",
    "  ax_cifar_mse = fig.add_subplot(gs_base[:, 1])\n",
    "  ax_cifar_mse = plot_data_mse(ax_cifar_mse, cifar_mse, cifar_means, cifar_stds, num_groups, num_per_group,\n",
    "    bar_groups, bar_width, colors, inner_group_names, cifar_outer_group_names, cifar_title)\n",
    "  ax_cifar_mse.set_ylabel(\"\")\n",
    "  ax_mnist_mse.grid(b=False, which='both', axis='both')\n",
    "  ax_cifar_mse.grid(b=False, which='both', axis='both')\n",
    "  plt.show()\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv stop confidence\n",
    "#stop_conf = 0.90#.95\n",
    "\n",
    "# path to projects directory\n",
    "projects_dir = root_path+'/Projects/'\n",
    "analysis_dir = '/analysis/0.0/'\n",
    "\n",
    "# kurakin analysis path\n",
    "#k_file_path = analysis_dir+'savefiles/class_adversary_analysis_test_kurakin_targeted.npz'\n",
    "#k_img_path = analysis_dir+'savefiles/class_adversary_images_analysis_test_kurakin_targeted.npz'\n",
    "k_file_path = analysis_dir+'savefiles/class_adversary_analysis_test_temp_kurakin_targeted.npz'\n",
    "k_img_path = analysis_dir+'savefiles/class_adversary_images_analysis_test_temp_kurakin_targeted.npz'\n",
    "\n",
    "k_file_path2 = analysis_dir+'savefiles/class_adversary_analysis_test_temp2_kurakin_targeted.npz'\n",
    "k_img_path2 = analysis_dir+'savefiles/class_adversary_images_analysis_test_temp2_kurakin_targeted.npz'\n",
    "\n",
    "# carlini analysis path\n",
    "c_file_path = analysis_dir+'savefiles/class_adversary_analysis_test_carlini_targeted.npz'\n",
    "c_img_path = analysis_dir+'savefiles/class_adversary_images_analysis_test_carlini_targeted.npz'\n",
    "\n",
    "# model names - note mnist 768 2layer was retrained and so image/label indices will not match up\n",
    "mnist_lca_768_2layer = 'slp_lca_768_latent_mnist'#'slp_lca_768_latent_75_steps_mnist'\n",
    "mnist_lca_768_3layer = 'mlp_lca_768_latent_75_steps_mnist'\n",
    "mnist_lca_1568_2layer = 'slp_lca_1568_latent_mnist'#'slp_lca_1568_latent_75_steps_mnist'\n",
    "mnist_lca_1568_3layer = 'mlp_lca_1568_latent_75_steps_mnist'\n",
    "mnist_mlp_768_2layer = 'mlp_768_mnist'#'mlp_cosyne_mnist'\n",
    "mnist_mlp_768_3layer = 'mlp_3layer_cosyne_mnist'\n",
    "mnist_mlp_1568_2layer = 'mlp_1568_mnist'\n",
    "mnist_mlp_1568_3layer = 'mlp_1568_3layer_mnist'\n",
    "\n",
    "cifar_lca_1568_2layer = 'mlp_lca_latent_cifar10_gray_2layer'\n",
    "cifar_lca_1568_3layer = 'mlp_lca_latent_cifar10_gray_3layer'\n",
    "cifar_lca_3136_2layer = 'mlp_lca_latent_cifar10_gray_3136_2layer'\n",
    "cifar_lca_3136_3layer = 'mlp_lca_latent_cifar10_gray_3136_3layer'\n",
    "cifar_mlp_1568_2layer = 'mlp_cifar10_gray_2layer'\n",
    "cifar_mlp_1568_3layer = 'mlp_cifar10_gray_3layer'\n",
    "cifar_mlp_3136_2layer = 'mlp_cifar10_gray_3136_2layer'\n",
    "cifar_mlp_3136_3layer = 'mlp_cifar10_gray_3136_3layer'\n",
    "\n",
    "output_dir = projects_dir+mnist_mlp_768_2layer+analysis_dir+'vis/'\n",
    "\n",
    "#[mlp/lca][768/1568][2L/3L]\n",
    "mnist_files = [\n",
    "    [ # mlp\n",
    "        [ # 768\n",
    "            projects_dir+mnist_mlp_768_2layer+k_file_path,\n",
    "            projects_dir+mnist_mlp_768_3layer+k_file_path\n",
    "        ], [ # 1568\n",
    "            projects_dir+mnist_mlp_1568_2layer+k_file_path,\n",
    "            projects_dir+mnist_mlp_1568_3layer+k_file_path\n",
    "        ]\n",
    "    ], [ # lca\n",
    "        [ # 768\n",
    "            projects_dir+mnist_lca_768_2layer+k_file_path,\n",
    "            projects_dir+mnist_lca_768_3layer+k_file_path\n",
    "        ], [ # 1568\n",
    "            projects_dir+mnist_lca_1568_2layer+k_file_path2,\n",
    "            projects_dir+mnist_lca_1568_3layer+k_file_path\n",
    "        ]\n",
    "        \n",
    "    ]\n",
    "]\n",
    "\n",
    "#[mlp/lca][1568/3136][2L/3L]\n",
    "cifar_keys = [\n",
    "    [ # mlp\n",
    "        [ # 1568\n",
    "            cifar_mlp_1568_2layer, cifar_mlp_1568_3layer\n",
    "        ], [ # 3136\n",
    "            cifar_mlp_3136_2layer, cifar_mlp_3136_3layer\n",
    "        ]\n",
    "    ], [ # lca\n",
    "        [ # 1568\n",
    "            cifar_lca_1568_2layer, cifar_lca_1568_3layer\n",
    "        ], [ # 3136\n",
    "            cifar_lca_3136_2layer, cifar_lca_3136_3layer\n",
    "        ]\n",
    "        \n",
    "    ]\n",
    "]\n",
    "\n",
    "#Load data\n",
    "pickle_filename = (root_path+'/DeepSparseCoding/tf1x/vis/'\n",
    "    +'vis_class_adversarial_analysis.pkl')#CIFAR10_adv_Sheng.pkl'\n",
    "with open(pickle_filename, 'rb') as f:\n",
    "    cifar_saved_info = pickle.load(f)\n",
    "    \n",
    "# file_lists is indexed [mnist/cifar][mlp/lca][768/1568][2L/3L]\n",
    "file_lists = [mnist_files, cifar_saved_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[mnist/cifar][mlp/lca][768/1568][2L/3L]\n",
    "orig_images, orig_labels, adv_images, target_labels, mses, mse_means, mse_stds = get_adv_data(file_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labelrotation = 50\n",
    "bar_width = 0.4\n",
    "inner_group_names = [\"w/ LCA\", \"w/o LCA\"]\n",
    "mnist_outer_group_names = [\"2L; 768N\", \"3L; 768N\", \"2L; 1568N\", \"3L; 1568N\"]\n",
    "cifar_outer_group_names = [\"2L; 1568N\", \"3L; 1568N\", \"2L; 3136N\", \"3L; 3136N\"]\n",
    "COLORS = [\n",
    "  [1.0, 0.0, 0.0], #\"r\"\n",
    "  [0.0, 0.0, 1.0] #\"b\"\n",
    "]\n",
    "# bar_groups are organized from left to right, with space between inner lists\n",
    "bar_groups = np.array([[0, 1], [2, 3], [4, 5], [6, 7]])\n",
    "num_groups = bar_groups.shape[0]\n",
    "num_per_group = bar_groups.shape[1]\n",
    "\n",
    "outer_group_names = [mnist_outer_group_names, cifar_outer_group_names]\n",
    "titles = [\"MNIST\", \"Grayscale CIFAR\"]\n",
    "\n",
    "adv_fig = plot_adv_robustness(mses, mse_means, mse_stds, num_groups, num_per_group, bar_groups, bar_width,\n",
    "    COLORS, inner_group_names, outer_group_names, titles, text_width, width_ratio=1.0, dpi=dpi)\n",
    "\n",
    "#for analyzer in analyzer_list:\n",
    "for ext in file_extensions:\n",
    "    save_name = (output_dir+'/adv_mse_comparison_boxplots'+ext)\n",
    "    adv_fig.savefig(save_name, transparent=False, bbox_inches='tight', pad_inches=0.05, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     56
    ]
   },
   "outputs": [],
   "source": [
    "def convert_cifar_label_set(labels):\n",
    "    return [convert_cifar_label(label) for label in labels]\n",
    "\n",
    "def convert_cifar_label(label):\n",
    "    if(label == 0 or label == \"0\"):\n",
    "        return \"airplane\"\n",
    "    if(label == 1 or label == \"1\"):\n",
    "        return \"automobile\"\n",
    "    if(label == 2 or label == \"2\"):\n",
    "        return \"bird\"\n",
    "    if(label == 3 or label == \"3\"):\n",
    "        return \"cat\"\n",
    "    if(label == 4 or label == \"4\"):\n",
    "        return \"deer\"\n",
    "    if(label == 5 or label == \"5\"):\n",
    "        return \"dog\"\n",
    "    if(label == 6 or label == \"6\"):\n",
    "        return \"frog\"\n",
    "    if(label == 7 or label == \"7\"):\n",
    "        return \"horse\"\n",
    "    if(label == 8 or label == \"8\"):\n",
    "        return \"ship\"\n",
    "    if(label == 9 or label == \"9\"):\n",
    "        return \"truck\"\n",
    "    return None\n",
    "\n",
    "def show_image_with_label(axis, image, clf, vmin=0, vmax=1, cmap=\"Greys\"):\n",
    "  im = axis.imshow(image, cmap=cmap, interpolation=\"nearest\", vmin=vmin, vmax=vmax)\n",
    "  for spine in axis.spines.values():\n",
    "    spine.set_visible(False)\n",
    "  axis.tick_params(\n",
    "    axis=\"both\",\n",
    "    bottom=\"off\",\n",
    "    top=\"off\", \n",
    "    left=\"off\",\n",
    "    right=\"off\")\n",
    "  axis.set_xticks([])\n",
    "  axis.set_yticks([])\n",
    "  props = dict(facecolor='white', alpha=1)\n",
    "  if clf is not None:\n",
    "    axis.text(.05, .95, str(clf), bbox=props,\n",
    "      verticalalignment='center', color = \"black\")\n",
    "  return im\n",
    "\n",
    "def make_grid_subplots(fig, gs, mlp_grp, lca_grp,\n",
    "                       labels,\n",
    "                       #orig_labels, target_labels, group_names,\n",
    "                       group_name_loc,\n",
    "                       orig_y_adj, start_idx=0, num_categories=3, crop_amount=0, hspace=0.5, wspace=-0.4,\n",
    "                       cmap=\"Greys_r\"):\n",
    "    \"\"\"\n",
    "    labels [mlp/lca][orig/target/plot][2L768N/2L1568N]\n",
    "    \"\"\"\n",
    "    ax_orig_list = []\n",
    "    gs_sub0_list = []\n",
    "    gs_sub1_list = []\n",
    "    for i in range(num_categories):\n",
    "        ax_orig_list.append(fig.add_subplot(gs[i, :2]))\n",
    "        gs_sub0_list.append(gridspec.GridSpecFromSubplotSpec(2, 2, gs[i, 2:4], hspace=hspace, wspace=wspace))\n",
    "        gs_sub1_list.append(gridspec.GridSpecFromSubplotSpec(2, 2, gs[i, 4:], hspace=hspace, wspace=wspace))\n",
    "    for category_idx in range(num_categories):\n",
    "        image_idx = category_idx + start_idx\n",
    "        orig_ax = ax_orig_list[category_idx]\n",
    "        if category_idx == 0:\n",
    "            orig_ax.set_title(\"Unperturbed\", y=orig_y_adj)\n",
    "        orig_img = crop(np.squeeze(mlp_grp[0][0][image_idx, ...]), crop_amount)\n",
    "        orig_im_handle = show_image_with_label(orig_ax, orig_img, labels[0][0][0][image_idx], cmap=cmap)\n",
    "        for model_idx, gs_sub in enumerate([gs_sub0_list[category_idx], gs_sub1_list[category_idx]]):\n",
    "            mlp_adv_img = crop(np.squeeze(mlp_grp[1][model_idx][image_idx, ...]), crop_amount)\n",
    "            mlp_diff_img = crop(np.squeeze(mlp_grp[2][model_idx][image_idx, ...]), crop_amount)\n",
    "            lca_adv_img = crop(np.squeeze(lca_grp[1][model_idx][image_idx, ...]), crop_amount)\n",
    "            lca_diff_img = crop(np.squeeze(lca_grp[2][model_idx][image_idx, ...]), crop_amount)\n",
    "            diff_vmin = np.min(np.concatenate((mlp_diff_img, lca_diff_img)))\n",
    "            diff_vmax = np.max(np.concatenate((mlp_diff_img, lca_diff_img)))\n",
    "            img_list = [[mlp_adv_img, lca_adv_img], [mlp_diff_img, lca_diff_img]]\n",
    "            for i in range(2): # row [adv, pert]\n",
    "                for j in range(2): # col [w/o LCA, w/ LCA]\n",
    "                    current_ax = fig.add_subplot(gs_sub[i, j])\n",
    "                    current_target_label = None\n",
    "                    current_image = img_list[i][j]\n",
    "                    if i == 0:\n",
    "                        vmin = 0.0\n",
    "                        vmax = 1.0\n",
    "                        if j == 0: # top left image\n",
    "                            if model_idx == 0:\n",
    "                                current_ax.set_ylabel(r\"$s^{*}$\")\n",
    "                            current_target_label = labels[j][1][model_idx][image_idx]\n",
    "                            if category_idx == 0: # top category only\n",
    "                                x_loc = group_name_loc[0]\n",
    "                                y_loc = group_name_loc[1]\n",
    "                                text_handle = current_ax.text(x_loc, y_loc, labels[j][2][model_idx],\n",
    "                                    horizontalalignment='left', verticalalignment='bottom')\n",
    "                    if i == 1:\n",
    "                        vmin = np.round(diff_vmin, 2)\n",
    "                        vmax = np.round(diff_vmax, 2)\n",
    "                        if j == 0 and model_idx == 0:\n",
    "                            current_ax.set_ylabel(r\"$e$\")\n",
    "                        if j == 0 and category_idx == num_categories-1: # bottom left\n",
    "                            current_ax.set_xlabel(\"w/o\\nLCA\")\n",
    "                        elif j == 1 and category_idx == num_categories-1: # bottom right\n",
    "                            current_ax.set_xlabel(\"w/\\nLCA\")\n",
    "                    im_handle = show_image_with_label(current_ax, current_image, current_target_label, vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "                    if j == 1:\n",
    "                      pf.add_colorbar_to_im(im_handle, aspect=10, ax=current_ax, ticks=[vmin, vmax])\n",
    "\n",
    "def plot_adv_images(image_groups, labels, mnist_start_idx, cifar_start_idx, text_width=200, width_ratio=1.0, dpi=100):\n",
    "    mnist_mlp_grp, mnist_lca_grp = image_groups[0]\n",
    "    cifar_mlp_grp, cifar_lca_grp = image_groups[1]\n",
    "    hspace_0 = -0.3\n",
    "    wspace_0 = 0.3\n",
    "    hspace_1 = 0.0\n",
    "    wspace_1 = 3.3\n",
    "    hspace_2 = -0.7\n",
    "    wspace_2 = 0.2\n",
    "    orig_y_adj = 1.15\n",
    "    img_label_loc = [-8.0, -8.0] # [x, y]\n",
    "    num_y_plots = 2\n",
    "    num_x_plots = 1\n",
    "    fig = plt.figure(figsize=nc.set_size(text_width, width_ratio, [num_y_plots, num_x_plots]), dpi=dpi)\n",
    "    gs_base = plt.GridSpec(num_y_plots, num_x_plots, figure=fig, wspace=wspace_0, hspace=hspace_0)\n",
    "    num_categories = 1\n",
    "    \n",
    "    gs_mnist = gridspec.GridSpecFromSubplotSpec(num_categories, 6, gs_base[0], hspace=hspace_1, wspace=wspace_1)\n",
    "    make_grid_subplots(fig, gs_mnist, mnist_mlp_grp, mnist_lca_grp,\n",
    "        labels[0], img_label_loc, orig_y_adj, mnist_start_idx,\n",
    "        num_categories, hspace=hspace_2, wspace=wspace_2, cmap=\"Greys\")\n",
    "    \n",
    "    gs_cifar = gridspec.GridSpecFromSubplotSpec(num_categories, 6, gs_base[1], hspace=hspace_1, wspace=wspace_1)\n",
    "    make_grid_subplots(fig, gs_cifar, cifar_mlp_grp, cifar_lca_grp,\n",
    "        labels[1], img_label_loc, orig_y_adj, cifar_start_idx,\n",
    "        num_categories, crop_amount=2, hspace=hspace_2, wspace=wspace_2, cmap=\"Greys_r\")\n",
    "    \n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     17,
     80
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mnist_start_idx = 37\n",
    "cifar_start_idx = 16\n",
    "\n",
    "mnist_img_labels = [mnist_outer_group_names[0], mnist_outer_group_names[2]] # only looking at 2L models\n",
    "cifar_img_labels = [cifar_outer_group_names[0], cifar_outer_group_names[2]]\n",
    "\n",
    "mnist = 0\n",
    "cifar = 1\n",
    "mlp = 0\n",
    "lca = 1\n",
    "small = 0\n",
    "large = 1\n",
    "shallow = 0\n",
    "deep = 1\n",
    "\n",
    "# from [orig/adv][mnist/cifar][mlp/lca][768/1568][2L/3L]\n",
    "# to   [mnist/cifar][mlp/lca][orig/adv/diff][2L768N/2L1568N]\n",
    "image_groups = [\n",
    "   [ #mnist \n",
    "       [ # mlp\n",
    "           [ # orig\n",
    "               orig_images[mnist][mlp][small][shallow],\n",
    "               orig_images[mnist][mlp][large][shallow]\n",
    "           ],\n",
    "           [ # adv\n",
    "               adv_images[mnist][mlp][small][shallow],\n",
    "               adv_images[mnist][mlp][large][shallow]\n",
    "           ],\n",
    "           [ # diff\n",
    "               orig_images[mnist][mlp][small][shallow] - adv_images[mnist][mlp][small][shallow],\n",
    "               orig_images[mnist][mlp][large][shallow] - adv_images[mnist][mlp][large][shallow]\n",
    "           ]\n",
    "       ],\n",
    "       [ # lca\n",
    "           [ # orig\n",
    "               orig_images[mnist][lca][small][shallow],\n",
    "               orig_images[mnist][lca][large][shallow] \n",
    "           ],\n",
    "           [ # adv\n",
    "               adv_images[mnist][lca][small][shallow],\n",
    "               adv_images[mnist][lca][large][shallow]\n",
    "           ],\n",
    "           [ # diff\n",
    "               orig_images[mnist][lca][small][shallow] - adv_images[mnist][lca][small][shallow],\n",
    "               orig_images[mnist][lca][large][shallow] - adv_images[mnist][lca][large][shallow]\n",
    "           ]\n",
    "       ]\n",
    "   ],\n",
    "   [ #cifar\n",
    "       [ # mlp\n",
    "           [ # orig\n",
    "               orig_images[cifar][mlp][small][shallow],\n",
    "               orig_images[cifar][mlp][large][shallow]\n",
    "           ],\n",
    "           [ # adv\n",
    "               adv_images[cifar][mlp][small][shallow],\n",
    "               adv_images[cifar][mlp][large][shallow]\n",
    "           ],\n",
    "           [ # diff\n",
    "               orig_images[cifar][mlp][small][shallow] - adv_images[cifar][mlp][small][shallow],\n",
    "               orig_images[cifar][mlp][large][shallow] - adv_images[cifar][mlp][large][shallow]\n",
    "           ]\n",
    "       ],\n",
    "       [ # lca\n",
    "           [ # orig\n",
    "               orig_images[cifar][lca][small][shallow],\n",
    "               orig_images[cifar][lca][large][shallow]\n",
    "           ],\n",
    "           [ # adv\n",
    "               adv_images[cifar][lca][small][shallow],\n",
    "               adv_images[cifar][lca][large][shallow]\n",
    "           ],\n",
    "           [ # diff\n",
    "               orig_images[cifar][lca][small][shallow] - adv_images[cifar][lca][small][shallow],\n",
    "               orig_images[cifar][lca][large][shallow] - adv_images[cifar][lca][large][shallow]\n",
    "           ]\n",
    "       ]\n",
    "   ]\n",
    "]\n",
    "\n",
    "label_groups = [\n",
    "   [ #mnist \n",
    "       [ # mlp\n",
    "           [ # orig\n",
    "               orig_labels[mnist][mlp][small][shallow],\n",
    "               orig_labels[mnist][mlp][large][shallow]\n",
    "           ],\n",
    "           [ # adv\n",
    "               target_labels[mnist][mlp][small][shallow],\n",
    "               target_labels[mnist][mlp][large][shallow]\n",
    "           ],\n",
    "           mnist_img_labels\n",
    "       ],\n",
    "       [ # lca\n",
    "           [ # orig\n",
    "               orig_labels[mnist][lca][small][shallow],\n",
    "               orig_labels[mnist][lca][large][shallow] \n",
    "           ],\n",
    "           [ # adv\n",
    "               target_labels[mnist][lca][small][shallow],\n",
    "               target_labels[mnist][lca][large][shallow]\n",
    "           ],\n",
    "           mnist_img_labels\n",
    "       ]\n",
    "   ],\n",
    "   [ #cifar\n",
    "       [ # mlp\n",
    "           [ # orig\n",
    "               convert_cifar_label_set(orig_labels[cifar][mlp][small][shallow]),\n",
    "               convert_cifar_label_set(orig_labels[cifar][mlp][large][shallow])\n",
    "           ],\n",
    "           [ # adv\n",
    "               convert_cifar_label_set(target_labels[cifar][mlp][small][shallow]),\n",
    "               convert_cifar_label_set(target_labels[cifar][mlp][large][shallow])\n",
    "           ],\n",
    "           cifar_img_labels\n",
    "       ],\n",
    "       [ # lca\n",
    "           [ # orig\n",
    "               convert_cifar_label_set(orig_labels[cifar][lca][small][shallow]),\n",
    "               convert_cifar_label_set(orig_labels[cifar][lca][large][shallow])\n",
    "           ],\n",
    "           [ # adv\n",
    "               convert_cifar_label_set(target_labels[cifar][lca][small][shallow]),\n",
    "               convert_cifar_label_set(target_labels[cifar][lca][large][shallow])\n",
    "           ],\n",
    "           cifar_img_labels\n",
    "       ]\n",
    "   ]\n",
    "]\n",
    "\n",
    "#labels = [orig_labels, target_labels, [mnist_img_labels, cifar_img_labels]]\n",
    "adv_img_fig = plot_adv_images(image_groups, label_groups, mnist_start_idx, cifar_start_idx, text_width, width_ratio=1.0, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for analyzer in analyzer_list:\n",
    "for ext in file_extensions:\n",
    "    save_name = (output_dir+'/adv_mse_comparison_example_images'+ext)\n",
    "    adv_img_fig.savefig(save_name, transparent=False, bbox_inches=\"tight\", pad_inches=0.05, dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Figures/Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram the MSE datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_conf_step(analysis_files, model_names):\n",
    "    fig, ax = plt.subplots()\n",
    "    for file, name in zip(analysis_files, model_names):\n",
    "        analysis = np.load(file, allow_pickle=True)[\"data\"].item()\n",
    "        mean_conf = np.mean(np.max(np.squeeze(analysis['adversarial_outputs']), axis=-1), axis=-1)[1:]\n",
    "        ax.plot(mean_conf, label=name)\n",
    "        max_conf = np.max(mean_conf)\n",
    "        print(f'Maximum confidence: {max_conf}')\n",
    "    ax.legend(loc=\"lower right\", framealpha=1.0)\n",
    "    ax.set_xlabel(\"Attack Step\")\n",
    "    ax.set_ylabel(\"Mean Target-class Confidence\")\n",
    "    ax.set_ylim(0, 1.01)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[mlp/lca][768/1568][2L/3L]\n",
    "lca_files = [mnist_files[1][0][0]]\n",
    "mlp_files = [mnist_files[0][0][0]]\n",
    "files = mlp_files + lca_files\n",
    "names = ['w/o LCA', 'w/ LCA'] \n",
    "\n",
    "conf_fig = plot_average_conf_step(files, names)\n",
    "#for analyzer in analyzer_list:\n",
    "for ext in file_extensions:\n",
    "    save_name = (output_dir+'adv_mse_comparison_example_images'+ext)\n",
    "    conf_fig.savefig(save_name, transparent=False, bbox_inches=\"tight\", pad_inches=0.01, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_targeted_conf_index(analysis, stop_conf): # for targeted attacks\n",
    "    outputs = np.squeeze(analysis[\"adversarial_outputs\"])\n",
    "    inv_true_labels = 1 - np.squeeze(analysis['input_labels'])[None, ...] # add time step dimension\n",
    "    outputs *= inv_true_labels # zero out correct class\n",
    "    confs = np.max(outputs, axis=-1)\n",
    "    stop_indices = []\n",
    "    for i in range(1, confs.shape[0]):\n",
    "        gt_stop_conf = np.nonzero(confs[i,:] >= stop_conf)[0]\n",
    "        if len(gt_stop_conf) > 0:\n",
    "            stop_indices.append(gt_stop_conf)\n",
    "        else:\n",
    "            stop_indices.append(-1)\n",
    "    return stop_indices\n",
    "\n",
    "def get_rects(filename_list, metric, stop_conf):\n",
    "    data = []; means = []; stds = [];\n",
    "    for file in filename_list:\n",
    "        metrics = np.load(file, allow_pickle=True)[\"data\"].item()\n",
    "        stop_indices = find_targeted_conf_index(metrics, stop_conf)\n",
    "        for stop_step in stop_indices:\n",
    "            MSE = metrics[metric][0, stop_step, np.arange(metrics[metric].shape[-1])]\n",
    "            data.append(MSE)\n",
    "            means.append(np.mean(MSE))\n",
    "            stds.append(np.std(MSE)) \n",
    "    return data, means, stds\n",
    "\n",
    "def get_mses(file_list):\n",
    "    data = []; means = []; stds = [];\n",
    "    for file in file_list:\n",
    "        results = np.load(file, allow_pickle=True)['data'].item()\n",
    "        if 'input_adv_mses' in results.keys():\n",
    "            mse_results = np.squeeze(results['input_adv_mses'])\n",
    "            conf_results = np.squeeze(results['adversarial_outputs'])\n",
    "            num_steps, num_images = mse_results.shape\n",
    "            max_confs = np.max(conf_results, axis=-1)\n",
    "            stop_times = []\n",
    "            for image in range(num_images):\n",
    "                for time in range(num_steps):\n",
    "                    if max_confs[time, image] > 90:\n",
    "                        stop_times.append(time)\n",
    "                        break # stop checking times for this image\n",
    "            MSE = np.zeros(num_images)\n",
    "            for data_idx, time in enumerate(stop_times):\n",
    "                MSE[data_idx] = mse_results[time, data_idx]\n",
    "        elif 'conf_mean_squared_distances' in results.keys():\n",
    "            #import IPython; IPython.embed(); raise SystemExit\n",
    "            stop_time = np.squeeze(results['conf_adversarial_time_step'][0])[:100]\n",
    "            MSE = np.squeeze(results['conf_mean_squared_distances'])[stop_time, :100]\n",
    "        else:\n",
    "            print('bleh')\n",
    "            import IPython; IPython.embed(); raise SystemExit\n",
    "        data.append(MSE[:100])\n",
    "        means.append(np.mean(MSE[:100]))\n",
    "        stds.append(np.std(MSE[:100])) \n",
    "    return data, means, stds\n",
    "\n",
    "def get_mnist_mse_data(file_lists, metric, stop_conf):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ------\n",
    "    file_lists: list\n",
    "        list of list with organization\n",
    "        inner list: model-type filenames (i.e. lca or mlp)\n",
    "        outer lists: groups across models (i.e. layer depth)\n",
    "    metric: str\n",
    "        the analysis metric to average over\n",
    "    stop_conf: float\n",
    "        the desired classifier confidence for stopping the adversarial attack\n",
    "    \"\"\"\n",
    "    data = []; means = []; stds = []\n",
    "    for file_list in file_lists:\n",
    "        #data_step, means_step, stds_step = get_rects(file_list, metric, stop_conf) \n",
    "        data_step, means_step, stds_step = get_mses(file_list)\n",
    "        means.append(means_step)\n",
    "        data.append(data_step)\n",
    "        stds.append(stds_step)\n",
    "    return (data, means, stds)\n",
    "\n",
    "def multi_model_compare(ax, data, means, stds, colors, names, xtick_labels,\n",
    "                        xlabel, ylabel, ylim, width, title, fontsize):\n",
    "    # orgnaize the data\n",
    "    cmap_gray = cm.get_cmap(\"gray\")\n",
    "    N = len(data) # number of depths\n",
    "    M = len(data[0]) # number of models being compared\n",
    "    # create the bar chart\n",
    "    ind = np.arange(M)  # the x locations for the depths    \n",
    "    rects = []\n",
    "    for i in range(N):\n",
    "        # the bars\n",
    "        x = ind + i * (width+.01)\n",
    "        rect = ax.bar(x, means[i], color=colors[i], yerr=stds[i], width=width, alpha=1.0)\n",
    "        rects.append(rect)\n",
    "        # the data points\n",
    "        bar_data = np.array(data[i])\n",
    "        x_tiled = np.tile(x+(width/4), (bar_data.shape[-1],1)).T\n",
    "        #import IPython; IPython.embed(); raise SystemExit\n",
    "        ax.scatter(x_tiled, bar_data, color='black', alpha=1.0, s=1, zorder=2)\n",
    "    ax.set_xticks(ind + ((M-1)*(width+.01))/2)\n",
    "    ax.set_xticklabels(xtick_labels, fontsize=fontsize)\n",
    "    ax.set_ylabel(ylabel, fontsize=fontsize)\n",
    "    ax.set_xlabel(xlabel, fontsize=fontsize)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.yaxis.grid(which=\"major\", color=cmap_gray(.8), linestyle='--', linewidth=1)\n",
    "    ax.tick_params(\"both\", labelsize=fontsize)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.set_ylim([0, ylim])\n",
    "    ax.set_title(title, fontsize=fontsize)\n",
    "    ax.legend([r[0] for r in rects], names, fontsize=fontsize, loc='upper right', framealpha=1.0)\n",
    "    return data\n",
    "\n",
    "def adv_mse_comparison_plot(ax, file_lists, metric, stop_conf,\n",
    "                            colors, names, xtick_labels, xlabel, ylabel, ylim, width, title, fontsize):\n",
    "    \"\"\"\n",
    "    Bar chart that compares a designated metric for each model in file_lists\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    file_lists: list\n",
    "        list of list with organization\n",
    "        inner list: model-type filenames (i.e. lca or mlp)\n",
    "        outer lists: groups across models (i.e. layer depth)\n",
    "    metric: str\n",
    "        the analysis metric to average over\n",
    "    stop_conf: float\n",
    "        the desired classifier confidence for stopping the adversarial attack\n",
    "    colors: list\n",
    "        list of list of matplotlib color codes for each model\n",
    "        has same nested order as file_lists\n",
    "    xtick_labels: list\n",
    "        list of the group labels (i.e. layer depths)\n",
    "    names: list\n",
    "        names of the models (i.e. lca or mlp)\n",
    "    xlabel: str\n",
    "        x axis label\n",
    "    \"\"\"\n",
    "    data, means, stds = get_mnist_mse_data(file_lists, metric, stop_conf)\n",
    "    return multi_model_compare(ax, data, means, stds, colors, names, xtick_labels, xlabel, ylabel, ylim, width, title, fontsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_file = mnist_files[0][0][0]#mnist_mlp_768_2layer#\"mlp_cosyne_mnist\"\n",
    "lca_file = mnist_files[1][0][0]#mnist_lca_768_2layer#\"slp_lca_768_latent_75_steps_mnist\"\n",
    "lista = 'slp_lista_768_5_layers_mnist'\n",
    "lista_k_file_path = '/analysis/0.0/savefiles/class_adversary_analysis_test_kurakin_targeted.npz'\n",
    "\n",
    "lista_file = projects_dir + lista + lista_k_file_path\n",
    "#lca_img_file, lista_img_file, mlp_img_file = (path + model_name + k_img_path for model_name in [lca, lista, mlp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = [[color_vals['md_blue'], color_vals['md_green'], color_vals['md_red']]]\n",
    "xtick_labels = ['MLP', 'w/ LISTA', 'w/ LCA']\n",
    "file_lists = [[mlp_file, lista_file, lca_file]] #, lca_1568_files]\n",
    "\n",
    "metric = \"input_adv_mses\"\n",
    "names = [None]\n",
    "\n",
    "stop_conf=.90\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data_points = adv_mse_comparison_plot(ax, file_lists, metric, stop_conf, colors, names,\n",
    "                                     xtick_labels, \"\", \"Mean Squared Distance\", .08, width=.5,\n",
    "                                     title=\"Adversarial MNIST at 90% Confidence\", fontsize=fontsize)\n",
    "ax.get_legend().set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "#out_list = [path + model_name + \"/analysis/0.0/vis/mlp_lista_lca_adv_comparison\"\n",
    "#  for model_name in [mnist_mlp_768_2layer, lista, mnist_lca_768_2layer]]\n",
    "#for out_name in out_list:\n",
    "#  for ext in [\".png\", \".eps\"]:\n",
    "#    save_name = out_name+ext\n",
    "#    fig.savefig(save_name, transparent=False, bbox_inches=\"tight\", pad_inches=0.01, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lca_data = get_mnist_mse_data([lca_768_files], metric, stop_conf)[0][0]\n",
    "n_bins = 25\n",
    "\n",
    "plt.figure()\n",
    "hist1, bins = np.histogram(lca_data[0], n_bins)\n",
    "plt.bar(bins[:n_bins],hist1/len(lca_data[0]), width = .003, alpha=.7, label=\"lca_2layer\")\n",
    "\n",
    "hist2, bins = np.histogram(lca_data[1], n_bins)\n",
    "plt.bar(bins[:n_bins],hist2/len(lca_data[1]), width = .003, alpha=.7, label=\"lca_3layer\")\n",
    "\n",
    "plt.xlabel(\"Adversarial MSE\")\n",
    "plt.ylabel(\"Frequency of Images\")\n",
    "plt.legend(framealpha=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average target-class confidence per kurakin attack step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average adversarial MSE per carlini attack step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_mse_step(analysis_files, recons, confs, colors, title, model_names, bar_width, hatches, figsize, dpi):\n",
    "    fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "    num_conditions = len(analysis_files)\n",
    "    gs_top = gridspec.GridSpec(num_conditions, num_conditions)\n",
    "    axes = []\n",
    "    for condition, (condition_analysis_files, recon, conf) in enumerate(zip(analysis_files, recons, confs)):\n",
    "        #gs0 = gridspec.GridSpec(1, 2, wspace=0.2, width_ratios = [2, 1])#, hspace=0.3)\n",
    "        gs0 = gridspec.GridSpecFromSubplotSpec(1, 2, gs_top[condition, :],\n",
    "            wspace=0.2, width_ratios = [2, 1])#, hspace=0.3)\n",
    "        left_gs = gridspec.GridSpecFromSubplotSpec(1, 2, gs0[0], wspace=1.3)\n",
    "        right_gs = gridspec.GridSpecFromSubplotSpec(1, 1, gs0[1], wspace=0.9)\n",
    "        group_data = []\n",
    "        group_means = []\n",
    "        handles = []\n",
    "        for x_ax_idx, key in enumerate(['input_adv_mses', 'adversarial_outputs']):\n",
    "            axes.append(fig.add_subplot(left_gs[x_ax_idx]))\n",
    "            for file_idx, (file, name) in enumerate(zip(condition_analysis_files, model_names)):\n",
    "                analysis = np.load(file, allow_pickle=True)[\"data\"].item()\n",
    "                adv_conf = 100*np.max(np.squeeze(analysis['adversarial_outputs']), axis=-1)\n",
    "                if x_ax_idx == 0:\n",
    "                    if condition == 0:\n",
    "                        axes[-1].set_ylabel('Adversarial\\nConfidence')\n",
    "                    axes[-1].axhline(90.0, color='black', linestyle='dashed', linewidth=1) \n",
    "                    axes[-1].set_ylim([0, 100.1])\n",
    "                    mean_vals = np.mean(adv_conf, axis=-1)[1:]\n",
    "                    std_vals = np.std(adv_conf, axis=-1)[1:]\n",
    "                else:\n",
    "                    if condition == 0:\n",
    "                        axes[-1].set_ylabel('Adversarial Mean\\nSquared Distance')\n",
    "                    adv_mse = np.squeeze(analysis['input_adv_mses'])\n",
    "                    axes[-1].yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f'))\n",
    "                    thresh_indices = np.argwhere(np.mean(adv_conf, axis=-1)>90)\n",
    "                    first_adv_cross = np.min(thresh_indices[thresh_indices>2]) # first couple are original label\n",
    "                    axes[-1].axvline(first_adv_cross, color=colors[file_idx][0], linestyle='dashed', linewidth=1)\n",
    "                    mean_vals = np.mean(adv_mse, axis=-1)[1:]\n",
    "                    std_vals = np.std(adv_mse, axis=-1)[1:]\n",
    "                    group_data.append(adv_mse[first_adv_cross, :])\n",
    "                    group_means.append(mean_vals[first_adv_cross])\n",
    "                    max_val = 0.03#np.max(mean_vals)+std_vals[np.argmax(mean_vals)]\n",
    "                    axes[-1].set_ylim([0, max_val])\n",
    "                axes[-1].plot(range(len(mean_vals)), mean_vals, label=name,\n",
    "                    lw=2, color=colors[file_idx][0], zorder=1)\n",
    "                axes[-1].fill_between(range(len(mean_vals)), mean_vals + std_vals , mean_vals - std_vals,\n",
    "                    edgecolor=colors[file_idx][1], alpha=1.0, zorder=0, facecolor=\"none\",\n",
    "                    hatch=hatches[file_idx], rasterized=False)\n",
    "                if condition == num_conditions-1:\n",
    "                    axes[-1].set_xlabel('Attack Step')\n",
    "                axes[-1].grid(False)\n",
    "        axes.append(fig.add_subplot(right_gs[0]))\n",
    "        x_pos = np.arange(2) + 2 * bar_width\n",
    "        linewidth = 1\n",
    "        medianprops = dict(linestyle='--', linewidth=linewidth, color='k')\n",
    "        meanprops = dict(linestyle='-', linewidth=linewidth, color='k')\n",
    "        float_colors = [[52/255, 152/255, 219/255], [231/255, 76/255, 60/255]] # blue, red\n",
    "        axes[-1].set_title(f'c={recon}, '+r'$\\kappa$'+f'={conf}')\n",
    "        for data, means, pos, color, name in zip(group_data, group_means, x_pos, float_colors, model_names):\n",
    "            boxprops = dict(linestyle='-', linewidth=linewidth, color=color)\n",
    "            whiskerprops = boxprops\n",
    "            capprops = boxprops\n",
    "            handles.append(axes[-1].boxplot(data, sym='', positions=[pos],\n",
    "                whis=(5, 95), widths=bar_width, meanline=True, showmeans=True, boxprops=boxprops,\n",
    "                whiskerprops=whiskerprops, capprops=capprops, medianprops=medianprops,\n",
    "                meanprops=meanprops\n",
    "            ))\n",
    "            axes[-1].set_ylim([0, max_val])\n",
    "            axes[-1].set_yticklabels('')\n",
    "            axes[-1].get_xaxis().set_ticks([])\n",
    "            axes[-1].grid(False)\n",
    "            axes[-1].text(pos, 0.0025, name, horizontalalignment='center', verticalalignment='center')\n",
    "    fig.subplots_adjust(top=0.8)\n",
    "    fig.suptitle(title, y=0.98)\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = [[color_vals['md_blue'], color_vals['lt_blue']], [color_vals['md_red'], color_vals['lt_red']]]\n",
    "model_names = ['w/o LCA', 'w/ LCA']\n",
    "hatches = ['///', '\\\\\\\\\\\\']\n",
    "\n",
    "#carlini_title = 'Networks with an LCA layer require larger\\nperturbations for equal confidence with the Carlini attack'\n",
    "#carlini_title = 'Networks with an LCA layer are more robust than without'\n",
    "carlini_title = ''\n",
    "\n",
    "all_recons = []\n",
    "all_confs = []\n",
    "all_files = []\n",
    "for recon in ['0.5', '1.0']:\n",
    "    for conf in ['0.0', '10.0']:\n",
    "        if conf == '10.0':\n",
    "            extra_str = '_'\n",
    "            temp = '1.00'\n",
    "        else:\n",
    "            extra_str = ''\n",
    "            temp = '1.0'\n",
    "        c_file_path = (f'{analysis_dir}savefiles/class_adversary_analysis_test'+\n",
    "            f'{extra_str}temp{temp}_conf{conf}_recon{recon}_carlini_targeted.npz')\n",
    "        c_mlp_files = [projects_dir + model_name + c_file_path for model_name in [mnist_mlp_768_2layer]]\n",
    "        temp = '0.65'\n",
    "        c_file_path = (f'{analysis_dir}savefiles/class_adversary_analysis_test'+\n",
    "            f'{extra_str}temp{temp}_conf{conf}_recon{recon}_carlini_targeted.npz')\n",
    "        c_lca_files = [projects_dir + model_name + c_file_path for model_name in [mnist_lca_768_2layer]]\n",
    "        c_files = c_mlp_files + c_lca_files\n",
    "        all_recons.append(recon)\n",
    "        all_confs.append(conf)\n",
    "        all_files.append(c_files)\n",
    "\n",
    "figsize = nc.set_size(text_width, fraction=1.0, subplot=[2*2, 3])\n",
    "fig, ax = plot_average_mse_step(all_files, all_recons, all_confs, colors, carlini_title,\n",
    "    model_names, bar_width, hatches, figsize, dpi)\n",
    "\n",
    "out_list = [projects_dir + model_name + '/analysis/0.0/vis/carlini_mse_vs_iteration_k0.0-10.0_conditions'\n",
    "    for model_name in [mnist_lca_768_2layer, mnist_mlp_768_2layer]]\n",
    "for out_name in out_list:\n",
    "    for ext in file_extensions:\n",
    "        save_name = out_name+ext\n",
    "        fig.savefig(save_name, transparent=False, bbox_inches=\"tight\", pad_inches=0.01, dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Differentiable Loss Surface Adversarial Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class params(object):\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.version = '0.0'\n",
    "        self.projects_dir = os.path.expanduser(\"~\")+\"/Work/Projects/\"     \n",
    "        self.save_info = 'test'\n",
    "        self.overwrite_analysis_log = False\n",
    "        self.do_neuron_visualization=False\n",
    "        \n",
    "def get_label_est(model_name, input_images):\n",
    "    # Get params, set dirs\n",
    "    analysis_params = params(model_name) # construct object\n",
    "\n",
    "    # Load arguments\n",
    "    model_name_list = os.listdir(analysis_params.projects_dir)\n",
    "    analysis_params.model_dir = analysis_params.projects_dir+analysis_params.model_name\n",
    "\n",
    "    model_log_file = (analysis_params.model_dir+\"/logfiles/\"+analysis_params.model_name\n",
    "      +\"_v\"+analysis_params.version+\".log\")\n",
    "    model_logger = Logger(model_log_file, overwrite=False)\n",
    "    model_log_text = model_logger.load_file()\n",
    "    model_params = model_logger.read_params(model_log_text)[-1]\n",
    "    analysis_params.model_type = model_params.model_type\n",
    "\n",
    "    # Initialize & setup analyzer\n",
    "    analyzer = ap.get_analyzer(analysis_params.model_type)\n",
    "    analyzer.setup(analysis_params)\n",
    "    analysis_params.data_type = analyzer.model_params.data_type\n",
    "    analyzer.setup_model(analyzer.model_params)\n",
    "    \n",
    "    # run forward pass\n",
    "    with tf.compat.v1.Session(graph=analyzer.model.graph) as sess:\n",
    "        feed_dict = analyzer.model.get_feed_dict(input_images, is_test=True)\n",
    "        sess.run(analyzer.model.init_op, feed_dict)\n",
    "        analyzer.model.load_full_model(sess, analyzer.analysis_params.cp_loc)\n",
    "        label_est = sess.run(analyzer.model.label_est, feed_dict)\n",
    "        \n",
    "    return label_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare adv. MSE for more differentiable loss surface\n",
    " LCA vs LISTA\n",
    " \n",
    "### See if LISTA adv. examples translate to LCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_data(metric_files, img_files, stop_conf):\n",
    "  imgs = [np.load(file, allow_pickle=True)[\"data\"].item() for file in img_files]\n",
    "  metrics = [np.load(file, allow_pickle=True)[\"data\"].item() for file in metric_files]\n",
    "  indices = [np.array(find_targeted_conf_index(r, stop_conf)) for r in metrics]\n",
    "  input_images = [r[\"input_images\"].reshape(-1, 28, 28) for r in metrics]\n",
    "  input_clf = [np.argmax(r[\"input_labels\"], axis=1) for r in metrics]\n",
    "  adv_images = [r[\"adversarial_images\"][0, i, np.arange(len(i)), :].reshape(-1, 28, 28) for i, r in zip(indices, imgs)]\n",
    "  adv_clf = [np.argmax(r[\"adversarial_outputs\"][0, i, np.arange(len(i))], axis=1)\n",
    "    for i, r in zip(indices,metrics)]\n",
    "  return input_images, input_clf, adv_images, adv_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_stop_conf = .95\n",
    "# get lista adv images\n",
    "#input_images, input_clf, adv_images, adv_clf = get_results([lista_file], [lista_img_file], lista_stop_conf)\n",
    "input_images, input_clf, adv_images, adv_clf = get_mnist_data([lista_file], [lista_img_file], lista_stop_conf)\n",
    "# pass through the models\n",
    "lca_softmax_labels = get_label_est(lca, adv_images[0])\n",
    "lista_softmax_labels = get_label_est(lista, adv_images[0])\n",
    "mlp_softmax_labels = get_label_est(mlp, adv_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize the data\n",
    "lca_confs_lista_adv = [out[cls] for out, cls in zip(lca_softmax_labels, adv_clf[0])]\n",
    "lca_confs_input = [out[cls] for out, cls in zip(lca_softmax_labels, input_clf[0])]\n",
    "\n",
    "mlp_confs_lista_adv = [out[cls] for out, cls in zip(mlp_softmax_labels, adv_clf[0])]\n",
    "mlp_confs_input = [out[cls] for out, cls in zip(mlp_softmax_labels, input_clf[0])]\n",
    "\n",
    "lista_confs_lista_adv = [out[cls] for out, cls in zip(lista_softmax_labels, adv_clf[0])]\n",
    "lista_confs_input = [out[cls] for out, cls in zip(lista_softmax_labels, input_clf[0])]\n",
    "\n",
    "filter_indices = np.where(np.array(lista_confs_lista_adv) > .8)[0]\n",
    "\n",
    "data = [[lista_confs_input, lista_confs_lista_adv],\n",
    "        [mlp_confs_input, mlp_confs_lista_adv],\n",
    "        [lca_confs_input, lca_confs_lista_adv]]\n",
    "\n",
    "data = [[np.array(confs)[filter_indices] for confs in model_confs] for model_confs in data]\n",
    "means = [[np.mean(confs) for confs in model_confs] for model_confs in data]\n",
    "stds = [np.array([[0,0],[np.std(confs) for confs in model_confs]]) for model_confs in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [color_vals['md_green'], color_vals['md_blue'], color_vals['md_red']]\n",
    "xtick_labels = [\"Original Label\", \"Adv Target Label\"]\n",
    "xlabel = None#\"Class Position\"\n",
    "ylabel = \"Softmax Confidence\"\n",
    "names = ['w/ LISTA', 'MLP', 'w/ LCA']\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "multi_model_compare(ax, data, means, stds, colors, names, xtick_labels, xlabel, ylabel, 1, width=.25,\n",
    "                   title=\"Transferability of LISTA\\nAdversarial Images\", fontsize=fontsize);\n",
    "legend = ax.get_legend()\n",
    "legend.set_bbox_to_anchor([1.28,0.94,0,0], transform=fig.transFigure)\n",
    "plt.show()\n",
    "\n",
    "out_list += [path + lista + \"/analysis/0.0/vis/lista_adv_transferability\"]\n",
    "for out_name in out_list:\n",
    "  for ext in file_extensions:\n",
    "    save_name = out_name+ext\n",
    "    fig.savefig(save_name, transparent=False, bbox_inches=\"tight\", pad_inches=0.01, dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Adv Image Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid_subplots_with_fontsize(fig, gs, mlp_grp, lca_grp, orig_labels, target_labels, group_names, group_name_loc,\n",
    "                       orig_y_adj, start_idx=0, num_categories=3, crop_ammount=0, hspace=0.5, wspace=-0.4,\n",
    "                       cmap=\"Greys_r\", fontsize=12):\n",
    "  ax_orig_list = []\n",
    "  gs_sub0_list = []\n",
    "  gs_sub1_list = []\n",
    "  for i in range(num_categories):\n",
    "    ax_orig_list.append(fig.add_subplot(gs[i, :2]))\n",
    "    gs_sub0_list.append(gridspec.GridSpecFromSubplotSpec(2, 2, gs[i, 2:4], hspace=hspace, wspace=wspace))\n",
    "    gs_sub1_list.append(gridspec.GridSpecFromSubplotSpec(2, 2, gs[i, 4:], hspace=hspace, wspace=wspace))\n",
    "  \n",
    "  for category_idx in range(num_categories):\n",
    "    image_idx = category_idx + start_idx\n",
    "    orig_ax = ax_orig_list[category_idx]\n",
    "    if category_idx == 0:\n",
    "      orig_ax.set_title(\"Unperturbed\", y=orig_y_adj)#, fontsize=fontsize)\n",
    "    orig_img = crop(np.squeeze(mlp_grp[0][0][image_idx, ...]), crop_ammount)\n",
    "    orig_im_handle = show_image_with_label(orig_ax, orig_img, orig_labels[0][0][image_idx], cmap=cmap)\n",
    "    for model_idx, gs_sub in enumerate([gs_sub0_list[category_idx], gs_sub1_list[category_idx]]):\n",
    "      mlp_adv_img = crop(np.squeeze(mlp_grp[1][model_idx][image_idx, ...]), crop_ammount)\n",
    "      mlp_diff_img = crop(np.squeeze(mlp_grp[2][model_idx][image_idx, ...]), crop_ammount)\n",
    "      lca_adv_img = crop(np.squeeze(lca_grp[1][model_idx][image_idx, ...]), crop_ammount)\n",
    "      lca_diff_img = crop(np.squeeze(lca_grp[2][model_idx][image_idx, ...]), crop_ammount)\n",
    "      diff_vmin = np.min(np.concatenate((mlp_diff_img, lca_diff_img)))\n",
    "      diff_vmax = np.max(np.concatenate((mlp_diff_img, lca_diff_img)))\n",
    "      img_list = [[mlp_adv_img, lca_adv_img], [mlp_diff_img, lca_diff_img]]\n",
    "      for i in range(2): # row [adv, pert]\n",
    "        for j in range(2): # col [w/o LCA, w/ LCA]\n",
    "          current_ax = fig.add_subplot(gs_sub[i, j])\n",
    "          current_target_label = None\n",
    "          current_image = img_list[i][j]\n",
    "          if i == 0:\n",
    "            vmin = 0.0\n",
    "            vmax = 1.0\n",
    "            if j == 0: # top left image\n",
    "              if model_idx == 0:\n",
    "                current_ax.set_ylabel(r\"$s^{*}_{T}$\")#, fontsize=fontsize)\n",
    "              current_target_label = target_labels[j][model_idx][image_idx]\n",
    "              if category_idx == 0: # top category only\n",
    "                x_loc = group_name_loc[0]\n",
    "                y_loc = group_name_loc[1]\n",
    "                text_handle = current_ax.text(x_loc, y_loc, group_names[j+model_idx],#, fontsize=fontsize,\n",
    "                  horizontalalignment='left', verticalalignment='bottom')\n",
    "          else: # i == 1\n",
    "            vmin = np.round(diff_vmin, 2)\n",
    "            vmax = np.round(diff_vmax, 2)\n",
    "            if j == 0 and model_idx == 0:\n",
    "                current_ax.set_ylabel(r\"$s-s^{*}_{T}$\")#, fontsize=fontsize)\n",
    "            if j == 0 and category_idx == num_categories-1: # bottom left\n",
    "              current_ax.set_xlabel(\"w/o\\nLCA\")#, fontsize=fontsize)\n",
    "            elif j == 1 and category_idx == num_categories-1: # bottom right\n",
    "              current_ax.set_xlabel(\"w/\\nLCA\")#, fontsize=fontsize)\n",
    "          im_handle = show_image_with_label(current_ax, current_image, current_target_label, vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "          if j == 1:\n",
    "            pf.add_colorbar_to_im(im_handle, aspect=10, ax=current_ax, ticks=[vmin, vmax])#, labelsize=fontsize/2)\n",
    "\n",
    "def plot_adv_images_with_figsize(image_groups, labels, mnist_start_idx, cifar_start_idx, figsize, dpi=100):\n",
    "  mnist_mlp_grp, mnist_lca_grp = image_groups[0]\n",
    "  cifar_mlp_grp, cifar_lca_grp = image_groups[1]\n",
    "  mnist_orig_labels, mnist_target_labels, mnist_img_labels = labels[0]\n",
    "  cifar_orig_labels, cifar_target_labels, cifar_img_labels = labels[1]\n",
    "    \n",
    "  hspace = 0.3\n",
    "  wspace = 1.8\n",
    "  sub_hspace = 0.2\n",
    "  sub_wspace = 0.2\n",
    "  orig_y_adj = 1.10\n",
    "  img_label_loc = [-8.0, -8.0] # [x, y]\n",
    "  fig = plt.figure(figsize=[figsize[0]/2, figsize[1]], dpi=dpi)\n",
    "  gs0 = plt.GridSpec(2, 1, figure=fig, hspace=0.3)\n",
    "  \n",
    "  num_categories=3\n",
    "  \n",
    "  gs_mnist = gridspec.GridSpecFromSubplotSpec(num_categories, 6, gs0[0], hspace=hspace, wspace=wspace)\n",
    "  make_grid_subplots_with_fontsize(fig, gs_mnist, mnist_mlp_grp, mnist_lca_grp, mnist_orig_labels,\n",
    "    mnist_target_labels, mnist_img_labels, img_label_loc, orig_y_adj, mnist_start_idx, num_categories,\n",
    "    hspace=sub_hspace, wspace=sub_wspace, cmap=\"Greys\")\n",
    "  \n",
    "  gs_cifar = gridspec.GridSpecFromSubplotSpec(num_categories, 6, gs0[1], hspace=hspace, wspace=wspace)\n",
    "  make_grid_subplots_with_fontsize(fig, gs_cifar, cifar_mlp_grp, cifar_lca_grp, cifar_orig_labels,\n",
    "    cifar_target_labels, cifar_img_labels, img_label_loc, orig_y_adj, 0, num_categories,\n",
    "    hspace=sub_hspace, wspace=sub_wspace, cmap=\"Greys_r\")\n",
    "  \n",
    "  plt.show()\n",
    "  return fig          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figsize = nc.set_size(text_width, fraction=1.0, subplot=[16, 16])\n",
    "full_adv_img_fig = plot_adv_images_with_figsize(image_groups, label_groups, mnist_start_idx=44, cifar_start_idx=0,\n",
    "  figsize=figsize, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
