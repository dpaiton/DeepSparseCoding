{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate weight matrix + cov_a & send to Bruno as a mat file\n",
    "\n",
    "* Plot eigenvectors of cov_a in addition to pooling weights\n",
    "\n",
    "* Why are there no negative eigenvectors?\n",
    "\n",
    "* Create summary plot for cov_a\n",
    "\n",
    "* rerun everything with ICA vs LCA\n",
    "\n",
    "* Aapo does not subsample\n",
    "  * If you reduce to K dimensions, then you technically should only need K pooling filters\n",
    "  * How to space them evenly?\n",
    "\n",
    "* Rewrite pooling as a 3-term energy function, write LCA to get feedback from pooling layer\n",
    "  * could also get feedback from eigenvectors\n",
    " \n",
    "* Once you have a 2-layer network, you should be able to compress or denoise an image\n",
    "\n",
    "* Write up that discussess all pooling methods in one framework? I've started this...\n",
    "\n",
    "* For get_dictionary_stats: Should use center of mass of envelope, instead of max pixel value.\n",
    "  * Upsample fft / ifft for finding a better center\n",
    "  * This prevents issues with aliasing. You're limited to 16x16px resolution, but the true center of the BF could be between pixels.\n",
    "  * Convolving with Laplacian will make it slightly more peaky, but will not solve the aliasing problem.\n",
    "  * Possible methods could be fitting Gaussian, computing convex hull, computing blob boundaries.\n",
    "  * Add zero padding option to hilbertize\n",
    "    * Can zero-pad in space domain to get interpolated fft\n",
    "    * can zero-pad (e.g. 16x16 -> 64x64) in fft domain, then ifft, to get interpolated space image\n",
    "    * once you get centers, multiply by e.g. 64/16\n",
    "  * Finding centers\n",
    "    * center of mass of (upsampled?) thresholded image\n",
    "    * fit a gaussian\n",
    "  \n",
    "* For a given row in covariance matrix, plot that BF, then plot the BF corresponding to the top N indices in the row. These should be visually similar to the row's BF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import cv2\n",
    "import os                                                                       \n",
    "import skimage.draw\n",
    "import numpy as np                                                              \n",
    "import tensorflow as tf                                                         \n",
    "                                                                                \n",
    "import data.data_picker as dp                                                   \n",
    "import utils.plot_functions as pf                                               \n",
    "import utils.image_processing as ip                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "params = {                                                                      \n",
    "  ## Model params                                                               \n",
    "  \"out_dir\": os.path.expanduser(\"~\")+\"/Work/Projects/strongPCA/outputs/\",       \n",
    "  \"chk_dir\": os.path.expanduser(\"~\")+\"/Work/Projects/strongPCA/checkpoints/\",\n",
    "  #\"data_dir\": \"/media/tbell/datasets/\", #for remote machine\n",
    "  \"data_dir\": os.path.expanduser(\"~\")+\"/Work/Datasets/\", #for local runs                       \n",
    "  \"load_chk\": False,                                                             \n",
    "  \"update_interval\": 100,                                                       \n",
    "  \"device\": \"/cpu:0\",                                                           \n",
    "  \"learning_rate\": 0.12,                                                        \n",
    "  \"num_neurons\": 500,                                                           \n",
    "  \"sparse_mult\": 0.2,                                                           \n",
    "  \"num_inference_steps\": 20,                                                    \n",
    "  \"eta\": 0.001/0.03, #dt/tau                                                    \n",
    "  \"eps\": 1e-12,                                                                 \n",
    "  ## Data params                                                                \n",
    "  \"data_type\": \"vanhateren\",                                                    \n",
    "  \"rand_state\": np.random.RandomState(12345),                                   \n",
    "  \"num_images\": 500,                                                          \n",
    "  \"num_batches\": int(1e6),                                                         \n",
    "  \"batch_size\": 100,                                                            \n",
    "  \"patch_edge_size\": 16,                                                        \n",
    "  \"overlapping_patches\": True,                                                  \n",
    "  \"patch_variance_threshold\": 1e-6,                                             \n",
    "  \"conv\": False,                                                                \n",
    "  \"whiten_images\": True}                                                        \n",
    "                                                                                \n",
    "## Calculated params                                                            \n",
    "params[\"epoch_size\"] = params[\"batch_size\"] * params[\"num_batches\"]             \n",
    "params[\"num_pixels\"] = int(params[\"patch_edge_size\"]**2)                        \n",
    "params[\"dataset_shape\"] = [int(val)                                             \n",
    "    for val in [params[\"epoch_size\"], params[\"num_pixels\"]]],                   \n",
    "params[\"phi_shape\"] = [params[\"num_pixels\"], params[\"num_neurons\"]]             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = dp.get_data(params[\"data_type\"], params)                                 \n",
    "params[\"input_shape\"] = [                                                       \n",
    "  data[\"train\"].num_rows*data[\"train\"].num_cols*data[\"train\"].num_channels]     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Define graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()                                                              \n",
    "with tf.device(params[\"device\"]):                                               \n",
    "  with graph.as_default():                                                      \n",
    "    with tf.name_scope(\"placeholders\") as scope:                                \n",
    "      x = tf.placeholder(tf.float32, shape=[None,               \n",
    "        params[\"num_pixels\"]], name=\"input_data\")                               \n",
    "      sparse_mult = tf.placeholder(tf.float32, shape=(), name=\"sparse_mult\")    \n",
    "\n",
    "    with tf.name_scope(\"constants\") as scope:                                   \n",
    "      u_zeros = tf.zeros(shape=tf.stack([tf.shape(x)[0],                        \n",
    "        params[\"num_neurons\"]]), dtype=tf.float32, name=\"u_zeros\")              \n",
    "\n",
    "    with tf.name_scope(\"step_counter\") as scope:                                \n",
    "      global_step = tf.Variable(0, trainable=False, name=\"global_step\")         \n",
    "\n",
    "    with tf.variable_scope(\"weights\") as scope:                                 \n",
    "      phi_init = tf.truncated_normal(params[\"phi_shape\"], mean=0.0, stddev=0.2, \n",
    "        dtype=tf.float32, name=\"phi_init\")                                  \n",
    "      phi = tf.get_variable(name=\"phi\", dtype=tf.float32,                       \n",
    "        initializer=phi_init, trainable=True)                                   \n",
    "\n",
    "    with tf.name_scope(\"norm_weights\") as scope:                                \n",
    "      norm_phi = phi.assign(tf.nn.l2_normalize(phi,                             \n",
    "        dim=0, epsilon=params[\"eps\"], name=\"row_l2_norm\"))                      \n",
    "      norm_weights = tf.group(norm_phi,                                         \n",
    "        name=\"l2_normalization\")                                                \n",
    "\n",
    "    with tf.name_scope(\"inference\") as scope:                                   \n",
    "      u = tf.Variable(u_zeros, trainable=False,                                 \n",
    "        validate_shape=False, name=\"u\")                                         \n",
    "      # soft thresholded, rectified                                             \n",
    "      a = tf.where(tf.greater(u, sparse_mult),                                  \n",
    "        tf.subtract(u, sparse_mult), u_zeros,                                   \n",
    "        name=\"activity\")                                                        \n",
    "\n",
    "    with tf.name_scope(\"output\") as scope:                                      \n",
    "      with tf.name_scope(\"image_estimate\"):                                     \n",
    "        x_ = tf.matmul(a, tf.transpose(phi),                                    \n",
    "          name=\"reconstruction\")                                                \n",
    "\n",
    "    with tf.name_scope(\"loss\") as scope:                                        \n",
    "      with tf.name_scope(\"unsupervised\"):                                       \n",
    "        recon_loss = tf.reduce_mean(0.5 *                                       \n",
    "          tf.reduce_sum(tf.pow(tf.subtract(x, x_), 2.0),                        \n",
    "          axis=[1]), name=\"recon_loss\")                                         \n",
    "        sparse_loss = sparse_mult * tf.reduce_mean(                             \n",
    "          tf.reduce_sum(tf.abs(a), axis=[1]),                                   \n",
    "          name=\"sparse_loss\")                                                   \n",
    "        unsupervised_loss = (recon_loss + sparse_loss)                          \n",
    "      total_loss = unsupervised_loss                                            \n",
    "\n",
    "    with tf.name_scope(\"update_u\") as scope:                                    \n",
    "      lca_b = tf.matmul(x, phi, name=\"driving_input\")                           \n",
    "      lca_g = (tf.matmul(tf.transpose(phi), phi,                                \n",
    "        name=\"gram_matrix\") -                                                   \n",
    "        tf.constant(np.identity(params[\"phi_shape\"][1], dtype=np.float32),      \n",
    "        name=\"identity_matrix\"))                                                \n",
    "      lca_explain_away = tf.matmul(a, lca_g,                                    \n",
    "        name=\"explaining_away\")                                                 \n",
    "      du = lca_b - lca_explain_away - u                                         \n",
    "      step_inference = tf.group(u.assign_add(params[\"eta\"] * du),               \n",
    "        name=\"step_inference\")                                                  \n",
    "      reset_activity = tf.group(u.assign(u_zeros),                              \n",
    "        name=\"reset_activity\")                                                  \n",
    "\n",
    "    with tf.name_scope(\"performance_metrics\") as scope:                         \n",
    "      with tf.name_scope(\"reconstruction_quality\"):                             \n",
    "        MSE = tf.reduce_mean(tf.pow(tf.subtract(x, x_), 2.0),                   \n",
    "          axis=[1, 0], name=\"mean_squared_error\")                               \n",
    "\n",
    "    with tf.name_scope(\"optimizers\") as scope:                                  \n",
    "      learning_rates = tf.train.exponential_decay(                              \n",
    "        learning_rate=params[\"learning_rate\"],                                  \n",
    "        global_step=global_step,                                                \n",
    "        decay_steps=int(np.floor(params[\"num_batches\"]*0.8)),                   \n",
    "        decay_rate=0.5,                                                         \n",
    "        staircase=True,                                                         \n",
    "        name=\"phi_annealing_schedule\")                                          \n",
    "      optimizer = tf.train.GradientDescentOptimizer(learning_rates,             \n",
    "        name=\"phi_optimizer\")                                                   \n",
    "      update_weights = optimizer.minimize(total_loss, global_step=global_step, \n",
    "        var_list=[phi], name=\"phi_minimizer\")                                   \n",
    "\n",
    "    full_saver = tf.train.Saver(var_list=[phi], max_to_keep=2)                  \n",
    "\n",
    "    with tf.name_scope(\"summaries\") as scope:                                   \n",
    "      #tf.summary.image(\"input\", tf.reshape(x, [params[\"batch_size\"],           \n",
    "      #  params[\"patch_edge_size\"], params[\"patch_edge_size\"], 1]))             \n",
    "      #tf.summary.image(\"weights\", tf.reshape(tf.transpose(phi),                \n",
    "      #  [params[\"num_neurons\"], params[\"patch_edge_size\"],                     \n",
    "      #  params[\"patch_edge_size\"], 1]))                                        \n",
    "      tf.summary.histogram(\"u\", u)                                              \n",
    "      tf.summary.histogram(\"a\", a)                                              \n",
    "      tf.summary.histogram(\"phi\", phi)                                          \n",
    "\n",
    "    merged_summaries = tf.summary.merge_all()                                   \n",
    "    train_writer = tf.summary.FileWriter(params[\"out_dir\"], graph)              \n",
    "\n",
    "    with tf.name_scope(\"initialization\") as scope:                              \n",
    "      init_op = tf.group(tf.global_variables_initializer(),                     \n",
    "        tf.local_variables_initializer())                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train LCA model (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not params[\"load_chk\"]:\n",
    "  if not os.path.exists(params[\"out_dir\"]):\n",
    "    os.makedirs(params[\"out_dir\"])\n",
    "  if not os.path.exists(params[\"chk_dir\"]):\n",
    "    os.makedirs(params[\"chk_dir\"])\n",
    "\n",
    "  with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init_op,\n",
    "    feed_dict={x:np.zeros([params[\"batch_size\"]]+params[\"input_shape\"],\n",
    "      dtype=np.float32)})\n",
    "    \n",
    "    batch_steps = []\n",
    "    losses = []\n",
    "    sparsities = []\n",
    "    recon_errors = []\n",
    "    for b_step in range(params[\"num_batches\"]):\n",
    "      data_batch = data[\"train\"].next_batch(params[\"batch_size\"])\n",
    "      input_data = data_batch[0]\n",
    "    \n",
    "      feed_dict = {x:input_data, sparse_mult:params[\"sparse_mult\"]}\n",
    "    \n",
    "      sess.run(norm_weights, feed_dict)\n",
    "    \n",
    "      for inference_step in range(params[\"num_inference_steps\"]):               \n",
    "        sess.run(step_inference, feed_dict)\n",
    "    \n",
    "      sess.run(update_weights, feed_dict)\n",
    "    \n",
    "      current_step = sess.run(global_step)\n",
    "      if (current_step % params[\"update_interval\"] == 0):\n",
    "        summary = sess.run(merged_summaries, feed_dict)\n",
    "        train_writer.add_summary(summary, current_step)\n",
    "        full_saver.save(sess, save_path=params[\"chk_dir\"]+\"lca_chk\",\n",
    "          global_step=global_step)\n",
    "    \n",
    "        [current_loss, a_vals, recons, recon_err, weights] = sess.run(\n",
    "          [total_loss, a, x_, MSE, phi], feed_dict)\n",
    "        a_vals_max = np.array(a_vals.max()).tolist()\n",
    "        a_frac_act = np.array(np.count_nonzero(a_vals)\n",
    "          / float(params[\"batch_size\"]*params[\"num_neurons\"])).tolist()\n",
    "        batch_steps.append(current_step)\n",
    "        losses.append(current_loss)\n",
    "        sparsities.append(a_frac_act)\n",
    "        recon_errors.append(recon_err)\n",
    "    \n",
    "        print_dict = {\"current_step\":str(current_step).zfill(5),\n",
    "          \"loss\":str(current_loss),\n",
    "          \"a_max\":str(a_vals_max),\n",
    "          \"a_frac_act\":str(a_frac_act)}\n",
    "        print(print_dict)\n",
    "        pf.save_data_tiled(weights.T.reshape((params[\"num_neurons\"],\n",
    "          params[\"patch_edge_size\"], params[\"patch_edge_size\"])),\n",
    "          normalize=False, title=\"Dictionary at step \"+str(current_step),\n",
    "          save_filename=(params[\"out_dir\"]+\"phi_\"+str(current_step).zfill(5)\n",
    "          +\".png\"))\n",
    "        pf.save_data_tiled(recons.reshape((params[\"batch_size\"],\n",
    "          params[\"patch_edge_size\"], params[\"patch_edge_size\"])),\n",
    "          normalize=False, title=\"Recons at step \"+str(current_step),\n",
    "          save_filename=(params[\"out_dir\"]+\"recons_\"\n",
    "          +str(current_step).zfill(5)+\".png\"))\n",
    "      output_data = {\"batch_step\":batch_steps, \"total_loss\":losses,\n",
    "        \"frac_active\":sparsities, \"recon_MSE\":recon_errors}\n",
    "      pf.save_stats(output_data, save_filename=(params[\"out_dir\"]+\"loss.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load large image dataset for PCA analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Incremental covariance matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_incremental_cov(values, prev_cov=None):\n",
    "  if prev_cov is None:\n",
    "    return np.cov(values)\n",
    "  else:\n",
    "    return prev_cov + np.cov(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Pooling filters function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_pooling_filters(a_cov, num_pooling_dims):\n",
    "  evals, evecs = np.linalg.eig(a_cov)\n",
    "  sort_indices = np.argsort(evals)[::-1]\n",
    "  top_vecs = evecs[sort_indices[:num_pooling_dims]]\n",
    "  pooling_filters = np.dot(top_vecs.T, top_vecs)\n",
    "  return pooling_filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Compute activity covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_images = 1e6\n",
    "batch_size = int(1e2)\n",
    "a_cov = None\n",
    "with tf.Session(graph=graph) as sess:\n",
    "  sess.run(init_op,\n",
    "    feed_dict={x:np.zeros([batch_size]+params[\"input_shape\"],\n",
    "    dtype=np.float32)})\n",
    "  full_saver.restore(sess, tf.train.latest_checkpoint(params[\"chk_dir\"]))\n",
    "  tot_images = 0\n",
    "  while tot_images < num_images: \n",
    "    input_data = data[\"train\"].next_batch(batch_size)[0]\n",
    "    feed_dict = {x:input_data, sparse_mult:params[\"sparse_mult\"]}\n",
    "    for inference_step in range(params[\"num_inference_steps\"]):               \n",
    "      sess.run(step_inference, feed_dict)\n",
    "    [a_vals, weights] = sess.run([a, phi], feed_dict)\n",
    "    a_cov = compute_incremental_cov(a_vals.T, a_cov) \n",
    "    tot_images += batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Compute pooling filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_pooling_dims = 50 #K\n",
    "pooling_filters = compute_pooling_filters(a_cov, num_pooling_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Hilbertize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def hilbertize(dictionary, params):\n",
    "  cart2pol = lambda x,y: (np.arctan2(y,x), np.hypot(x, y))\n",
    "  \n",
    "  # Amount of zero padding for fft2 (closest power of 2)                          \n",
    "  N = np.int(2**(np.ceil(np.log2(params[\"patch_edge_size\"]))))                                  \n",
    "                                                                                  \n",
    "  # Analytic signal envelope for dictionary                                                \n",
    "  # (Hilbet transform of each basis function)                                     \n",
    "  env = np.zeros((params[\"num_neurons\"], params[\"num_pixels\"]), dtype=complex)                        \n",
    "                                                                                  \n",
    "  # Fourier transform of dictionary                                               \n",
    "  bffs = np.zeros((params[\"num_neurons\"], N, N), dtype=complex)                               \n",
    "  # Filtered Fourier transform of dictionary                                               \n",
    "  bff_filt = np.zeros((params[\"num_neurons\"], N**2), dtype=complex)                               \n",
    "\n",
    "  # Hilbert filters\n",
    "  hil_filt = np.zeros((params[\"num_neurons\"], params[\"patch_edge_size\"],\n",
    "    params[\"patch_edge_size\"]))\n",
    "\n",
    "  # Grid for creating filter\n",
    "  f = (2/N) * np.pi * np.arange(-N/2.0, N/2.0)                                    \n",
    "  (fx, fy) = np.meshgrid(f, f)                                                    \n",
    "  (theta, r) = cart2pol(fx, fy)                                                    \n",
    "                                                                                  \n",
    "  for neuron_idx in range(params[\"num_neurons\"]):                                           \n",
    "    # Grab single basis function, reshape to a square image                       \n",
    "    bf = dictionary[:, neuron_idx].reshape(params[\"patch_edge_size\"],\n",
    "      params[\"patch_edge_size\"])               \n",
    "                                                                                  \n",
    "    # Convert basis function into DC-centered Fourier domain                      \n",
    "    bff = np.fft.fftshift(np.fft.fft2(bf, [N, N]))\n",
    "    bffs[neuron_idx, ...] = bff\n",
    "                                                                                  \n",
    "    # Find indices of the peak amplitude                                          \n",
    "    max_ys = np.abs(bff).argmax(axis=0) # Returns row index for each col          \n",
    "    max_x = np.argmax(np.abs(bff).max(axis=0))                                    \n",
    "                                                                                  \n",
    "    # Convert peak amplitude location into angle in freq domain                   \n",
    "    fx_ang = f[max_x]                                                             \n",
    "    fy_ang = f[max_ys[max_x]]                                                     \n",
    "    theta_max = np.arctan2(fy_ang, fx_ang)                                        \n",
    "                                                                                  \n",
    "    # Define the half-plane with respect to the maximum                           \n",
    "    ang_diff = np.abs(theta-theta_max)                                            \n",
    "    idx = (ang_diff>np.pi).nonzero()                                              \n",
    "    ang_diff[idx] = 2.0 * np.pi - ang_diff[idx]                                   \n",
    "    hil_filt[neuron_idx, ...] = (ang_diff < np.pi/2.0).astype(int)                                        \n",
    "                                                                                  \n",
    "    # Create analytic signal from the inverse FT of the half-plane filtered bf\n",
    "    abf = np.fft.ifft2(np.fft.fftshift(hil_filt[neuron_idx, ...]*bff))                                  \n",
    "    env[neuron_idx, ...] = abf[0:params[\"patch_edge_size\"],\n",
    "      0:params[\"patch_edge_size\"]].reshape(params[\"num_pixels\"])    \n",
    "    bff_filt[neuron_idx, ...] = (hil_filt[neuron_idx, ...]*bff).reshape(N**2)                                     \n",
    "  return (env, bff_filt, hil_filt, bffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Get dictionary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_dictionary_stats(dictionary, params):\n",
    "  envelope, bff_filt, hil_filter, bffs = hilbertize(dictionary, params)\n",
    "  \n",
    "  basis_funcs = []\n",
    "  envelopes = []\n",
    "  filters = []\n",
    "  envelope_centers = []\n",
    "  fourier_centers = []\n",
    "  fourier_maps = []\n",
    "  orientations = []\n",
    "  lengths = []\n",
    "  line_images = []\n",
    "  blob_images = []\n",
    "  for bf_idx in range(params[\"num_neurons\"]):\n",
    "    # Reformatted individual basis function\n",
    "    basis_funcs.append(np.squeeze(ip.reshape_data(dictionary.T[bf_idx,...],\n",
    "      flatten=False)[0]))\n",
    "  \n",
    "    # Reformatted individual envelope filter\n",
    "    envelopes.append(np.squeeze(ip.reshape_data(np.abs(envelope[bf_idx,...]),\n",
    "      flatten=False)[0]))\n",
    "\n",
    "    # Basis function center\n",
    "    max_ys = envelopes[bf_idx].argmax(axis=0) # Returns row index for each col          \n",
    "    max_x = np.argmax(envelopes[bf_idx].max(axis=0))\n",
    "    y_cen = max_ys[max_x]\n",
    "    x_cen = max_x\n",
    "    envelope_centers.append((y_cen, x_cen)) \n",
    "\n",
    "    # Basis function orientation\n",
    "    filt = hil_filter[bf_idx, ...]\n",
    "    filters.append(filt)\n",
    "    y, x = np.nonzero(filt)\n",
    "    x = x - np.mean(x)\n",
    "    y = y - np.mean(y)\n",
    "    coords = np.vstack([y, x])\n",
    "    evals, evecs = np.linalg.eigh(np.cov(coords))\n",
    "    sort_indices = np.argsort(evals)[::-1]\n",
    "    filt_evec = evecs[:, sort_indices[0]]\n",
    "    orientations.append(filt_evec)\n",
    "\n",
    "    # Basis function length\n",
    "    env_evals, env_evecs = np.linalg.eigh(np.cov(envelopes[bf_idx])) \n",
    "    sorted_indices = np.argsort(env_evals)[::-1]\n",
    "    env_major_length = env_evals[sort_indices[0]]\n",
    "    env_minor_length = env_evals[sort_indices[1]]\n",
    "    lengths.append((env_major_length, env_minor_length))\n",
    "\n",
    "    # Rastered basis function line representation\n",
    "    out_image = np.zeros_like(basis_funcs[bf_idx]) # row (y), col (x)\n",
    "    y_start = np.int32(np.max([0, np.min([np.ceil(y_cen-filt_evec[0]*lengths[bf_idx][0]), 15])]))\n",
    "    x_start = np.int32(np.max([0, np.min([np.ceil(x_cen-filt_evec[1]*lengths[bf_idx][0]), 15])]))\n",
    "    y_end = np.int32(np.max([0, np.min([np.ceil(y_cen+filt_evec[0]*lengths[bf_idx][0]), 15])]))\n",
    "    x_end = np.int32(np.max([0, np.min([np.ceil(x_cen+filt_evec[1]*lengths[bf_idx][0]), 15])]))\n",
    "    y_lin, x_lin = skimage.draw.line(y_start, x_start, y_end, x_end)\n",
    "    out_image[y_lin, x_lin] += 1\n",
    "    line_images.append(out_image)\n",
    "\n",
    "    # Rastered basis function blob representations\n",
    "    #out_image = np.zeros_like(basis_funcs[bf_idx]) # row (y), col (x)\n",
    "    #rot = np.arctan2(orientations[bf_idx][0], orientations[bf_idx][1])\n",
    "    #y_elip, x_elip = skimage.draw.ellipse(y_cen, x_cen, env_major_length,\n",
    "    #  env_minor_length, rotation=rot)\n",
    "    #out_image[y_elip, x_elip] += 1\n",
    "    thr_env = envelopes[bf_idx].copy()\n",
    "    thr_env[np.where(thr_env<np.mean(thr_env)+2*np.std(thr_env))] = 0\n",
    "    #out_image = np.zeros_like(basis_funcs[bf_idx]) # row (y), col (x)\n",
    "    #env_y, env_x = np.nonzero(thr_env)\n",
    "    #points = np.hstack([env_y, env_x])\n",
    "    #import scipy.spatial\n",
    "    #hull = scipy.spatial.ConvexHull(points)\n",
    "    #out_image[points[hull.vertices,0], points[hull.vertices,1]] += 1\n",
    "    blob_images.append(thr_env)\n",
    "#\n",
    "    # Fourier function center\n",
    "    fourier_map = np.sqrt(np.real(bffs[bf_idx, ...])**2+np.imag(bffs[bf_idx, ...])**2)\n",
    "    fourier_maps.append(fourier_map)\n",
    "    max_fys = fourier_map.argmax(axis=0)\n",
    "    max_fx = np.argmax(fourier_map.max(axis=0))\n",
    "    fy_cen = max_ys[max_x]\n",
    "    fx_cen = max_x\n",
    "    fourier_centers.append((fy_cen, fx_cen))\n",
    "    \n",
    "  output = {\"basis_functions\":basis_funcs, \"envelopes\":envelopes,\n",
    "    \"filters\":filters, \"envelope_centers\":envelope_centers, \"lengths\":lengths,\n",
    "    \"fourier_centers\":fourier_centers, \"fourier_maps\":fourier_maps,\n",
    "    \"orientations\":orientations, \"line_images\":line_images,\n",
    "    \"blob_images\":blob_images}\n",
    "  return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Plot hilbert analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_hilbert_analysis(weights, params):\n",
    "  Envelope, bff_filt, Hil_filter, bff = hilbertize(weights, params)\n",
    "  N = np.int32(np.sqrt(bff_filt.shape[1]))\n",
    "                                                                                  \n",
    "  # Plot BFs, Hilbert filtered fourier amplitudes, raw fourier amplitudes\n",
    "  fig, sub_axes = plt.subplots(3, 1, figsize=(64,64))\n",
    "  \n",
    "  plot_data = pf.pad_data(weights.T.reshape((params[\"num_neurons\"],\n",
    "    params[\"patch_edge_size\"], params[\"patch_edge_size\"])))\n",
    "  bf_axis_image = sub_axes[0].imshow(plot_data, cmap=\"Greys_r\",\n",
    "    interpolation=\"nearest\")\n",
    "  sub_axes[0].tick_params(axis=\"both\", bottom=\"off\", top=\"off\", left=\"off\",\n",
    "    right=\"off\")\n",
    "  sub_axes[0].get_xaxis().set_visible(False)\n",
    "  sub_axes[0].get_yaxis().set_visible(False)\n",
    "  sub_axes[0].set_title(\"Basis Functions\", fontsize=32)\n",
    "  \n",
    "  plot_data = pf.pad_data(np.abs(Envelope).reshape((params[\"num_neurons\"],\n",
    "    params[\"patch_edge_size\"], params[\"patch_edge_size\"])))\n",
    "  hil_axis_image = sub_axes[1].imshow(plot_data, cmap=\"Greys_r\",\n",
    "    interpolation=\"nearest\")\n",
    "  sub_axes[1].tick_params(axis=\"both\", bottom=\"off\", top=\"off\", left=\"off\",\n",
    "    right=\"off\")\n",
    "  sub_axes[1].get_xaxis().set_visible(False)\n",
    "  sub_axes[1].get_yaxis().set_visible(False)\n",
    "  sub_axes[1].set_title(\"Analytic Signal Amplitude Envelope\", fontsize=32)\n",
    "  \n",
    "  resh_Zf = np.abs(bff_filt).reshape((params[\"num_neurons\"], N, N))                             \n",
    "  output_z = np.zeros(resh_Zf.shape)                                              \n",
    "  for i in range(params[\"num_neurons\"]):                                                    \n",
    "    output_z[i,...] = resh_Zf[i,...] / np.max(resh_Zf[i,...])                     \n",
    "  plot_data = pf.pad_data(output_z)\n",
    "  hil_axis_image = sub_axes[2].imshow(plot_data, cmap=\"Greys_r\",\n",
    "    interpolation=\"nearest\")\n",
    "  sub_axes[2].tick_params(axis=\"both\", bottom=\"off\", top=\"off\", left=\"off\",\n",
    "    right=\"off\")\n",
    "  sub_axes[2].get_xaxis().set_visible(False)\n",
    "  sub_axes[2].get_yaxis().set_visible(False)\n",
    "  sub_axes[2].set_title(\"Fourier Amplitude Spectrum\", fontsize=32)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Plot basis function statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_bf_stats(weights, params):\n",
    "    bf_stats = get_dictionary_stats(weights, params)\n",
    "\n",
    "    num_bf = 10\n",
    "    fig, sub_axes = plt.subplots(num_bf, 6, figsize=(36,36))\n",
    "    for bf_idx in range(num_bf):\n",
    "      bf = bf_stats[\"basis_functions\"][bf_idx]\n",
    "      env = bf_stats[\"envelopes\"][bf_idx]\n",
    "      filt = bf_stats[\"filters\"][bf_idx]\n",
    "      fourier = bf_stats[\"fourier_maps\"][bf_idx]\n",
    "      y, x = bf_stats[\"orientations\"][bf_idx]\n",
    "      y_cen, x_cen = bf_stats[\"envelope_centers\"][bf_idx]\n",
    "      length = bf_stats[\"lengths\"][bf_idx] \n",
    "      line_img = bf_stats[\"line_images\"][bf_idx]\n",
    "      blob_img = bf_stats[\"blob_images\"][bf_idx]\n",
    "      fy_cen, fx_cen = bf_stats[\"fourier_centers\"][bf_idx]\n",
    "\n",
    "      sub_axes[bf_idx, 0].imshow(bf, cmap=\"Greys_r\", interpolation=\"Nearest\")\n",
    "      sub_axes[bf_idx, 0].tick_params(axis=\"both\", bottom=\"off\", top=\"off\",\n",
    "        left=\"off\", right=\"off\")\n",
    "      sub_axes[bf_idx, 0].get_xaxis().set_visible(False)\n",
    "      sub_axes[bf_idx, 0].get_yaxis().set_visible(False)\n",
    "\n",
    "      sub_axes[bf_idx, 1].imshow(env, cmap=\"Greys_r\", interpolation=\"Nearest\")\n",
    "      sub_axes[bf_idx, 1].tick_params(axis=\"both\", bottom=\"off\", top=\"off\",\n",
    "        left=\"off\", right=\"off\")\n",
    "      sub_axes[bf_idx, 1].get_xaxis().set_visible(False)\n",
    "      sub_axes[bf_idx, 1].get_yaxis().set_visible(False)\n",
    "\n",
    "      sub_axes[bf_idx, 2].imshow(filt, cmap=\"Greys_r\", interpolation=\"Nearest\")\n",
    "      sub_axes[bf_idx, 2].tick_params(axis=\"both\", bottom=\"off\", top=\"off\",\n",
    "        left=\"off\", right=\"off\")\n",
    "      sub_axes[bf_idx, 2].get_xaxis().set_visible(False)\n",
    "      sub_axes[bf_idx, 2].get_yaxis().set_visible(False)\n",
    "\n",
    "      sub_axes[bf_idx, 3].imshow(fourier, cmap=\"Greys_r\", interpolation=\"Nearest\")\n",
    "      sub_axes[bf_idx, 3].tick_params(axis=\"both\", top=\"off\", right=\"off\",\n",
    "        bottom=\"off\", left=\"off\")\n",
    "      sub_axes[bf_idx, 3].spines[\"left\"].set_position(\"center\")\n",
    "      sub_axes[bf_idx, 3].spines[\"left\"].set_color(\"black\")\n",
    "      sub_axes[bf_idx, 3].spines[\"left\"].set_linewidth(2.5)\n",
    "      sub_axes[bf_idx, 3].spines[\"bottom\"].set_position(\"center\")\n",
    "      sub_axes[bf_idx, 3].spines[\"bottom\"].set_color(\"black\")\n",
    "      sub_axes[bf_idx, 3].spines[\"bottom\"].set_linewidth(2.5)\n",
    "      sub_axes[bf_idx, 3].spines[\"top\"].set_color(\"none\")\n",
    "      sub_axes[bf_idx, 3].spines[\"right\"].set_color(\"none\")\n",
    "      sub_axes[bf_idx, 3].set_yticklabels([])\n",
    "      sub_axes[bf_idx, 3].set_xticklabels([])\n",
    "      sub_axes[bf_idx, 3].set_ylim([0, fourier.shape[0]-1])\n",
    "      sub_axes[bf_idx, 3].set_xlim([0, fourier.shape[1]-1])\n",
    "\n",
    "      line_img[y_cen, x_cen] += 1\n",
    "      sub_axes[bf_idx, 4].imshow(line_img, interpolation=\"Nearest\")\n",
    "      sub_axes[bf_idx, 4].tick_params(axis=\"both\", bottom=\"off\", top=\"off\",\n",
    "        left=\"off\", right=\"off\")\n",
    "      sub_axes[bf_idx, 4].get_xaxis().set_visible(False)\n",
    "      sub_axes[bf_idx, 4].get_yaxis().set_visible(False)\n",
    "        \n",
    "      blob_img[np.where(blob_img>0)] = 1\n",
    "      blob_img[y_cen, x_cen] += 1\n",
    "      sub_axes[bf_idx, 5].imshow(blob_img, interpolation=\"Nearest\")\n",
    "      sub_axes[bf_idx, 5].tick_params(axis=\"both\", bottom=\"off\", top=\"off\",\n",
    "        left=\"off\", right=\"off\")\n",
    "      sub_axes[bf_idx, 5].get_xaxis().set_visible(False)\n",
    "      sub_axes[bf_idx, 5].get_yaxis().set_visible(False)\n",
    "\n",
    "    sub_axes[0,0].set_title(\"bf\", fontsize=32)\n",
    "    sub_axes[0,1].set_title(\"envelope\", fontsize=32)\n",
    "    sub_axes[0,2].set_title(\"filter\", fontsize=32)\n",
    "    sub_axes[0,3].set_title(\"fourier map\", fontsize=32)\n",
    "    sub_axes[0,4].set_title(\"envelope summary line\", fontsize=22)\n",
    "    sub_axes[0,5].set_title(\"envelope summary blob\", fontsize=22)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Plot pooling filter images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_pooling_filter_images(params, weights, pooling_filters,\n",
    "  num_connected_weights, num_pooling_filters):\n",
    "    \n",
    "  bf_stats = get_dictionary_stats(weights, params)\n",
    "  \n",
    "  out_images = np.zeros((num_pooling_filters, params[\"patch_edge_size\"],\n",
    "    params[\"patch_edge_size\"]))\n",
    "  for filter_idx in range(num_pooling_filters):\n",
    "    example_filter = pooling_filters[filter_idx,:]\n",
    "    top_indices = np.argsort(example_filter)[::-1]\n",
    "  \n",
    "    out_image = np.zeros((params[\"patch_edge_size\"], params[\"patch_edge_size\"]))\n",
    "    for bf_idx in range(num_connected_weights):\n",
    "      top_idx = top_indices[bf_idx]\n",
    "      bot_idx = top_indices[::-1][bf_idx]\n",
    "      top_line_indices = np.nonzero(bf_stats[\"images\"][top_idx])\n",
    "      bottom_line_indices = np.nonzero(bf_stats[\"images\"][bot_idx])\n",
    "      out_image[top_line_indices] = np.real(evals[top_idx])\n",
    "      out_image[bottom_line_indices] = np.real(evals[bot_idx]) \n",
    "    out_images[filter_idx, ...] = out_image\n",
    "  plot_data = pf.pad_data(out_images, pad_values=0)\n",
    "  \n",
    "  fig, sub_axes = plt.subplots(1, figsize=(10,10))\n",
    "  img_ax = sub_axes.imshow(plot_data, cmap=\"bwr\", vmin=-1, vmax=1,\n",
    "    interpolation=\"nearest\")\n",
    "  sub_axes.tick_params(axis=\"both\", bottom=\"off\", top=\"off\", left=\"off\",\n",
    "    right=\"off\")\n",
    "  sub_axes.get_xaxis().set_visible(False)\n",
    "  sub_axes.get_yaxis().set_visible(False)\n",
    "  cbar = fig.colorbar(img_ax, ticks=[-1, 0, 1])\n",
    "  cbar.ax.tick_params(labelsize=24)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Generalized plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_pooling_func(plot_func, params, weights, pooling_filters,\n",
    "  num_connected_weights, num_pooling_filters):\n",
    "\n",
    "  bf_stats = get_dictionary_stats(weights, params)\n",
    "\n",
    "  cmap = plt.get_cmap('bwr')\n",
    "  cNorm = matplotlib.colors.Normalize(vmin=-1, vmax=1)\n",
    "  scalarMap = matplotlib.cm.ScalarMappable(norm=cNorm, cmap=cmap)\n",
    "\n",
    "  num_plts_y = np.int32(np.ceil(np.sqrt(num_pooling_filters)))\n",
    "  num_plts_x = np.int32(np.floor(np.sqrt(num_pooling_filters)))+1 # +cbar row\n",
    "\n",
    "  print(pooling_filters.shape[0])\n",
    "  fig, sub_axes = plt.subplots(num_plts_y, num_plts_x, figsize=(24,24))\n",
    "  filter_idx_list = np.random.choice(np.arange(pooling_filters.shape[0],\n",
    "    dtype=np.int32), size=num_pooling_filters, replace=False)\n",
    "  filter_total = 0\n",
    "  for y_id in range(num_plts_y):\n",
    "    for x_id in range(num_plts_x):\n",
    "      if (filter_total < num_pooling_filters and x_id != num_plts_x-1):\n",
    "        filter_idx = filter_idx_list[filter_total]\n",
    "        example_filter = pooling_filters[filter_idx, :]\n",
    "        top_indices = np.argsort(example_filter)[::-1]\n",
    "        filter_norm = np.max(np.abs(example_filter))\n",
    "        for bf_idx in top_indices[:num_connected_weights]:\n",
    "          connection_strength = example_filter[bf_idx]/filter_norm\n",
    "          colorVal = scalarMap.to_rgba(connection_strength)\n",
    "          plot_func(sub_axes[y_id, x_id], bf_stats, bf_idx, colorVal)\n",
    "        sub_axes[y_id, x_id].set_xlim(0, 15)\n",
    "        sub_axes[y_id, x_id].set_ylim(0, 15)\n",
    "        sub_axes[y_id, x_id].set_aspect(\"equal\")\n",
    "        filter_total += 1\n",
    "      else:\n",
    "        sub_axes[y_id, x_id].spines[\"right\"].set_color(\"none\")\n",
    "        sub_axes[y_id, x_id].spines[\"top\"].set_color(\"none\")\n",
    "        sub_axes[y_id, x_id].spines[\"left\"].set_color(\"none\")\n",
    "        sub_axes[y_id, x_id].spines[\"bottom\"].set_color(\"none\")\n",
    "      sub_axes[y_id, x_id].invert_yaxis()\n",
    "      sub_axes[y_id, x_id].set_yticklabels([])\n",
    "      sub_axes[y_id, x_id].set_xticklabels([])\n",
    "      sub_axes[y_id, x_id].set_aspect(\"equal\")\n",
    "      sub_axes[y_id, x_id].tick_params(axis=\"both\", bottom=\"off\", top=\"off\",\n",
    "        left=\"off\", right=\"off\")\n",
    "  scalarMap._A = []\n",
    "  cbar = fig.colorbar(scalarMap, ax=list(sub_axes[:, -1]), ticks=[-1, 0, 1])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Render lines to represent BFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_lines(axis, bf_stats, bf_idx, colorVal):\n",
    "  y, x = bf_stats[\"orientations\"][bf_idx]\n",
    "  y_cen, x_cen = bf_stats[\"envelope_centers\"][bf_idx]\n",
    "  length = 3#bf_stats[\"lengths\"][bf_idx][0]\n",
    "  x_points = [(x_cen-x*length/2), (x_cen+x*length/2)]\n",
    "  y_points = [(y_cen-y*length/2), (y_cen+y*length/2)]  \n",
    "  axis.plot(x_points, y_points, color=colorVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Render oriented ellipse to represent BFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_ellipse(axis, bf_stats, bf_idx, colorVal):\n",
    "  y_cen, x_cen = bf_stats[\"envelope_centers\"][bf_idx]\n",
    "  (y_ang, x_ang) = bf_stats[\"orientations\"][bf_idx]\n",
    "  angle = np.rad2deg(np.arctan2(y_ang, x_ang))\n",
    "  e = matplotlib.patches.Ellipse(xy=[x_cen, y_cen], width=0.8,\n",
    "    height=0.3, angle=angle, color=colorVal, alpha=1.0, fill=True)\n",
    "  axis.add_artist(e)\n",
    "  e.set_clip_box(axis.bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct analysis plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Eigenvalues of activity covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evals, evecs = np.linalg.eig(a_cov)\n",
    "fig, ax = plt.subplots(1, figsize=(10,10))\n",
    "ax.semilogy(evals)\n",
    "ax.set_xlim(1, 500) # Ignore first eigenvalue\n",
    "ax.set_ylim(0, 1200)\n",
    "ax.set_yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Basis function analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_hilbert_analysis(weights, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_bf_stats(weights, params) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Pooling and eigen summary plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_pooling_filters = 50\n",
    "num_connected_weights = 250 # Top connected weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_pooling_filter_images(params, weights, pooling_filters,\n",
    "  num_connected_weights, num_pooling_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_pooling_func(plot_lines, params, weights, pooling_filters,\n",
    "  num_connected_weights, num_pooling_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_pooling_func(plot_ellipse, params, weights, evecs[sort_indices[:50]],\n",
    "  num_connected_weights, num_pooling_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_pooling_func(plot_ellipse, params, weights, pooling_filters,\n",
    "  num_connected_weights, num_pooling_filters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
