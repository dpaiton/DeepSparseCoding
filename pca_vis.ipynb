{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load or train LCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os                                                                       \n",
    "import numpy as np                                                              \n",
    "import tensorflow as tf                                                         \n",
    "                                                                                \n",
    "import data.data_picker as dp                                                   \n",
    "import utils.plot_functions as pf                                               \n",
    "import utils.image_processing as ip                                             \n",
    "                                                                                \n",
    "params = {                                                                      \n",
    "  ## Model params                                                               \n",
    "  \"out_dir\": os.path.expanduser(\"~\")+\"/Work/Projects/strongPCA/outputs/\",       \n",
    "  \"chk_dir\": os.path.expanduser(\"~\")+\"/Work/Projects/strongPCA/checkpoints/\",   \n",
    "  \"data_dir\": os.path.expanduser(\"~\")+\"/Work/Datasets/\",                        \n",
    "  \"load_chk\": True,                                                             \n",
    "  \"update_interval\": 100,                                                       \n",
    "  \"device\": \"/cpu:0\",                                                           \n",
    "  \"learning_rate\": 0.12,                                                        \n",
    "  \"num_neurons\": 500,                                                           \n",
    "  \"sparse_mult\": 0.2,                                                           \n",
    "  \"num_inference_steps\": 20,                                                    \n",
    "  \"eta\": 0.001/0.03, #dt/tau                                                    \n",
    "  \"eps\": 1e-12,                                                                 \n",
    "  ## Data params                                                                \n",
    "  \"data_type\": \"vanhateren\",                                                    \n",
    "  \"rand_state\": np.random.RandomState(12345),                                   \n",
    "  \"num_images\": 5,#50,                                                          \n",
    "  \"num_batches\": 30000,                                                         \n",
    "  \"batch_size\": 100,                                                            \n",
    "  \"patch_edge_size\": 16,                                                        \n",
    "  \"overlapping_patches\": True,                                                  \n",
    "  \"patch_variance_threshold\": 1e-6,                                             \n",
    "  \"conv\": False,                                                                \n",
    "  \"whiten_images\": True}                                                        \n",
    "                                                                                \n",
    "## Calculated params                                                            \n",
    "params[\"epoch_size\"] = params[\"batch_size\"] * params[\"num_batches\"]             \n",
    "params[\"num_pixels\"] = int(params[\"patch_edge_size\"]**2)                        \n",
    "params[\"dataset_shape\"] = [int(val)                                             \n",
    "    for val in [params[\"epoch_size\"], params[\"num_pixels\"]]],                   \n",
    "params[\"phi_shape\"] = [params[\"num_pixels\"], params[\"num_neurons\"]]             \n",
    "                                                                                \n",
    "graph = tf.Graph()                                                              \n",
    "with tf.device(params[\"device\"]):                                               \n",
    "  with graph.as_default():                                                      \n",
    "    with tf.name_scope(\"placeholders\") as scope:                                \n",
    "      x = tf.placeholder(tf.float32, shape=[params[\"batch_size\"],               \n",
    "        params[\"num_pixels\"]], name=\"input_data\")                               \n",
    "      sparse_mult = tf.placeholder(tf.float32, shape=(), name=\"sparse_mult\")    \n",
    "                                                                                \n",
    "    with tf.name_scope(\"constants\") as scope:                                   \n",
    "      u_zeros = tf.zeros(shape=tf.stack([tf.shape(x)[0],                        \n",
    "        params[\"num_neurons\"]]), dtype=tf.float32, name=\"u_zeros\")              \n",
    "      u_noise = tf.truncated_normal(                                            \n",
    "        shape=tf.stack([tf.shape(x)[0], params[\"num_neurons\"]]),                \n",
    "        mean=0.0, stddev=0.1, dtype=tf.float32, name=\"u_noise\")                 \n",
    "                                                                                \n",
    "    with tf.name_scope(\"step_counter\") as scope:                                \n",
    "      global_step = tf.Variable(0, trainable=False, name=\"global_step\")         \n",
    "                                                                                \n",
    "    with tf.variable_scope(\"weights\") as scope:                                 \n",
    "      phi_init = tf.truncated_normal(params[\"phi_shape\"], mean=0.0, stddev=0.5, \n",
    "        dtype=tf.float32, name=\"phi_init\")                                  \n",
    "      phi = tf.get_variable(name=\"phi\", dtype=tf.float32,                       \n",
    "        initializer=phi_init, trainable=True)                                   \n",
    "                                                                                \n",
    "    with tf.name_scope(\"norm_weights\") as scope:                                \n",
    "      norm_phi = phi.assign(tf.nn.l2_normalize(phi,                             \n",
    "        dim=0, epsilon=params[\"eps\"], name=\"row_l2_norm\"))                      \n",
    "      norm_weights = tf.group(norm_phi,                                         \n",
    "        name=\"l2_normalization\")                                                \n",
    "                                                                                \n",
    "    with tf.name_scope(\"inference\") as scope:                                   \n",
    "      u = tf.Variable(u_zeros, trainable=False,                                 \n",
    "        validate_shape=False, name=\"u\")                                         \n",
    "      # soft thresholded, rectified                                             \n",
    "      a = tf.where(tf.greater(u, sparse_mult),                                  \n",
    "        tf.subtract(u, sparse_mult), u_zeros,                                   \n",
    "        name=\"activity\")                                                        \n",
    "                                                                                \n",
    "    with tf.name_scope(\"output\") as scope:                                      \n",
    "      with tf.name_scope(\"image_estimate\"):                                     \n",
    "        x_ = tf.matmul(a, tf.transpose(phi),                                    \n",
    "          name=\"reconstruction\")                                                \n",
    "                                                                                \n",
    "    with tf.name_scope(\"loss\") as scope:                                        \n",
    "      with tf.name_scope(\"unsupervised\"):                                       \n",
    "        recon_loss = tf.reduce_mean(0.5 *                                       \n",
    "          tf.reduce_sum(tf.pow(tf.subtract(x, x_), 2.0),                        \n",
    "          axis=[1]), name=\"recon_loss\")                                         \n",
    "        sparse_loss = sparse_mult * tf.reduce_mean(                             \n",
    "          tf.reduce_sum(tf.abs(a), axis=[1]),                                   \n",
    "          name=\"sparse_loss\")                                                   \n",
    "        unsupervised_loss = (recon_loss + sparse_loss)                          \n",
    "      total_loss = unsupervised_loss                                            \n",
    "                                                                                \n",
    "    with tf.name_scope(\"update_u\") as scope:                                    \n",
    "      lca_b = tf.matmul(x, phi, name=\"driving_input\")                           \n",
    "      lca_g = (tf.matmul(tf.transpose(phi), phi,                                \n",
    "        name=\"gram_matrix\") -                                                   \n",
    "        tf.constant(np.identity(params[\"phi_shape\"][1], dtype=np.float32),      \n",
    "        name=\"identity_matrix\"))                                                \n",
    "      lca_explain_away = tf.matmul(a, lca_g,                                    \n",
    "        name=\"explaining_away\")                                                 \n",
    "      du = lca_b - lca_explain_away - u                                         \n",
    "      step_inference = tf.group(u.assign_add(params[\"eta\"] * du),               \n",
    "        name=\"step_inference\")                                                  \n",
    "      reset_activity = tf.group(u.assign(u_zeros),                              \n",
    "        name=\"reset_activity\")                                                  \n",
    "                                                                                \n",
    "    with tf.name_scope(\"performance_metrics\") as scope:                         \n",
    "      with tf.name_scope(\"reconstruction_quality\"):                             \n",
    "        MSE = tf.reduce_mean(tf.pow(tf.subtract(x, x_), 2.0),                   \n",
    "          axis=[1, 0], name=\"mean_squared_error\")                               \n",
    "                                                                                \n",
    "    with tf.name_scope(\"optimizers\") as scope:                                  \n",
    "      learning_rates = tf.train.exponential_decay(                              \n",
    "        learning_rate=params[\"learning_rate\"],                                  \n",
    "        global_step=global_step,                                                \n",
    "        decay_steps=int(np.floor(params[\"num_batches\"]*0.9)),                   \n",
    "        decay_rate=0.5,                                                         \n",
    "        staircase=True,                                                         \n",
    "        name=\"phi_annealing_schedule\")                                          \n",
    "      optimizer = tf.train.GradientDescentOptimizer(learning_rates,             \n",
    "        name=\"phi_optimizer\")                                                   \n",
    "      update_weights = optimizer.minimize(total_loss, global_step=global_step, \n",
    "        var_list=[phi], name=\"phi_minimizer\")                                   \n",
    "                                                                                \n",
    "    full_saver = tf.train.Saver(var_list=[phi], max_to_keep=2)                  \n",
    "                                                                                \n",
    "    with tf.name_scope(\"summaries\") as scope:                                   \n",
    "      #tf.summary.image(\"input\", tf.reshape(x, [params[\"batch_size\"],           \n",
    "      #  params[\"patch_edge_size\"], params[\"patch_edge_size\"], 1]))             \n",
    "      #tf.summary.image(\"weights\", tf.reshape(tf.transpose(phi),                \n",
    "      #  [params[\"num_neurons\"], params[\"patch_edge_size\"],                     \n",
    "      #  params[\"patch_edge_size\"], 1]))                                        \n",
    "      tf.summary.histogram(\"u\", u)                                              \n",
    "      tf.summary.histogram(\"a\", a)                                              \n",
    "      tf.summary.histogram(\"phi\", phi)                                          \n",
    "                                                                                \n",
    "    merged_summaries = tf.summary.merge_all()                                   \n",
    "    train_writer = tf.summary.FileWriter(params[\"out_dir\"], graph)              \n",
    "                                                                                \n",
    "    with tf.name_scope(\"initialization\") as scope:                              \n",
    "      init_op = tf.group(tf.global_variables_initializer(),                     \n",
    "        tf.local_variables_initializer())                                       \n",
    "                                                                                \n",
    "if not os.path.exists(params[\"out_dir\"]):                                       \n",
    "  os.makedirs(params[\"out_dir\"])                                                \n",
    "if not os.path.exists(params[\"chk_dir\"]):                                       \n",
    "  os.makedirs(params[\"chk_dir\"])                                                \n",
    "                                                                                \n",
    "print(\"Loading data...\")                                                        \n",
    "data = dp.get_data(params[\"data_type\"], params)                                 \n",
    "params[\"input_shape\"] = [                                                       \n",
    "  data[\"train\"].num_rows*data[\"train\"].num_cols*data[\"train\"].num_channels]     \n",
    "                                                                                \n",
    "print(\"Initializing Session...\")\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "\n",
    "sess.run(init_op,\n",
    "feed_dict={x:np.zeros([params[\"batch_size\"]]+params[\"input_shape\"],\n",
    "  dtype=np.float32)})\n",
    "\n",
    "if params[\"load_chk\"]:\n",
    "  full_saver.restore(sess, tf.train.latest_checkpoint(params[\"chk_dir\"]))\n",
    "else:\n",
    "  batch_steps = []\n",
    "  losses = []\n",
    "  sparsities = []\n",
    "  recon_errors = []\n",
    "  for b_step in range(params[\"num_batches\"]):\n",
    "    data_batch = data[\"train\"].next_batch(params[\"batch_size\"])\n",
    "    input_data = data_batch[0]\n",
    "  \n",
    "    feed_dict = {x:input_data, sparse_mult:params[\"sparse_mult\"]}\n",
    "  \n",
    "    sess.run(norm_weights, feed_dict)\n",
    "  \n",
    "    for inference_step in range(params[\"num_inference_steps\"]):               \n",
    "      sess.run(step_inference, feed_dict)\n",
    "  \n",
    "    sess.run(update_weights, feed_dict)\n",
    "  \n",
    "    current_step = sess.run(global_step)\n",
    "    if (current_step % params[\"update_interval\"] == 0):\n",
    "      summary = sess.run(merged_summaries, feed_dict)\n",
    "      train_writer.add_summary(summary, current_step)\n",
    "      full_saver.save(sess, save_path=params[\"chk_dir\"]+\"lca_chk\",\n",
    "        global_step=global_step)\n",
    "  \n",
    "      [current_loss, a_vals, recons, recon_err, weights] = sess.run(\n",
    "        [total_loss, a, x_, MSE, phi], feed_dict)\n",
    "      a_vals_max = np.array(a_vals.max()).tolist()\n",
    "      a_frac_act = np.array(np.count_nonzero(a_vals)\n",
    "        / float(params[\"batch_size\"]*params[\"num_neurons\"])).tolist()\n",
    "      batch_steps.append(current_step)\n",
    "      losses.append(current_loss)\n",
    "      sparsities.append(a_frac_act)\n",
    "      recon_errors.append(recon_err)\n",
    "  \n",
    "      print_dict = {\"current_step\":str(current_step).zfill(5),\n",
    "        \"loss\":str(current_loss),\n",
    "        \"a_max\":str(a_vals_max),\n",
    "        \"a_frac_act\":str(a_frac_act)}\n",
    "      print(print_dict)\n",
    "      pf.save_data_tiled(weights.T.reshape((params[\"num_neurons\"],\n",
    "        params[\"patch_edge_size\"], params[\"patch_edge_size\"])),\n",
    "        normalize=False, title=\"Dictionary at step \"+str(current_step),\n",
    "        save_filename=(params[\"out_dir\"]+\"phi_\"+str(current_step).zfill(5)\n",
    "        +\".png\"))\n",
    "      pf.save_data_tiled(recons.reshape((params[\"batch_size\"],\n",
    "        params[\"patch_edge_size\"], params[\"patch_edge_size\"])),\n",
    "        normalize=False, title=\"Recons at step \"+str(current_step),\n",
    "        save_filename=(params[\"out_dir\"]+\"recons_\"\n",
    "        +str(current_step).zfill(5)+\".png\"))\n",
    "    output_data = {\"batch_step\":batch_steps, \"total_loss\":losses,\n",
    "      \"frac_active\":sparsities, \"recon_MSE\":recon_errors}\n",
    "    pf.save_stats(output_data, save_filename=(params[\"out_dir\"]+\"loss.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a new batch of data for PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_data = data[\"train\"].next_batch(100)[0]\n",
    "feed_dict = {x:input_data, sparse_mult:params[\"sparse_mult\"]}\n",
    "[a_vals, weights] = sess.run([a, phi], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def hilbertize(dictionary, params):\n",
    "  cart2pol = lambda x,y: (np.arctan2(y,x), np.hypot(x, y))\n",
    "  \n",
    "  # Amount of zero padding for fft2 (closest power of 2)                          \n",
    "  N = np.int(2**(np.ceil(np.log2(params[\"patch_edge_size\"]))))                                  \n",
    "                                                                                  \n",
    "  # Analytic signal envelope for dictionary                                                \n",
    "  # (Hilbet transform of each basis function)                                     \n",
    "  Env = np.zeros((params[\"num_pixels\"], params[\"num_neurons\"]), dtype=complex)                        \n",
    "                                                                                  \n",
    "  # Fourier transform of dictionary                                               \n",
    "  Zf = np.zeros((N**2, params[\"num_neurons\"]), dtype=complex)                               \n",
    "\n",
    "  # Hilbert filters\n",
    "  Hil_filt = np.zeros((params[\"num_neurons\"], params[\"patch_edge_size\"],\n",
    "    params[\"patch_edge_size\"]))\n",
    "\n",
    "  # Grid for creating filter\n",
    "  f = (2/N) * np.pi * np.arange(-N/2.0, N/2.0)                                    \n",
    "  (fx, fy) = np.meshgrid(f, f)                                                    \n",
    "  (theta, r) = cart2pol(fx, fy)                                                    \n",
    "                                                                                  \n",
    "  for neuron_idx in range(params[\"num_neurons\"]):                                           \n",
    "    # Grab single basis function, reshape to a square image                       \n",
    "    bf = dictionary[:,neuron_idx].reshape(params[\"patch_edge_size\"],\n",
    "      params[\"patch_edge_size\"])               \n",
    "                                                                                  \n",
    "    # Convert basis function into DC-centered Fourier domain                      \n",
    "    bff = np.fft.fftshift(np.fft.fft2(bf, [N, N]))                                \n",
    "                                                                                  \n",
    "    # Find indices of the peak amplitude                                          \n",
    "    max_ys = np.abs(bff).argmax(axis=0) # Returns row index for each col          \n",
    "    max_x = np.argmax(np.abs(bff).max(axis=0))                                    \n",
    "                                                                                  \n",
    "    # Convert peak amplitude location into angle in freq domain                   \n",
    "    fx_ang = f[max_x]                                                             \n",
    "    fy_ang = f[max_ys[max_x]]                                                     \n",
    "    theta_max = np.arctan2(fy_ang, fx_ang)                                        \n",
    "                                                                                  \n",
    "    # Define the half-plane with respect to the maximum                           \n",
    "    ang_diff = np.abs(theta-theta_max)                                            \n",
    "    idx = (ang_diff>np.pi).nonzero()                                              \n",
    "    ang_diff[idx] = 2.0 * np.pi - ang_diff[idx]                                   \n",
    "    Hil_filt[neuron_idx, ...] = (ang_diff < np.pi/2.0).astype(int)                                        \n",
    "                                                                                  \n",
    "    # Create analytic signal from the inverse FT of the half-plane filtered bf\n",
    "    abf = np.fft.ifft2(np.fft.fftshift(Hil_filt[neuron_idx, ...]*bff))                                  \n",
    "    Env[:, neuron_idx] = abf[0:params[\"patch_edge_size\"],\n",
    "      0:params[\"patch_edge_size\"]].reshape(params[\"num_pixels\"])    \n",
    "    Zf[:, neuron_idx] = (Hil_filt[neuron_idx, ...]*bff).reshape(N**2)                                     \n",
    "  return (Env, Zf, Hil_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Envelope, Zf, Hil_filter = hilbertize(weights, params)\n",
    "N = np.int32(np.sqrt(Zf.shape[0]))\n",
    "                                                                                \n",
    "# Plot BFs, Hilbert filtered fourier amplitudes, raw fourier amplitudes\n",
    "fig, sub_axes = plt.subplots(1, 3)\n",
    "\n",
    "plot_data = pf.pad_data(weights.T.reshape((params[\"num_neurons\"],\n",
    "  params[\"patch_edge_size\"], params[\"patch_edge_size\"])))\n",
    "bf_axis_image = sub_axes[0].imshow(plot_data, cmap=\"Greys_r\",\n",
    "  interpolation=\"nearest\")\n",
    "sub_axes[0].tick_params(\n",
    "  axis=\"both\",\n",
    "  bottom=\"off\",\n",
    "  top=\"off\",\n",
    "  left=\"off\",\n",
    "  right=\"off\")\n",
    "sub_axes[0].get_xaxis().set_visible(False)\n",
    "sub_axes[0].get_yaxis().set_visible(False)\n",
    "sub_axes[0].set_title(\"Basis Functions\", fontsize=10)\n",
    "\n",
    "plot_data = pf.pad_data(np.abs(Envelope).T.reshape((params[\"num_neurons\"],\n",
    "  params[\"patch_edge_size\"], params[\"patch_edge_size\"])))\n",
    "hil_axis_image = sub_axes[1].imshow(plot_data, cmap=\"Greys_r\",\n",
    "  interpolation=\"nearest\")\n",
    "sub_axes[1].tick_params(\n",
    "  axis=\"both\",\n",
    "  bottom=\"off\",\n",
    "  top=\"off\",\n",
    "  left=\"off\",\n",
    "  right=\"off\")\n",
    "sub_axes[1].get_xaxis().set_visible(False)\n",
    "sub_axes[1].get_yaxis().set_visible(False)\n",
    "sub_axes[1].set_title(\"Analytic Signal Amplitude Envelope\", fontsize=10)\n",
    "\n",
    "resh_Zf = np.abs(Zf).T.reshape((params[\"num_neurons\"], N, N))                             \n",
    "output_z = np.zeros(resh_Zf.shape)                                              \n",
    "for i in range(params[\"num_neurons\"]):                                                    \n",
    "  output_z[i,...] = resh_Zf[i,...] / np.max(resh_Zf[i,...])                     \n",
    "plot_data = pf.pad_data(output_z)\n",
    "hil_axis_image = sub_axes[2].imshow(plot_data, cmap=\"Greys_r\",\n",
    "  interpolation=\"nearest\")\n",
    "sub_axes[2].tick_params(\n",
    "  axis=\"both\",\n",
    "  bottom=\"off\",\n",
    "  top=\"off\",\n",
    "  left=\"off\",\n",
    "  right=\"off\")\n",
    "sub_axes[2].get_xaxis().set_visible(False)\n",
    "sub_axes[2].get_yaxis().set_visible(False)\n",
    "sub_axes[2].set_title(\"Fourier Amplitude Spectrum\", fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_dictionary_stats(dictionary, params):\n",
    "dictionary = weights\n",
    "\n",
    "Envelope, Zf, Hil_filter = hilbertize(dictionary, params)\n",
    "\n",
    "basis_funcs = []\n",
    "envelopes = []\n",
    "filters = []\n",
    "centers = []\n",
    "orientations = []\n",
    "scales = []\n",
    "lengths = []\n",
    "images = []\n",
    "for bf_idx in range(params[\"num_neurons\"]):\n",
    "  # Reformatted individual basis function\n",
    "  basis_funcs.append(np.squeeze(ip.reshape_data(dictionary.T[bf_idx,...],\n",
    "    flatten=False)[0]))\n",
    "\n",
    "  # Reformatted individual envelope filter\n",
    "  envelopes.append(np.squeeze(ip.reshape_data(np.abs(Envelope.T[bf_idx,...]),\n",
    "    flatten=False)[0]))\n",
    "  \n",
    "  # Basis function center\n",
    "  env_resh = np.squeeze(ip.reshape_data(Envelope.T[bf_idx, :],\n",
    "    flatten=False)[0])\n",
    "  max_ys = np.abs(env_resh).argmax(axis=0) # Returns row index for each col          \n",
    "  max_x = np.argmax(np.abs(env_resh).max(axis=0))\n",
    "  y_cen = max_ys[max_x]\n",
    "  x_cen = max_x\n",
    "  centers.append((y_cen, x_cen))\n",
    "  \n",
    "  # Basis function orientation\n",
    "  filt = Hil_filter[bf_idx, ...]\n",
    "  filters.append(filt)\n",
    "  y, x = np.nonzero(filt)\n",
    "  x = x - np.mean(x)\n",
    "  y = y - np.mean(y)\n",
    "  coords = np.vstack([y, x])\n",
    "  evals, evecs = np.linalg.eig(np.cov(coords))\n",
    "  sort_indices = np.argsort(evals)[::-1]\n",
    "  evec = evecs[:, sort_indices[0]]\n",
    "  orientations.append(evec)\n",
    "  #length = np.sqrt(evec[0]**2+evec[1]**2)*np.abs(evals[sort_indices[0]])\n",
    "  length = evals[sort_indices[0]]\n",
    "  lengths.append(length)\n",
    "\n",
    "  out_image = np.zeros((16, 16)) # row (y), col (x)\n",
    "  x_start = np.int32(np.max([0, np.min([np.ceil(x_cen-evec[1]*length/2), 15])]))\n",
    "  y_start = np.int32(np.max([0, np.min([np.ceil(y_cen-evec[0]*length/2), 15])]))\n",
    "  x_end = np.int32(np.max([0, np.min([np.ceil(x_cen+evec[1]*length/2), 15])]))\n",
    "  y_end = np.int32(np.max([0, np.min([np.ceil(y_cen+evec[0]*length/2), 15])]))\n",
    "  y_lin, x_lin = skimage.draw.line(y_start, x_start, y_end, x_end)\n",
    "  out_image[y_lin, x_lin] += 1\n",
    "  images.append(out_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.draw\n",
    "\n",
    "num_bf = 8\n",
    "fig, sub_axes = plt.subplots(num_bf, 5)\n",
    "for bf_idx in range(num_bf):\n",
    "  bf = basis_funcs[bf_idx]\n",
    "  env = envelopes[bf_idx]\n",
    "  filt = filters[bf_idx]\n",
    "  y, x = orientations[bf_idx]\n",
    "  y_cen, x_cen = centers[bf_idx]\n",
    "  length = lengths[bf_idx]/3 #TODO: rescaling is arbitrary - eigen value is too large upon visual inspection\n",
    "\n",
    "  sub_axes[bf_idx, 0].imshow(bf, cmap=\"Greys_r\", interpolation=\"Nearest\")\n",
    "  sub_axes[bf_idx, 0].tick_params(axis=\"both\", bottom=\"off\", top=\"off\",\n",
    "    left=\"off\", right=\"off\")\n",
    "  sub_axes[bf_idx, 0].get_xaxis().set_visible(False)\n",
    "  sub_axes[bf_idx, 0].get_yaxis().set_visible(False)\n",
    "\n",
    "  sub_axes[bf_idx, 1].imshow(env, cmap=\"Greys_r\", interpolation=\"Nearest\")\n",
    "  sub_axes[bf_idx, 1].tick_params(axis=\"both\", bottom=\"off\", top=\"off\",\n",
    "    left=\"off\", right=\"off\")\n",
    "  sub_axes[bf_idx, 1].get_xaxis().set_visible(False)\n",
    "  sub_axes[bf_idx, 1].get_yaxis().set_visible(False)\n",
    "  \n",
    "  sub_axes[bf_idx, 2].imshow(filt, cmap=\"Greys_r\", interpolation=\"Nearest\")\n",
    "  sub_axes[bf_idx, 2].tick_params(axis=\"both\", bottom=\"off\", top=\"off\",\n",
    "    left=\"off\", right=\"off\")\n",
    "  sub_axes[bf_idx, 2].get_xaxis().set_visible(False)\n",
    "  sub_axes[bf_idx, 2].get_yaxis().set_visible(False)\n",
    "  \n",
    "  # TODO: Can't make this square...\n",
    "  sub_axes[bf_idx, 3].plot([(x_cen-x*length/2), (x_cen+x*length/2)],\n",
    "    [(y_cen-y*length/2), (y_cen+y*length/2)], color='blue')\n",
    "  sub_axes[bf_idx, 3].invert_yaxis()\n",
    "  sub_axes[bf_idx, 3].spines['left'].set_position('center')\n",
    "  sub_axes[bf_idx, 3].spines['left'].set_color('none')\n",
    "  sub_axes[bf_idx, 3].spines['right'].set_color('black')\n",
    "  sub_axes[bf_idx, 3].spines['bottom'].set_position('center')\n",
    "  sub_axes[bf_idx, 3].spines['bottom'].set_color('none')\n",
    "  sub_axes[bf_idx, 3].spines['top'].set_color('black')\n",
    "  sub_axes[bf_idx, 3].tick_params(axis=\"both\", bottom=\"off\", top=\"off\",\n",
    "    left=\"off\", right=\"off\")\n",
    "  sub_axes[bf_idx, 3].get_xaxis().set_visible(False)\n",
    "  sub_axes[bf_idx, 3].get_yaxis().set_visible(False)\n",
    "  sub_axes[bf_idx, 3].axis(\"equal\")\n",
    "  sub_axes[bf_idx, 3].set_aspect(\"equal\")\n",
    "\n",
    "  img = images[bf_idx]\n",
    "  img[y_cen, x_cen] += 1\n",
    "  sub_axes[bf_idx, 4].imshow(img, interpolation=\"Nearest\")\n",
    "  sub_axes[bf_idx, 4].tick_params(axis=\"both\", bottom=\"off\", top=\"off\",\n",
    "    left=\"off\", right=\"off\")\n",
    "  sub_axes[bf_idx, 4].get_xaxis().set_visible(False)\n",
    "  sub_axes[bf_idx, 4].get_yaxis().set_visible(False)\n",
    "    \n",
    "sub_axes[0,0].set_title(\"bf\")\n",
    "sub_axes[0,1].set_title(\"envelope\")\n",
    "sub_axes[0,2].set_title(\"filter\")\n",
    "sub_axes[0,3].set_title(\"eigenvect\")\n",
    "sub_axes[0,4].set_title(\"image\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_reduc = ip.pca_reduction(a_vals, num_dim=16)[0]\n",
    "print(a_reduc.shape)\n",
    "\n",
    "#evals, evecs = np.linalg.eig(np.cov(a_reduc))\n",
    "#sort_indices = np.argsort(evals)[::-1]\n",
    "\n",
    "# loop over top PCs\n",
    "#pc_idx = 0\n",
    "#val = evals[sort_indices[pc_idx]]\n",
    "#vec = evecs[sort_indices[pc_idx]]\n",
    "\n",
    "#num_lines = 2*4\n",
    "\n",
    "#fig, sub_axes = plt.subplots(1)\n",
    "#\n",
    "#alph = 1.0\n",
    "#for bf_idx in sort_indices[:int(num_lines/2)]: # Top lines\n",
    "#  sub_axes.imshow(images[bf_idx], alpha=alph, cmap=\"Reds\", interpolation=\"nearest\")\n",
    "#  alph -= 1.0/(num_lines/2)\n",
    "#    \n",
    "#alph = 1.0\n",
    "#for bf_idx in sort_indices[::-1][:int(num_lines/2)]: # Bottom lines\n",
    "#  sub_axes.imshow(images[bf_idx], alpha=alph, cmap=\"Blues\", interpolation=\"nearest\")\n",
    "#  alph -= 1.0/(num_lines/2)\n",
    "#\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
